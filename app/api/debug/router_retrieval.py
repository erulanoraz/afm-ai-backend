# app/api/debug/router_retrieval.py
import logging
from typing import List, Dict, Any

import numpy as np
from fastapi import APIRouter, Depends
from pydantic import BaseModel
from sqlalchemy.orm import Session

from app.db import get_db
from app.services.retrieval import get_file_docs_for_qualifier
from app.services.embeddings import embed_text

logger = logging.getLogger(__name__)

router = APIRouter(
    prefix="/debug/retrieval",
    tags=["DEBUG ‚Äì Retrieval"]
)


# ============================
# üì• –ú–æ–¥–µ–ª—å –∑–∞–ø—Ä–æ—Å–∞
# ============================

class RetrievalRequest(BaseModel):
    case_id: str
    query: str
    top_k: int = 20


# ============================
# üî¢ Cosine similarity
# ============================

def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:
    denom = np.linalg.norm(a) * np.linalg.norm(b)
    if denom == 0:
        return 0.0
    return float(a.dot(b) / denom)


# ============================
# üî¢ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è baseline weight
# ============================

def normalize_baseline_weight(w: float) -> float:
    """
    –ü—Ä–∏–≤–æ–¥–∏–º baseline_weight –∫ –¥–∏–∞–ø–∞–∑–æ–Ω—É [0, 1],
    —á—Ç–æ–±—ã –º–æ–∂–Ω–æ –±—ã–ª–æ —Å–º–µ—à–∏–≤–∞—Ç—å —Å cosine score.
    """
    if w <= 0:
        return 0.0
    # 1.5 ‚Äî —É—Å–ª–æ–≤–Ω—ã–π "–º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π" –≤–µ—Å, –º–æ–∂–Ω–æ –ø–æ–¥—Å—Ç—Ä–æ–∏—Ç—å –ø–æ–¥ –ø—Ä–∞–∫—Ç–∏–∫—É
    return float(max(0.0, min(1.0, w / 1.5)))


# ============================
# üß† DEBUG Retrieval 5.1
# ============================

@router.post(
    "/",
    summary="–ü—Ä–æ–≤–µ—Ä–∫–∞ Retrieval 5.1 (baseline + —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π rerank –ø–æ —á–∞–Ω–∫–∞–º)",
)
async def debug_retrieval(
    req: RetrievalRequest,
    db: Session = Depends(get_db),
):
    """
    –ü–æ–ª–Ω—ã–π debug Retrieval-–ø–∞–π–ø–ª–∞–π–Ω–∞:

    1) –ó–∞–±–∏—Ä–∞–µ–º —á–∞–Ω–∫–∏ Retrieval 5.0 (baseline –æ—Ç–±–æ—Ä –ø–æ –¥–µ–ª—É)
    2) –°—Ç—Ä–æ–∏–º embedding –¥–ª—è query
    3) –î–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞ —Å—Ç—Ä–æ–∏–º embedding —Ç–µ–∫—Å—Ç–∞ (—É—Å–µ—á—ë–Ω–Ω–æ)
    4) –°—á–∏—Ç–∞–µ–º cosine similarity(query, chunk)
    5) –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º baseline_weight –∏ cosine score ‚Üí final_score
    6) –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ final_score –∏ –æ—Ç–¥–∞—ë–º top-K —á–∞–Ω–∫–æ–≤

    –≠—Ç–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø—Ä–∏–±–ª–∏–∂–µ–Ω–æ –∫ –±–æ–µ–≤–æ–º—É —Ä–µ–∂–∏–º—É:
    - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ—Ç –∂–µ Retrieval 5.0 (get_file_docs_for_qualifier)
    - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ—Ç –∂–µ embed_text(), —á—Ç–æ –∏ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º RAG
    - —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–¥—ë—Ç –ø–æ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É —Å–∫–æ—Ä—É baseline + —Å–µ–º–∞–Ω—Ç–∏–∫–∞
    """

    # ---------------------------------------
    # 1) Retrieval 5.0 ‚Äî baseline –¥–æ–∫—É–º–µ–Ω—Ç—ã
    # ---------------------------------------
    docs: List[Dict[str, Any]] = get_file_docs_for_qualifier(
        db,
        case_id=req.case_id,
    )

    if not docs:
        return {
            "case_id": req.case_id,
            "query": req.query,
            "error": "no_docs",
            "message": "Retrieval 5.0 –Ω–µ –Ω–∞—à—ë–ª –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –¥–µ–ª—É",
        }

    baseline_count = len(docs)
    logger.info(f"[DEBUG Retrieval] Baseline docs: {baseline_count}")

    # ---------------------------------------
    # 2) Query embedding
    # ---------------------------------------
    try:
        q_vec_list = embed_text(req.query)
    except Exception as e:
        logger.error(f"[DEBUG Retrieval] –û—à–∏–±–∫–∞ embed_text –¥–ª—è query: {e}")
        return {
            "case_id": req.case_id,
            "query": req.query,
            "error": "embedding_error",
            "message": f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ embedding –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: {e}",
        }

    if not q_vec_list:
        return {
            "case_id": req.case_id,
            "query": req.query,
            "error": "embedding_error",
            "message": "Embedding-—Å–µ—Ä–≤–∏—Å –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π –≤–µ–∫—Ç–æ—Ä –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞",
        }

    q_vec = np.array(q_vec_list, dtype=np.float32)

    # ---------------------------------------
    # 3) –î–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞ —Å—á–∏—Ç–∞–µ–º cosine
    # ---------------------------------------
    results: List[Dict[str, Any]] = []

    for d in docs:
        text = d["text"] or ""
        if not text.strip():
            continue

        # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å embedding-–º–æ–¥–µ–ª—å
        chunk_text = text[:800]

        try:
            chunk_vec_list = embed_text(chunk_text)
        except Exception as e:
            logger.error(
                f"[DEBUG Retrieval] –û—à–∏–±–∫–∞ embed_text –¥–ª—è —á–∞–Ω–∫–∞ "
                f"{d.get('file_id')}:{d.get('chunk_id')}: {e}"
            )
            continue

        if not chunk_vec_list:
            # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —á–∞–Ω–∫–∏ –±–µ–∑ –≤–µ–∫—Ç–æ—Ä–∞
            continue

        chunk_vec = np.array(chunk_vec_list, dtype=np.float32)
        cosine_score = cosine_similarity(q_vec, chunk_vec)

        # baseline_weight —É–∂–µ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω –≤ get_file_docs_for_qualifier
        baseline_w = float(d.get("baseline_weight", 0.0))
        baseline_norm = normalize_baseline_weight(baseline_w)

        # –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∫–æ—Ä:
        #  - 60% —Å–µ–º–∞–Ω—Ç–∏–∫–∞
        #  - 40% baseline (—Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞, evidence)
        final_score = 0.6 * cosine_score + 0.4 * baseline_norm

        results.append(
            {
                "file_id": d["file_id"],
                "filename": d["filename"],
                "page": d["page"],
                "chunk_id": d["chunk_id"],
                "baseline_weight": baseline_w,
                "baseline_norm": baseline_norm,
                "cosine_score": cosine_score,
                "final_score": final_score,
                "text": (
                    text[:400] + "..."
                    if len(text) > 400
                    else text
                ),
            }
        )

    if not results:
        return {
            "case_id": req.case_id,
            "query": req.query,
            "baseline_docs": baseline_count,
            "error": "no_semantic_results",
            "message": "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –æ—Ü–µ–Ω–∫–∏ –¥–ª—è —á–∞–Ω–∫–æ–≤ "
                       "(embedding-–ø—Ä–æ–≤–∞–π–¥–µ—Ä –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç—ã–µ –≤–µ–∫—Ç–æ—Ä–∞).",
        }

    # ---------------------------------------
    # 4) –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –∏ top-K
    # ---------------------------------------
    results = sorted(results, key=lambda x: x["final_score"], reverse=True)
    top_k = max(1, req.top_k)
    results = results[:top_k]

    return {
        "case_id": req.case_id,
        "query": req.query,
        "top_k": top_k,
        "baseline_docs": baseline_count,
        "returned": len(results),
        "results": results,
    }
