# app/api/v1/upload.py
"""
Evidence Engine INGEST v3.1 ‚Äî Upload API
–ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø:
‚úÖ –£–¥–∞–ª–µ–Ω–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç app.tasks.ingest (–¥—É–±–ª–∏—Ä—É—é—â–µ–≥–æ –º–æ–¥—É–ª—è)
‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è app.services.ingest_service –Ω–∞–ø—Ä—è–º—É—é
‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Vector Store (—á–µ—Ä–µ–∑ vector_tasks)
"""

import uuid
import os
import tempfile
import zipfile
import shutil
import logging
import re
from typing import List, Optional, Dict

from fastapi import (
    APIRouter,
    Depends,
    UploadFile,
    File as FastAPIFile,
    HTTPException,
)
from sqlalchemy.orm import Session

from app.db import get_db
from app.db.models import File
from app.services.parser import extract_text_from_file
from app.utils.config import settings

# ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º ingest_service –Ω–∞–ø—Ä—è–º—É—é –≤–º–µ—Å—Ç–æ Celery task
from app.services.ingest_service import process_any_file

# ============================================================
# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
# ============================================================
MAX_FILE_SIZE_MB = getattr(settings, "MAX_FILE_SIZE_MB", 100)
MAX_FILE_SIZE_BYTES = MAX_FILE_SIZE_MB * 1024 * 1024

DEFAULT_INGEST_DIR = os.path.join(tempfile.gettempdir(), "afm_ingest")
INGEST_DIR = getattr(settings, "INGEST_DIR", DEFAULT_INGEST_DIR)
os.makedirs(INGEST_DIR, exist_ok=True)

router = APIRouter(prefix="/upload", tags=["Upload"])
logger = logging.getLogger(__name__)

CASE_ID_REGEX = r"(\d{15})"  # –Ω–æ–º–µ—Ä –ï–†–î–† / –¥–µ–ª–∞ ‚Äî 15 —Ü–∏—Ñ—Ä –ø–æ–¥—Ä—è–¥


# ============================================================
# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
# ============================================================

def _extract_case_id_from_name(name: str) -> Optional[str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–æ–º–µ—Ä –ï–†–î–† / –¥–µ–ª–∞ –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞.
    –ò—â–µ–º 15 –ø–æ–¥—Ä—è–¥ –∏–¥—É—â–∏—Ö —Ü–∏—Ñ—Ä.
    """
    if not name:
        return None
    m = re.search(CASE_ID_REGEX, name)
    return m.group(1) if m else None


def _extract_case_id_from_text(text: str) -> Optional[str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–æ–º–µ—Ä –ï–†–î–† / –¥–µ–ª–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
    """
    if not text:
        return None
    m = re.search(CASE_ID_REGEX, text)
    return m.group(1) if m else None


def _detect_case_id_from_pdf(pdf_path: str, filename: str) -> Optional[str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç case_id –∏–∑ —Ç–µ–∫—Å—Ç–∞ PDF (—á–µ—Ä–µ–∑ extract_text_from_file).
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è PDF –≤–Ω—É—Ç—Ä–∏ ZIP –∏ –æ–¥–∏–Ω–æ—á–Ω—ã—Ö PDF.
    """
    try:
        text = extract_text_from_file(pdf_path) or ""
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ PDF {filename}: {e}")
        return None

    if not text.strip():
        return None

    case_id = _extract_case_id_from_text(text)
    if case_id:
        logger.info(f"üîé case_id={case_id} –Ω–∞–π–¥–µ–Ω –≤–Ω—É—Ç—Ä–∏ PDF: {filename}")
    return case_id


def _detect_case_id_for_file(
    file_path: str,
    filename: str,
    outer_case_id: Optional[str] = None,
) -> Optional[str]:
    """
    Evidence Engine style detector:
    1) –ò–∑–≤–ª–µ–∫–∞–µ—Ç case_id –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
    2) –î–ª—è PDF/DOCX/TXT ‚Äî –≤—ã—Ç–∞—Å–∫–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏ –∏—â–µ—Ç case_id
    3) Fallback ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è outer_case_id (–¥–ª—è —Ñ–∞–π–ª–æ–≤ –≤ ZIP)
    """
    if not filename:
        filename = ""

    ext = os.path.splitext(filename)[1].lower()

    # 1) –ü–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
    case_id = _extract_case_id_from_name(filename)
    if case_id:
        logger.info(f"üîé case_id={case_id} –Ω–∞–π–¥–µ–Ω –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏: {filename}")
        return case_id

    # 2) –ü–æ —Ç–µ–∫—Å—Ç—É —Ñ–∞–π–ª–∞ ‚Äî PDF/DOCX/TXT
    text: str = ""

    try:
        if ext in [".pdf", ".docx", ".txt"]:
            # extract_text_from_file —É–º–µ–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –∏ —Å PDF, –∏ —Å DOCX/TXT
            text = extract_text_from_file(file_path) or ""
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ {filename}: {e}")
        text = ""

    if text.strip():
        case_id_from_text = _extract_case_id_from_text(text)
        if case_id_from_text:
            logger.info(
                f"üîé case_id={case_id_from_text} –Ω–∞–π–¥–µ–Ω –≤ —Ç–µ–∫—Å—Ç–µ: {filename}"
            )
            return case_id_from_text

    # 3) Fallback ‚Äî outer_case_id (–¥–ª—è —Ñ–∞–π–ª–æ–≤ –≤ ZIP)
    if outer_case_id:
        logger.info(
            f"‚ÑπÔ∏è –î–ª—è {filename} –∏—Å–ø–æ–ª—å–∑—É–µ–º outer_case_id={outer_case_id}"
        )
        return outer_case_id

    logger.info(f"‚ö†Ô∏è case_id –Ω–µ –Ω–∞–π–¥–µ–Ω: {filename}")
    return None


def _validate_file_size(file: UploadFile) -> None:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞."""
    if hasattr(file, "size") and file.size:
        if file.size > MAX_FILE_SIZE_BYTES:
            raise HTTPException(
                status_code=413,
                detail=(
                    f"–§–∞–π–ª {file.filename} —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π ({file.size / 1024 / 1024:.1f} –ú–ë). "
                    f"–ú–∞–∫—Å–∏–º—É–º: {MAX_FILE_SIZE_MB} –ú–ë"
                ),
            )


# ============================================================
# Main Upload Endpoint
# ============================================================

@router.post("/")
async def upload_files(
    files: List[UploadFile] = FastAPIFile(...),
    db: Session = Depends(get_db),
):
    """
    Evidence Engine INGEST v3.1:
    
    –ü—Ä–æ—Ü–µ—Å—Å:
    1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –±—ã—Å—Ç—Ä–æ)
    2. –°–æ–∑–¥–∞—ë–º –∑–∞–ø–∏—Å—å File –≤ –ë–î –∏ –∫–æ–º–º–∏—Ç–∏–º
    3. –í—ã–∑—ã–≤–∞–µ–º process_any_file() —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ (OCR + Chunker)
    4. process_any_file() —Å–æ–∑–¥–∞—ë—Ç —á–∞–Ω–∫–∏ –≤ –ë–î
    5. –í—ã–∑—ã–≤–∞–µ–º enqueue_chunk_vectorization() –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞
    6. Celery vector_tasks worker –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –≤ Weaviate
    
    –†–µ–∑—É–ª—å—Ç–∞—Ç: –ü–æ–ª–Ω–∞—è pipeline –æ—Ç upload –¥–æ Vector Store
    """
    results: List[dict] = []
    case_ids_map: Dict[str, List[str]] = {}

    logger.info(f"üì§ –ó–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è —Ñ–∞–π–ª–æ–≤: {len(files)}")

    for file in files:
        temp_path: Optional[str] = None

        try:
            # ========== 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ ==========
            try:
                _validate_file_size(file)
            except HTTPException as e:
                results.append({
                    "file_id": None,
                    "filename": file.filename,
                    "chunks_created": 0,
                    "error": e.detail,
                    "status": "failed",
                })
                continue

            # ========== 2. –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ==========
            with tempfile.NamedTemporaryFile(
                delete=False, suffix=f"_{file.filename}"
            ) as tmp:
                temp_path = tmp.name
                content = await file.read()
                if len(content) > MAX_FILE_SIZE_BYTES:
                    raise HTTPException(
                        status_code=413,
                        detail=f"–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π. –ú–∞–∫—Å–∏–º—É–º: {MAX_FILE_SIZE_MB} –ú–ë"
                    )
                tmp.write(content)

            ext = os.path.splitext(file.filename)[1].lower()
            logger.info(f"üì• –ó–∞–≥—Ä—É–∂–µ–Ω {file.filename}, —Ä–∞–∑–º–µ—Ä {len(content)} –±–∞–π—Ç")

            # ========== 3. ZIP ‚Äì —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞ ==========
            if ext == ".zip":
                extract_dir = tempfile.mkdtemp(prefix="unzipped_")
                try:
                    with zipfile.ZipFile(temp_path, "r") as zip_ref:
                        zip_ref.extractall(extract_dir)
                except zipfile.BadZipFile:
                    raise HTTPException(status_code=400, detail="ZIP –ø–æ–≤—Ä–µ–∂–¥—ë–Ω")

                outer_case_id = _extract_case_id_from_name(file.filename)
                logger.info(f"üì¶ ZIP —Ä–∞—Å–ø–∞–∫–æ–≤–∞–Ω, –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤...")

                zip_inner_ids: List[str] = []

                for root, _, inner_files in os.walk(extract_dir):
                    for inner_name in inner_files:
                        inner_path = os.path.join(root, inner_name)
                        inner_ext = os.path.splitext(inner_name)[1].lower()

                        if inner_ext not in [".pdf", ".docx", ".txt"]:
                            continue

                        inner_file_id = uuid.uuid4()

                        try:
                            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º case_id
                            case_id_to_save = _detect_case_id_for_file(
                                file_path=inner_path,
                                filename=inner_name,
                                outer_case_id=outer_case_id,
                            )

                            # –°–æ–∑–¥–∞—ë–º –∑–∞–ø–∏—Å—å File –≤ –ë–î
                            new_file = File(
                                file_id=inner_file_id,
                                filename=inner_name,
                                case_id=case_id_to_save,
                                s3_key=f"s3://afm-originals/{inner_name}",
                                ocr_confidence=0.0,
                                chunks_count=0,
                            )
                            db.add(new_file)
                            db.commit()
                            db.refresh(new_file)
                            logger.info(f"  ‚úÖ File –∑–∞–ø–∏—Å—å —Å–æ–∑–¥–∞–Ω–∞: {inner_file_id}")

                            # üî• –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï: –°–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª (OCR + Chunker)
                            try:
                                chunks_created = process_any_file(
                                    file_path=inner_path,
                                    file_id=inner_file_id,
                                    db=db
                                )
                                logger.info(f"  ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {chunks_created} —á–∞–Ω–∫–æ–≤")
                                
                                # –û–±–Ω–æ–≤–ª—è–µ–º –∑–∞–ø–∏—Å—å File —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —á–∞–Ω–∫–æ–≤
                                new_file.chunks_count = chunks_created
                                db.commit()
                                
                            except Exception as ocr_err:
                                logger.error(f"  ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞: {ocr_err}")
                                chunks_created = 0

                            if case_id_to_save:
                                case_ids_map.setdefault(case_id_to_save, []).append(
                                    str(inner_file_id)
                                )

                            results.append({
                                "file_id": str(inner_file_id),
                                "filename": inner_name,
                                "chunks_created": chunks_created,
                                "case_id": case_id_to_save,
                                "status": "completed" if chunks_created > 0 else "warning",
                            })
                            zip_inner_ids.append(str(inner_file_id))

                        except Exception as e:
                            db.rollback()
                            logger.error(f"‚ùå –û—à–∏–±–∫–∞ {inner_name}: {e}")
                            results.append({
                                "file_id": str(inner_file_id),
                                "filename": inner_name,
                                "chunks_created": 0,
                                "error": str(e),
                                "status": "failed",
                            })

                # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
                shutil.rmtree(extract_dir, ignore_errors=True)
                os.remove(temp_path)
                temp_path = None
                
                results.append({
                    "file_id": None,
                    "filename": file.filename,
                    "type": "zip_summary",
                    "files_processed": len(zip_inner_ids),
                    "status": "completed",
                })
                continue

            # ========== 4. PDF ‚Äì –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–æ—Ä–º–∞—Ç ==========
            if ext == ".pdf":
                file_id = uuid.uuid4()
                try:
                    case_id_extracted = _detect_case_id_for_file(
                        file_path=temp_path,
                        filename=file.filename,
                        outer_case_id=None,
                    )

                    # –°–æ–∑–¥–∞—ë–º –∑–∞–ø–∏—Å—å File –≤ –ë–î
                    new_file = File(
                        file_id=file_id,
                        filename=file.filename,
                        case_id=case_id_extracted,
                        s3_key=f"s3://afm-originals/{file.filename}",
                        ocr_confidence=0.0,
                        chunks_count=0,
                    )
                    db.add(new_file)
                    db.commit()
                    db.refresh(new_file)
                    logger.info(f"‚úÖ File –∑–∞–ø–∏—Å—å —Å–æ–∑–¥–∞–Ω–∞: {file_id}")

                    # üî• –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï: –°–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º (OCR + Chunker)
                    chunks_created = 0
                    try:
                        chunks_created = process_any_file(
                            file_path=temp_path,
                            file_id=file_id,
                            db=db
                        )
                        logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {chunks_created} —á–∞–Ω–∫–æ–≤")
                        
                        # –û–±–Ω–æ–≤–ª—è–µ–º –∑–∞–ø–∏—Å—å
                        new_file.chunks_count = chunks_created
                        db.commit()
                        
                    except Exception as ocr_err:
                        logger.error(f"‚ùå –û—à–∏–±–∫–∞ OCR: {ocr_err}")
                        chunks_created = 0

                    if case_id_extracted:
                        case_ids_map.setdefault(case_id_extracted, []).append(
                            str(file_id)
                        )

                    results.append({
                        "file_id": str(file_id),
                        "filename": file.filename,
                        "chunks_created": chunks_created,
                        "case_id": case_id_extracted,
                        "status": "completed" if chunks_created > 0 else "warning",
                    })

                except Exception as e:
                    db.rollback()
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ PDF {file.filename}: {e}")
                    results.append({
                        "file_id": str(file_id),
                        "filename": file.filename,
                        "chunks_created": 0,
                        "error": str(e),
                        "status": "failed",
                    })
                continue

            # ========== 5. DOCX / TXT ==========
            if ext in [".docx", ".txt"]:
                file_id = uuid.uuid4()
                try:
                    case_id_extracted = _detect_case_id_for_file(
                        file_path=temp_path,
                        filename=file.filename,
                        outer_case_id=None,
                    )

                    # –°–æ–∑–¥–∞—ë–º –∑–∞–ø–∏—Å—å File –≤ –ë–î
                    new_file = File(
                        file_id=file_id,
                        filename=file.filename,
                        case_id=case_id_extracted,
                        s3_key=f"s3://afm-originals/{file.filename}",
                        ocr_confidence=1.0,  # ‚Üê DOCX/TXT —É–∂–µ —Ç–µ–∫—Å—Ç, OCR –Ω–µ –Ω—É–∂–µ–Ω
                        chunks_count=0,
                    )
                    db.add(new_file)
                    db.commit()
                    db.refresh(new_file)
                    logger.info(f"‚úÖ File –∑–∞–ø–∏—Å—å —Å–æ–∑–¥–∞–Ω–∞: {file_id}")

                    # üî• –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï: –°–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
                    chunks_created = 0
                    try:
                        chunks_created = process_any_file(
                            file_path=temp_path,
                            file_id=file_id,
                            db=db
                        )
                        logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {chunks_created} —á–∞–Ω–∫–æ–≤")
                        
                        # –û–±–Ω–æ–≤–ª—è–µ–º –∑–∞–ø–∏—Å—å
                        new_file.chunks_count = chunks_created
                        db.commit()
                        
                    except Exception as ocr_err:
                        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {ocr_err}")
                        chunks_created = 0

                    if case_id_extracted:
                        case_ids_map.setdefault(case_id_extracted, []).append(
                            str(file_id)
                        )

                    results.append({
                        "file_id": str(file_id),
                        "filename": file.filename,
                        "chunks_created": chunks_created,
                        "case_id": case_id_extracted,
                        "status": "completed" if chunks_created > 0 else "warning",
                    })

                except Exception as e:
                    db.rollback()
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ {file.filename}: {e}")
                    results.append({
                        "file_id": str(file_id),
                        "filename": file.filename,
                        "chunks_created": 0,
                        "error": str(e),
                        "status": "failed",
                    })
                continue

            # ========== 6. –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç ==========
            results.append({
                "file_id": None,
                "filename": file.filename,
                "chunks_created": 0,
                "error": f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {ext}",
                "status": "failed",
            })

        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}", exc_info=True)
            results.append({
                "file_id": None,
                "filename": file.filename,
                "error": str(e),
                "chunks_created": 0,
                "status": "failed",
            })

        finally:
            # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            if temp_path and os.path.exists(temp_path):
                try:
                    os.remove(temp_path)
                except Exception:
                    pass

    # ========== –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç ==========
    successful_files = sum(1 for r in results if r["status"] == "completed")
    warning_files = sum(1 for r in results if r["status"] == "warning")
    failed_files = sum(1 for r in results if r["status"] == "failed")

    return {
        "uploaded_files": len([r for r in results if r.get("file_id")]),
        "successful": successful_files,
        "warnings": warning_files,
        "failed": failed_files,
        "results": results,
        "case_ids": case_ids_map if case_ids_map else None,
    }
