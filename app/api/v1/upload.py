# app/api/v1/upload.py
import uuid
import os
import tempfile
import zipfile
import shutil
import logging
import re
from typing import List, Optional, Dict

from fastapi import (
    APIRouter,
    Depends,
    UploadFile,
    File as FastAPIFile,
    HTTPException,
)
from sqlalchemy.orm import Session

from app.db import get_db
from app.db.models import File
from app.services.parser import extract_text_from_file
from app.utils.config import settings

# Celery-—Ç–∞—Å–∫ —Ñ–æ–Ω–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ (OCR + Chunker)
from app.tasks.ingest import process_file_task

# ============================================================
# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
# ============================================================
MAX_FILE_SIZE_MB = getattr(settings, "MAX_FILE_SIZE_MB", 100)
MAX_FILE_SIZE_BYTES = MAX_FILE_SIZE_MB * 1024 * 1024

# –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è, –≥–¥–µ –±—É–¥—É—Ç –ª–µ–∂–∞—Ç—å —Ñ–∞–π–ª—ã –¥–ª—è —Ñ–æ–Ω–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
DEFAULT_INGEST_DIR = os.path.join(tempfile.gettempdir(), "afm_ingest")
INGEST_DIR = getattr(settings, "INGEST_DIR", DEFAULT_INGEST_DIR)
os.makedirs(INGEST_DIR, exist_ok=True)

router = APIRouter(prefix="/upload", tags=["Upload"])
logger = logging.getLogger(__name__)


# ============================================================
# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
# ============================================================

def _extract_case_id_from_name(name: str) -> Optional[str]:
    """
    –ü—ã—Ç–∞–µ—Ç—Å—è –∏–∑–≤–ª–µ—á—å –Ω–æ–º–µ—Ä –ï–†–î–† / –¥–µ–ª–∞ –∏–∑ –ò–ú–ï–ù–ò —Ñ–∞–π–ª–∞ / –∞—Ä—Ö–∏–≤–∞.
    –ò—â–µ–º 15 –ø–æ–¥—Ä—è–¥ –∏–¥—É—â–∏—Ö —Ü–∏—Ñ—Ä.
    """
    if not name:
        return None
    m = re.search(r"(\d{15})", name)
    return m.group(1) if m else None


def _extract_case_id_from_text(text: str) -> Optional[str]:
    """
    –ü—ã—Ç–∞–µ—Ç—Å—è –∏–∑–≤–ª–µ—á—å –Ω–æ–º–µ—Ä –ï–†–î–† / –¥–µ–ª–∞ –∏–∑ –¢–ï–ö–°–¢–ê –¥–æ–∫—É–º–µ–Ω—Ç–∞.
    """
    if not text:
        return None
    m = re.search(r"(\d{15})", text)
    return m.group(1) if m else None


def _detect_case_id_for_file(
    file_path: str,
    filename: str,
    outer_case_id: Optional[str] = None,
) -> Optional[str]:
    """
    Evidence Engine style detector:

    1) –ø—Ä–æ–±—É–µ–º –≤—ã—Ç–∞—â–∏—Ç—å case_id –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞;
    2) –µ—Å–ª–∏ DOCX/TXT ‚Äî –ø—Ä–æ–±—É–µ–º –≤—ã—Ç–∞—â–∏—Ç—å —Ç–µ–∫—Å—Ç –∏ –Ω–∞–π—Ç–∏ —Ç–∞–º;
    3) –µ—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º outer_case_id (–¥–ª—è —Ñ–∞–π–ª–æ–≤ –≤–Ω—É—Ç—Ä–∏ ZIP).
    """
    if not filename:
        filename = ""

    ext = os.path.splitext(filename)[1].lower()

    # 1) –ü–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
    case_id = _extract_case_id_from_name(filename)
    if case_id:
        logger.info(f"üîé case_id={case_id} –Ω–∞–π–¥–µ–Ω –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞: {filename}")
        return case_id

    text: str = ""

    # 2) –ü–æ —Ç–µ–∫—Å—Ç—É —Ñ–∞–π–ª–∞ ‚Äî –¢–û–õ–¨–ö–û –¥–ª—è DOCX/TXT
    if ext in [".docx", ".txt"]:
        try:
            text = extract_text_from_file(file_path) or ""
        except Exception as e:
            logger.warning(
                f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞ {filename} –¥–ª—è –ø–æ–∏—Å–∫–∞ case_id: {e}"
            )
            text = ""

    if text.strip():
        case_id_from_text = _extract_case_id_from_text(text)
        if case_id_from_text:
            logger.info(
                f"üîé case_id={case_id_from_text} –Ω–∞–π–¥–µ–Ω –≤ —Ç–µ–∫—Å—Ç–µ —Ñ–∞–π–ª–∞: {filename}"
            )
            return case_id_from_text

    # 3) Fallback ‚Äî –±–µ—Ä–µ–º case_id —Å–Ω–∞—Ä—É–∂–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∏–∑ –∏–º–µ–Ω–∏ ZIP)
    if outer_case_id:
        logger.info(
            f"‚ÑπÔ∏è –î–ª—è —Ñ–∞–π–ª–∞ {filename} –∏—Å–ø–æ–ª—å–∑—É–µ–º outer_case_id={outer_case_id} "
            f"(–ø–æ –∏–º–µ–Ω–∏/—Ç–µ–∫—Å—Ç—É –Ω–µ –Ω–∞–π–¥–µ–Ω–æ)"
        )
        return outer_case_id

    logger.info(f"‚ö†Ô∏è case_id –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∏ –≤ –∏–º–µ–Ω–∏, –Ω–∏ –≤ —Ç–µ–∫—Å—Ç–µ —Ñ–∞–π–ª–∞: {filename}")
    return None


def _validate_file_size(file: UploadFile) -> None:
    if hasattr(file, "size") and file.size:
        if file.size > MAX_FILE_SIZE_BYTES:
            raise HTTPException(
                status_code=413,
                detail=(
                    f"–§–∞–π–ª {file.filename} —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π ({file.size / 1024 / 1024:.1f} –ú–ë). "
                    f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {MAX_FILE_SIZE_MB} –ú–ë"
                ),
            )


def _store_for_ingest(src_path: str, file_id: uuid.UUID, ext: str) -> str:
    """
    –ü–µ—Ä–µ–∫–ª–∞–¥—ã–≤–∞–µ–º —Ñ–∞–π–ª –≤ –ø–æ—Å—Ç–æ—è–Ω–Ω—É—é ingest-–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é,
    —á—Ç–æ–±—ã —Ñ–æ–Ω–æ–≤—ã–π Celery-—Ç–∞—Å–∫ –º–æ–≥ —Å –Ω–∏–º —Ä–∞–±–æ—Ç–∞—Ç—å —É–∂–µ –ø–æ—Å–ª–µ –æ—Ç–≤–µ—Ç–∞ API.
    """
    os.makedirs(INGEST_DIR, exist_ok=True)
    dst_path = os.path.join(INGEST_DIR, f"{file_id}{ext}")
    # –µ—Å–ª–∏ —Ñ–∞–π–ª —É–∂–µ –µ—Å—Ç—å ‚Äî –ø–µ—Ä–µ–∑–∞–ø–∏—à–µ–º
    if os.path.exists(dst_path):
        os.remove(dst_path)
    shutil.move(src_path, dst_path)
    return dst_path


def _enqueue_ingest_job(file_id: uuid.UUID, stored_path: str, ext: str) -> None:
    """
    –ö–∏–¥–∞–µ–º –∑–∞–¥–∞—á—É –≤ Celery: –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ–∞–π–ª (OCR/Chunker –∏ —Ç.–¥.).
    –í–ê–ñ–ù–û: –Ω–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –∑–∞–ø–∏—Å—å File —É–∂–µ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∑–∞–∫–æ–º–º–∏—á–µ–Ω–∞ –≤ –ë–î.
    """
    try:
        process_file_task.delay(str(file_id), stored_path, ext)
        logger.info(
            f"üì® Celery ingest task –ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∞ –≤ –æ—á–µ—Ä–µ–¥—å: file_id={file_id}, path={stored_path}"
        )
    except Exception as e:
        # –î–∞–∂–µ –µ—Å–ª–∏ Celery –Ω–µ –∑–∞–ø—É—â–µ–Ω, –∑–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤ –Ω–µ –¥–æ–ª–∂–Ω–∞ –ø–∞–¥–∞—Ç—å.
        logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç–∞–≤–∏—Ç—å ingest-—Ç–∞—Å–∫ –≤ –æ—á–µ—Ä–µ–¥—å –¥–ª—è {file_id}: {e}")


# ============================================================
# Evidence Engine INGEST v3.0
# ============================================================

@router.post("/")
async def upload_files(
    files: List[UploadFile] = FastAPIFile(...),
    db: Session = Depends(get_db),
):
    """
    Evidence Engine INGEST v3.0:

    ‚Ä¢ –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –±—ã—Å—Ç—Ä–∞—è: API —Ç–æ–ª—å–∫–æ –ø–∏—à–µ—Ç –∑–∞–ø–∏—Å–∏ File –≤ –ë–î
      –∏ –ø–µ—Ä–µ–∫–ª–∞–¥—ã–≤–∞–µ—Ç —Ñ–∞–π–ª—ã –≤ ingest-–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é.
    ‚Ä¢ –¢—è–∂—ë–ª—ã–π OCR/Chunker –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –≤ Celery (—Ñ–æ–Ω–æ–≤–æ).
    ‚Ä¢ –î–ª—è –ö–ê–ñ–î–û–ì–û —Ñ–∞–π–ª–∞:
        - File —Å–æ–∑–¥–∞—ë—Ç—Å—è –∏ —Ñ–∏–∫—Å–∏—Ä—É–µ—Ç—Å—è (db.commit) –¥–æ –∑–∞–ø—É—Å–∫–∞ Celery.
        - Celery –≤—Å–µ–≥–¥–∞ –≤–∏–¥–∏—Ç –∑–∞–ø–∏—Å—å –≤ –ë–î (–Ω–µ—Ç –æ—à–∏–±–∫–∏ "File ... –Ω–µ –Ω–∞–π–¥–µ–Ω").
    """
    results: List[dict] = []
    case_ids_map: Dict[str, List[str]] = {}

    logger.info(f"üì§ –ó–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è —Ñ–∞–π–ª–æ–≤: {len(files)}")

    for file in files:
        temp_path: Optional[str] = None

        try:
            # 1) –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞
            try:
                _validate_file_size(file)
            except HTTPException as e:
                results.append(
                    {
                        "file_id": None,
                        "filename": file.filename,
                        "chunks_created": 0,
                        "error": e.detail,
                        "status": "failed",
                    }
                )
                continue

            # 2) –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
            with tempfile.NamedTemporaryFile(
                delete=False, suffix=f"_{file.filename}"
            ) as tmp:
                temp_path = tmp.name
                content = await file.read()
                if len(content) > MAX_FILE_SIZE_BYTES:
                    raise HTTPException(
                        status_code=413,
                        detail=(
                            f"–§–∞–π–ª {file.filename} —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π. "
                            f"–ú–∞–∫—Å–∏–º—É–º: {MAX_FILE_SIZE_MB} –ú–ë"
                        ),
                    )
                tmp.write(content)

            ext = os.path.splitext(file.filename)[1].lower()
            logger.info(f"üì• –ó–∞–≥—Ä—É–∂–µ–Ω {file.filename}, —Ä–∞–∑–º–µ—Ä {len(content)} –±–∞–π—Ç")

            # ============================================================
            # 3) ZIP ‚Äì —Ä–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º, –∫–∞–∂–¥—ã–π inner-—Ñ–∞–π–ª —Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –≤ –ë–î
            #      –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ Celery –æ—Ç–¥–µ–ª—å–Ω–æ
            # ============================================================
            if ext == ".zip":
                extract_dir = tempfile.mkdtemp(prefix="unzipped_")
                try:
                    with zipfile.ZipFile(temp_path, "r") as zip_ref:
                        zip_ref.extractall(extract_dir)
                except zipfile.BadZipFile:
                    raise HTTPException(status_code=400, detail="ZIP —Ñ–∞–π–ª –ø–æ–≤—Ä–µ–∂–¥—ë–Ω")

                outer_case_id = _extract_case_id_from_name(file.filename)
                if outer_case_id:
                    logger.info(
                        f"üîé outer_case_id={outer_case_id} –Ω–∞–π–¥–µ–Ω –≤ –∏–º–µ–Ω–∏ ZIP {file.filename}"
                    )
                else:
                    logger.info(f"‚ÑπÔ∏è –í –∏–º–µ–Ω–∏ ZIP {file.filename} –Ω–æ–º–µ—Ä –¥–µ–ª–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω")

                zip_inner_ids: List[str] = []

                for root, _, inner_files in os.walk(extract_dir):
                    for inner_name in inner_files:
                        inner_path = os.path.join(root, inner_name)
                        inner_ext = os.path.splitext(inner_name)[1].lower()

                        if inner_ext not in [".pdf", ".docx", ".txt"]:
                            continue

                        inner_file_id = uuid.uuid4()

                        try:
                            # 3.1 –û–ø—Ä–µ–¥–µ–ª—è–µ–º case_id
                            case_id_to_save = _detect_case_id_for_file(
                                file_path=inner_path,
                                filename=inner_name,
                                outer_case_id=outer_case_id,
                            )

                            # 3.2 –°–æ–∑–¥–∞—ë–º –∑–∞–ø–∏—Å—å –≤ –ë–î –∏ –°–†–ê–ó–£ –∫–æ–º–º–∏—Ç–∏–º
                            new_file = File(
                                file_id=inner_file_id,
                                filename=inner_name,
                                case_id=case_id_to_save,
                                s3_key=f"s3://afm-originals/{inner_name}",
                                ocr_confidence=0.0,
                                chunks_count=0,
                            )
                            db.add(new_file)
                            db.commit()
                            db.refresh(new_file)

                            # 3.3 –ü–µ—Ä–µ–∫–ª–∞–¥—ã–≤–∞–µ–º —Ñ–∞–π–ª –≤ ingest-–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
                            stored_path = _store_for_ingest(
                                inner_path, inner_file_id, inner_ext
                            )

                            # 3.4 –ö–∏–¥–∞–µ–º ingest-–∑–∞–¥–∞—á—É –≤ Celery
                            _enqueue_ingest_job(
                                inner_file_id, stored_path, inner_ext
                            )

                            if case_id_to_save:
                                case_ids_map.setdefault(case_id_to_save, []).append(
                                    str(inner_file_id)
                                )

                            results.append(
                                {
                                    "file_id": str(inner_file_id),
                                    "filename": inner_name,
                                    "chunks_created": 0,
                                    "case_id": case_id_to_save,
                                    "s3_key": f"s3://afm-originals/{inner_name}",
                                    "status": "queued",
                                }
                            )
                            zip_inner_ids.append(str(inner_file_id))

                        except Exception as e:
                            db.rollback()
                            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ñ–∞–π–ª–∞ –≤ ZIP {inner_name}: {e}")
                            results.append(
                                {
                                    "file_id": str(inner_file_id),
                                    "filename": inner_name,
                                    "chunks_created": 0,
                                    "error": str(e),
                                    "status": "failed",
                                }
                            )

                # ZIP –∫–∞–∫ "–æ–±—ë—Ä—Ç–∫—É" —Ç–æ–∂–µ –æ—Ç—Ä–∞–∂–∞–µ–º –≤ –æ—Ç–≤–µ—Ç–µ (summary)
                results.append(
                    {
                        "file_id": None,
                        "filename": file.filename,
                        "type": "zip_summary",
                        "files_processed": len(zip_inner_ids),
                        "chunks_created": 0,
                        "case_id": outer_case_id,
                        "status": "queued",
                    }
                )

                # –∏—Å—Ö–æ–¥–Ω–∏–∫–∏ —É–∂–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω—ã –≤ INGEST_DIR, —ç—Ç—É –ø–∞–ø–∫—É –º–æ–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å
                shutil.rmtree(extract_dir, ignore_errors=True)
                # temp_path —Ç–æ–∂–µ –±–æ–ª—å—à–µ –Ω–µ –Ω—É–∂–µ–Ω
                os.remove(temp_path)
                temp_path = None
                continue

            # ============================================================
            # 4) PDF ‚Äì —Å–æ–∑–¥–∞—ë–º File, –∫–æ–º–º–∏—Ç–∏–º, –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ Celery
            # ============================================================
            if ext == ".pdf":
                file_id = uuid.uuid4()
                try:
                    case_id_extracted = _detect_case_id_for_file(
                        file_path=temp_path,
                        filename=file.filename,
                        outer_case_id=None,
                    )

                    new_file = File(
                        file_id=file_id,
                        filename=file.filename,
                        case_id=case_id_extracted,
                        s3_key=f"s3://afm-originals/{file.filename}",
                        ocr_confidence=0.0,
                        chunks_count=0,
                    )
                    db.add(new_file)
                    db.commit()
                    db.refresh(new_file)

                    # –ü–µ—Ä–µ–∫–ª–∞–¥—ã–≤–∞–µ–º PDF –≤ ingest-–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
                    stored_path = _store_for_ingest(temp_path, file_id, ext)
                    temp_path = None

                    # –ö–∏–¥–∞–µ–º —Ñ–æ–Ω–æ–≤—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
                    _enqueue_ingest_job(file_id, stored_path, ext)

                    if case_id_extracted:
                        case_ids_map.setdefault(case_id_extracted, []).append(
                            str(file_id)
                        )

                    results.append(
                        {
                            "file_id": str(file_id),
                            "filename": file.filename,
                            "chunks_created": 0,
                            "case_id": case_id_extracted,
                            "s3_key": f"s3://afm-originals/{file.filename}",
                            "status": "queued",
                        }
                    )

                except Exception as e:
                    db.rollback()
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ PDF {file.filename}: {e}")
                    results.append(
                        {
                            "file_id": str(file_id),
                            "filename": file.filename,
                            "chunks_created": 0,
                            "error": str(e),
                            "status": "failed",
                        }
                    )
                continue

            # ============================================================
            # 5) DOCX / TXT ‚Äì –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ PDF, –Ω–æ –±–µ–∑ OCR
            # ============================================================
            if ext in [".docx", ".txt"]:
                file_id = uuid.uuid4()
                try:
                    case_id_extracted = _detect_case_id_for_file(
                        file_path=temp_path,
                        filename=file.filename,
                        outer_case_id=None,
                    )

                    new_file = File(
                        file_id=file_id,
                        filename=file.filename,
                        case_id=case_id_extracted,
                        s3_key=f"s3://afm-originals/{file.filename}",
                        ocr_confidence=0.0,
                        chunks_count=0,
                    )
                    db.add(new_file)
                    db.commit()
                    db.refresh(new_file)

                    # –ü–µ—Ä–µ–∫–ª–∞–¥—ã–≤–∞–µ–º —Ñ–∞–π–ª –≤ ingest-–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
                    stored_path = _store_for_ingest(temp_path, file_id, ext)
                    temp_path = None

                    # –°—Ç–∞–≤–∏–º —Ñ–æ–Ω–æ–≤—É—é –∑–∞–¥–∞—á—É
                    _enqueue_ingest_job(file_id, stored_path, ext)

                    if case_id_extracted:
                        case_ids_map.setdefault(case_id_extracted, []).append(
                            str(file_id)
                        )

                    results.append(
                        {
                            "file_id": str(file_id),
                            "filename": file.filename,
                            "chunks_created": 0,
                            "case_id": case_id_extracted,
                            "status": "queued",
                        }
                    )

                except Exception as e:
                    db.rollback()
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {file.filename}: {e}")
                    results.append(
                        {
                            "file_id": str(file_id),
                            "filename": file.filename,
                            "chunks_created": 0,
                            "error": str(e),
                            "status": "failed",
                        }
                    )
                continue

            # ============================================================
            # 6) –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç
            # ============================================================
            results.append(
                {
                    "file_id": None,
                    "filename": file.filename,
                    "chunks_created": 0,
                    "error": f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {ext}",
                    "status": "failed",
                }
            )

        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}", exc_info=True)
            results.append(
                {
                    "file_id": None,
                    "filename": file.filename,
                    "error": str(e),
                    "chunks_created": 0,
                    "status": "failed",
                }
            )

        finally:
            # temp_path —É–¥–∞–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω –µ—â—ë –Ω–µ –±—ã–ª –ø–µ—Ä–µ–¥–∞–Ω –≤ ingest-–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            if temp_path and os.path.exists(temp_path):
                try:
                    os.remove(temp_path)
                except Exception:
                    pass

    # ============================================================
    # –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç (–≥–ª–æ–±–∞–ª—å–Ω—ã–π commit —É–∂–µ –Ω–µ –Ω—É–∂–µ–Ω ‚Äî –≤—Å—ë –ø–æ –º–µ—Å—Ç—É)
    # ============================================================
    successful_files = sum(1 for r in results if r["status"] in ("success", "queued"))
    failed_files = sum(1 for r in results if r["status"] == "failed")

    return {
        "uploaded_files": len(results),
        "successful": successful_files,
        "failed": failed_files,
        "results": results,
        "case_ids": case_ids_map or None,
    }
