# app/api/v1/upload.py
import uuid
import os
import tempfile
import zipfile
import shutil
import logging
import re
from typing import List, Optional

from fastapi import APIRouter, Depends, UploadFile, File as FastAPIFile, HTTPException
from sqlalchemy.orm import Session

from app.db import get_db
from app.db.models import File
from app.services.chunker import process_pdf_with_smart_ocr, process_text_into_chunks
from app.services.ocr_worker import extract_text_from_pdf  # OCR fallback
from app.services.retrieval import get_file_docs_for_qualifier
from app.services.agents.ai_qualifier import qualify_documents
from app.services.validation.verifier import run_full_verification
from app.services.parser import extract_text_from_file
from app.utils.config import settings

# ============================================================
# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
# ============================================================
MAX_FILE_SIZE_MB = getattr(settings, 'MAX_FILE_SIZE_MB', 100)
MAX_FILE_SIZE_BYTES = MAX_FILE_SIZE_MB * 1024 * 1024

router = APIRouter(prefix="/upload", tags=["Upload"])
logger = logging.getLogger(__name__)

# ============================================================
# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
# ============================================================

def _extract_case_id_from_name(name: str) -> Optional[str]:
    if not name:
        return None
    m = re.search(r"(\d{15})", name)
    return m.group(1) if m else None


def _validate_file_size(file: UploadFile) -> None:
    if hasattr(file, 'size') and file.size:
        if file.size > MAX_FILE_SIZE_BYTES:
            raise HTTPException(
                status_code=413,
                detail=f"–§–∞–π–ª {file.filename} —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π ({file.size / 1024 / 1024:.1f} –ú–ë). "
                       f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {MAX_FILE_SIZE_MB} –ú–ë"
            )

# ============================================================
# –û—Å–Ω–æ–≤–Ω–æ–π endpoint
# ============================================================

@router.post("/")
async def upload_files(
    files: List[UploadFile] = FastAPIFile(...),
    db: Session = Depends(get_db),
):
    results: List[dict] = []
    case_ids_map: dict = {}

    logger.info(f"üì§ –ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ {len(files)} —Ñ–∞–π–ª–æ–≤")

    for file in files:
        temp_path = None
        try:
            # 1Ô∏è‚É£ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞
            try:
                _validate_file_size(file)
            except HTTPException as e:
                logger.warning(f"‚ö†Ô∏è {e.detail}")
                results.append({
                    "file_id": None,
                    "filename": file.filename,
                    "chunks_created": 0,
                    "error": e.detail,
                    "status": "failed"
                })
                continue

            # 2Ô∏è‚É£ –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
            with tempfile.NamedTemporaryFile(delete=False, suffix=f"_{file.filename}") as tmp:
                temp_path = tmp.name
                content = await file.read()
                if len(content) > MAX_FILE_SIZE_BYTES:
                    raise HTTPException(
                        status_code=413,
                        detail=f"–§–∞–π–ª {file.filename} —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π. –ú–∞–∫—Å–∏–º—É–º: {MAX_FILE_SIZE_MB} –ú–ë"
                    )
                tmp.write(content)

            logger.info(f"üì• –ó–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª: {file.filename} ({len(content) / 1024:.1f} –ö–ë)")
            ext = os.path.splitext(file.filename)[1].lower()

            # ============================================================
            # 3Ô∏è‚É£ ZIP
            # ============================================================
            if ext == ".zip":
                extract_dir = tempfile.mkdtemp(prefix="unzipped_")
                try:
                    with zipfile.ZipFile(temp_path, "r") as zip_ref:
                        zip_ref.extractall(extract_dir)
                    logger.info(f"üì¶ –†–∞—Å–ø–∞–∫–æ–≤–∞–Ω –∞—Ä—Ö–∏–≤ {file.filename} ‚Üí {extract_dir}")
                except zipfile.BadZipFile:
                    raise HTTPException(
                        status_code=400,
                        detail=f"–§–∞–π–ª {file.filename} –ø–æ–≤—Ä–µ–∂–¥—ë–Ω –∏–ª–∏ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è ZIP –∞—Ä—Ö–∏–≤–æ–º"
                    )

                outer_case_id = _extract_case_id_from_name(file.filename)
                zip_inner_ids: List[str] = []

                for root, _, inner_files in os.walk(extract_dir):
                    for inner_name in inner_files:
                        inner_path = os.path.join(root, inner_name)
                        inner_ext = os.path.splitext(inner_name)[1].lower()
                        if inner_ext not in [".pdf", ".docx", ".txt"]:
                            logger.debug(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫ —Ñ–∞–π–ª–∞: {inner_name}")
                            continue

                        inner_file_id = uuid.uuid4()
                        chunks_created = 0
                        with db.begin_nested():
                            try:
                                detected_case = _extract_case_id_from_name(inner_name)
                                case_id_to_save = detected_case or outer_case_id
                                new_file = File(
                                    file_id=inner_file_id,
                                    filename=inner_name,
                                    case_id=case_id_to_save,
                                    s3_key=f"s3://afm-originals/{inner_name}",
                                    ocr_confidence=0.9,
                                )
                                db.add(new_file)
                                db.flush()

                                if inner_ext == ".pdf":
                                    chunks_created = process_pdf_with_smart_ocr(inner_path, inner_file_id, db)
                                else:
                                    text = extract_text_from_file(inner_path) or ""
                                    if not text.strip():
                                        raise ValueError("–ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç")
                                    chunks_created = process_text_into_chunks(inner_file_id, text, db)

                                if hasattr(new_file, "chunks_count"):
                                    new_file.chunks_count = chunks_created

                                logger.info(f"‚úÖ {inner_name}: {chunks_created} —á–∞–Ω–∫–æ–≤")

                                if case_id_to_save:
                                    case_ids_map.setdefault(case_id_to_save, []).append(str(inner_file_id))

                                results.append({
                                    "file_id": str(inner_file_id),
                                    "filename": inner_name,
                                    "chunks_created": chunks_created,
                                    "s3_key": f"s3://afm-originals/{inner_name}",
                                    "case_id": case_id_to_save,
                                    "status": "success"
                                })
                                zip_inner_ids.append(str(inner_file_id))
                            except ValueError as ve:
                                db.rollback()
                                logger.debug(f"‚è≠Ô∏è {inner_name} –ø—Ä–æ–ø—É—â–µ–Ω: {ve}")
                            except Exception as e:
                                db.rollback()
                                err = f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {inner_name}: {str(e)}"
                                logger.error(f"‚ùå {err}")
                                results.append({
                                    "file_id": str(inner_file_id),
                                    "filename": inner_name,
                                    "chunks_created": 0,
                                    "error": err,
                                    "status": "failed"
                                })

                # –∏—Ç–æ–≥ –ø–æ ZIP
                try:
                    zip_chunks_total = sum(
                        r.get("chunks_created", 0)
                        for r in results if r.get("file_id") in zip_inner_ids
                    )
                    results.append({
                        "file_id": None,
                        "filename": file.filename,
                        "type": "zip_summary",
                        "chunks_created": zip_chunks_total,
                        "files_processed": len(zip_inner_ids),
                        "case_id": outer_case_id,
                        "status": "success"
                    })
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ ZIP {file.filename}: {e}")
                finally:
                    shutil.rmtree(extract_dir, ignore_errors=True)

                continue

            # ============================================================
            # 4Ô∏è‚É£ PDF
            # ============================================================
            if ext == ".pdf":
                file_id = uuid.uuid4()
                chunks_created = 0
                with db.begin_nested():
                    try:
                        single_case_id = _extract_case_id_from_name(file.filename)
                        new_file = File(
                            file_id=file_id,
                            filename=file.filename,
                            case_id=single_case_id,
                            s3_key=f"s3://afm-originals/{file.filename}",
                            ocr_confidence=0.9,
                        )
                        db.add(new_file)
                        db.flush()
                        chunks_created = process_pdf_with_smart_ocr(temp_path, file_id, db)
                        if hasattr(new_file, "chunks_count"):
                            new_file.chunks_count = chunks_created
                        logger.info(f"‚úÖ PDF {file.filename}: {chunks_created} —á–∞–Ω–∫–æ–≤")

                        if single_case_id:
                            case_ids_map.setdefault(single_case_id, []).append(str(file_id))

                        results.append({
                            "file_id": str(file_id),
                            "filename": file.filename,
                            "chunks_created": chunks_created,
                            "s3_key": f"s3://afm-originals/{file.filename}",
                            "case_id": single_case_id,
                            "status": "success"
                        })
                    except Exception as e:
                        db.rollback()
                        err = f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ PDF {file.filename}: {str(e)}"
                        logger.error(f"‚ùå {err}")
                        results.append({
                            "file_id": str(file_id),
                            "filename": file.filename,
                            "chunks_created": 0,
                            "error": err,
                            "status": "failed"
                        })
                continue

            # ============================================================
            # 5Ô∏è‚É£ DOCX/TXT
            # ============================================================
            if ext in [".docx", ".txt"]:
                file_id = uuid.uuid4()
                chunks_created = 0
                with db.begin_nested():
                    try:
                        single_case_id = _extract_case_id_from_name(file.filename)
                        new_file = File(
                            file_id=file_id,
                            filename=file.filename,
                            case_id=single_case_id,
                            s3_key=f"s3://afm-originals/{file.filename}",
                            ocr_confidence=0.95,
                        )
                        db.add(new_file)
                        db.flush()
                        text = extract_text_from_file(temp_path) or ""
                        if not text.strip():
                            raise ValueError("–ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç")
                        chunks_created = process_text_into_chunks(file_id, text, db)
                        if hasattr(new_file, "chunks_count"):
                            new_file.chunks_count = chunks_created
                        logger.info(f"‚úÖ {ext.upper()} {file.filename}: {chunks_created} —á–∞–Ω–∫–æ–≤")

                        if single_case_id:
                            case_ids_map.setdefault(single_case_id, []).append(str(file_id))

                        results.append({
                            "file_id": str(file_id),
                            "filename": file.filename,
                            "chunks_created": chunks_created,
                            "s3_key": f"s3://afm-originals/{file.filename}",
                            "case_id": single_case_id,
                            "status": "success"
                        })
                    except ValueError as ve:
                        db.rollback()
                        logger.debug(f"‚è≠Ô∏è {file.filename} –ø—Ä–æ–ø—É—â–µ–Ω: {ve}")
                    except Exception as e:
                        db.rollback()
                        err = f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {file.filename}: {str(e)}"
                        logger.error(f"‚ùå {err}")
                        results.append({
                            "file_id": str(file_id),
                            "filename": file.filename,
                            "chunks_created": 0,
                            "error": err,
                            "status": "failed"
                        })
                continue

            # ============================================================
            # 6Ô∏è‚É£ –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç
            # ============================================================
            logger.warning(f"‚õî –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {ext}")
            results.append({
                "file_id": None,
                "filename": file.filename,
                "chunks_created": 0,
                "error": f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {ext}",
                "status": "failed"
            })

        except HTTPException:
            raise
        except Exception as e:
            err = f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file.filename}: {str(e)}"
            logger.error(f"‚ùå {err}", exc_info=True)
            results.append({
                "file_id": None,
                "filename": file.filename,
                "chunks_created": 0,
                "error": err,
                "status": "failed"
            })
        finally:
            if temp_path and os.path.exists(temp_path):
                try:
                    os.remove(temp_path)
                    logger.debug(f"üßπ –£–¥–∞–ª—ë–Ω –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª: {temp_path}")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å {temp_path}: {e}")

    # ‚úÖ –ï–¥–∏–Ω—ã–π –æ–±—â–∏–π –∫–æ–º–º–∏—Ç –ø–æ—Å–ª–µ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
    try:
        db.commit()
    except Exception as e:
        db.rollback()
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∫–æ–º–º–∏—Ç–∞: {e}", exc_info=True)

    # ============================================================
    # 7Ô∏è‚É£ –ö–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏—è
    # ============================================================
    qualification_results = []
    if case_ids_map:
        logger.info(f"ü§ñ –ó–∞–ø—É—Å–∫ –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–ª—è {len(case_ids_map)} –¥–µ–ª")
        for case_id, file_ids in case_ids_map.items():
            try:
                logger.info(f"üìã –ö–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–µ–ª–∞ {case_id} ({len(file_ids)} —Ñ–∞–π–ª–æ–≤)")
                docs = get_file_docs_for_qualifier(db, file_ids=file_ids, case_id=case_id)
                if not docs:
                    logger.warning(f"‚ö†Ô∏è –ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –¥–µ–ª–∞ {case_id}")
                    continue

                qualifier = qualify_documents(
                    case_id=case_id,
                    docs=docs,
                    city="–≥. –ü–∞–≤–ª–æ–¥–∞—Ä",
                    investigator_line="–°–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å –°–≠–† –î–≠–† –ø–æ –ü–∞–≤–ª–æ–¥–∞—Ä—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏",
                )
                verification = run_full_verification(qualifier)
                qualification_results.append({
                    "case_id": case_id,
                    "files_analyzed": len(file_ids),
                    "qualifier": qualifier,
                    "verification": verification,
                    "draft_postanovlenie": qualifier.get("final_postanovlenie")
                        if isinstance(qualifier, dict) else None,
                    "status": "success"
                })
                logger.info(f"‚úÖ –ö–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏—è {case_id} –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {qualifier.get('verdict', 'N/A')}")
            except Exception as e:
                err = f"–û—à–∏–±–∫–∞ –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–µ–ª–∞ {case_id}: {str(e)}"
                logger.error(f"‚ùå {err}")
                qualification_results.append({
                    "case_id": case_id,
                    "files_analyzed": len(file_ids),
                    "error": err,
                    "status": "failed"
                })

    # ============================================================
    # 8Ô∏è‚É£ –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
    # ============================================================
    successful_files = sum(1 for r in results if r.get("status") == "success")
    failed_files = sum(1 for r in results if r.get("status") == "failed")

    logger.info(f"üìä –ò—Ç–æ–≥: —É—Å–ø–µ—à–Ω–æ={successful_files}, –æ—à–∏–±–æ–∫={failed_files}, –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–π={len(qualification_results)}")

    return {
        "uploaded_files": len(results),
        "successful": successful_files,
        "failed": failed_files,
        "results": results,
        "qualifications": qualification_results or None,
    }
