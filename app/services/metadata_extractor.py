import logging
import re
from typing import Dict, Any, List, Optional

logger = logging.getLogger(__name__)


# ==========================
# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò
# ==========================

def _detect_language(text: str) -> Optional[str]:
    """–û—á–µ–Ω—å –≥—Ä—É–±–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞: ru / kk / en / mixed."""
    if not text:
        return None

    cyrillic = sum(1 for ch in text if "–ê" <= ch <= "—è" or ch in "–Å—ë–Ü—ñ“í“ì“ö“õ“¢“£“∞“±“Æ“Ø“∫“ª”®”©”ò”ô")
    latin = sum(1 for ch in text if "A" <= ch <= "z")

    if cyrillic > 0 and latin == 0:
        # —Ä—É—Å / –∫–∞–∑ ‚Äî –Ω–µ –¥–µ–ª–∏–º, –ø—Ä–æ—Å—Ç–æ 'cyrillic'
        return "cyrillic"
    if latin > 0 and cyrillic == 0:
        return "latin"
    if cyrillic > 0 and latin > 0:
        return "mixed"

    return None


def _extract_first_match(patterns: List[re.Pattern], text: str) -> Optional[str]:
    for p in patterns:
        m = p.search(text)
        if m:
            return m.group(1)
    return None


def _extract_all_matches(pattern: re.Pattern, text: str, max_items: int = 10) -> List[str]:
    results = []
    for m in pattern.finditer(text):
        val = m.group(0)
        if val not in results:
            results.append(val)
        if len(results) >= max_items:
            break
    return results


# ==========================
# –û–°–ù–û–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø
# ==========================

def extract_metadata(
    filename: str,
    file_bytes: bytes,
    text_hint: Optional[str] = None,
    sample_size: int = 8192,
) -> Dict[str, Any]:
    """
    –õ—ë–≥–∫–∏–π rule-based extractor –¥–ª—è –ø–µ—Ä–≤–∏—á–Ω—ã—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞.

    –†–∞–±–æ—Ç–∞–µ—Ç:
    - –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
    - –ø–æ –ø–µ—Ä–≤—ã–º –±–∞–π—Ç–∞–º —Ñ–∞–π–ª–∞ (–µ—Å–ª–∏ —ç—Ç–æ —Ç–µ–∫—Å—Ç)
    - –ø–æ text_hint (–µ—Å–ª–∏ –µ–≥–æ –ø–µ—Ä–µ–¥–∞–ª–∏ –≤—ã—à–µ –ø–æ –ø–∞–π–ø–ª–∞–π–Ω—É)

    –ù–ï –≤—ã–∑—ã–≤–∞–µ—Ç LLM, –Ω–µ –¥–µ–ª–∞–µ—Ç OCR.
    """
    metadata: Dict[str, Any] = {}

    # -----------------------------------------
    # 1) –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
    # -----------------------------------------
    fn_lower = filename.lower()

    metadata["filename"] = filename

    # –ü—Ä–æ—Å—Ç–µ–π—à–µ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ case_id / –±–æ–ª—å—à–∏—Ö –Ω–æ–º–µ—Ä–æ–≤ –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
    case_id_match = re.search(r"(\d{9,})", fn_lower)
    if case_id_match:
        metadata.setdefault("possible_numbers", [])
        num = case_id_match.group(1)
        if num not in metadata["possible_numbers"]:
            metadata["possible_numbers"].append(num)

    # -----------------------------------------
    # 2) –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    # -----------------------------------------
    text_sources: List[str] = []

    if text_hint:
        text_sources.append(text_hint)

    # –ø—ã—Ç–∞–µ–º—Å—è —Å–¥–µ–ª–∞—Ç—å —Å–µ–º–ø–ª —Ç–µ–∫—Å—Ç–∞ –∏–∑ –±–∞–π—Ç (–Ω–∞ —Å–ª—É—á–∞–π —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ / PDF —Å —Ç–µ–∫—Å—Ç–æ–º)
    if file_bytes:
        try:
            sample = file_bytes[:sample_size].decode("utf-8", errors="ignore")
            if sample.strip():
                text_sources.append(sample)
        except Exception:
            # –µ—Å–ª–∏ –Ω–µ –¥–µ–∫–æ–¥–∏—Ç—Å—è ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            pass

    # –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–µ—Ç –≤–æ–æ–±—â–µ ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ filename / possible_numbers
    if not text_sources:
        return metadata

    # –æ–±—ä–µ–¥–∏–Ω—è–µ–º –≤ –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö regex
    merged_text = "\n".join(text_sources)

    # -----------------------------------------
    # 3) –Ø–∑—ã–∫
    # -----------------------------------------
    lang = _detect_language(merged_text)
    if lang:
        metadata["language"] = lang

    # -----------------------------------------
    # 4) –î–∞—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞
    # -----------------------------------------
    date_patterns = [
        re.compile(r"\b(\d{2}[./-]\d{2}[./-]\d{4})\b"),  # 12.03.2024 / 12-03-2024
        re.compile(
            r"\b(\d{1,2}\s+"
            r"(—è–Ω–≤–∞—Ä—è|—Ñ–µ–≤—Ä–∞–ª—è|–º–∞—Ä—Ç–∞|–∞–ø—Ä–µ–ª—è|–º–∞—è|–∏—é–Ω—è|–∏—é–ª—è|–∞–≤–≥—É—Å—Ç–∞|—Å–µ–Ω—Ç—è–±—Ä—è|–æ–∫—Ç—è–±—Ä—è|–Ω–æ—è–±—Ä—è|–¥–µ–∫–∞–±—Ä—è)"
            r"\s+\d{4}\s*–≥(?:–æ–¥–∞)?)",
            re.IGNORECASE,
        ),
    ]

    doc_date = _extract_first_match(date_patterns, merged_text)
    if doc_date:
        metadata["document_date"] = doc_date

    # -----------------------------------------
    # 5) –ù–æ–º–µ—Ä–∞ –ö–£–ò / –ï–†–î–† / –ø—Ä–æ—á–∏–µ
    # -----------------------------------------
    kui_patterns = [
        re.compile(r"–ö–£–ò\s*‚Ññ\s*([0-9\-]+)", re.IGNORECASE),
        re.compile(r"–ö–£–ò\s*No\.?\s*([0-9\-]+)", re.IGNORECASE),
    ]
    erdr_patterns = [
        re.compile(r"–ï–†–î–†\s*‚Ññ\s*([0-9\-]+)", re.IGNORECASE),
        re.compile(r"–ï[–†–†]–î[–†–†]\s*‚Ññ\s*([0-9\-]+)", re.IGNORECASE),
    ]
    generic_doc_num_patterns = [
        re.compile(r"‚Ññ\s*([0-9]{6,})"),
    ]

    kui_number = _extract_first_match(kui_patterns, merged_text)
    if kui_number:
        metadata["kui_number"] = kui_number

    erdr_number = _extract_first_match(erdr_patterns, merged_text)
    if erdr_number:
        metadata["erdr_number"] = erdr_number

    doc_number = _extract_first_match(generic_doc_num_patterns, merged_text)
    if doc_number and "document_number" not in metadata:
        metadata["document_number"] = doc_number

    # -----------------------------------------
    # 6) –í–æ–∑–º–æ–∂–Ω—ã–µ –§–ò–û (–æ—á–µ–Ω—å –≥—Ä—É–±–æ)
    # -----------------------------------------
    # —à–∞–±–ª–æ–Ω —Ç–∏–ø–∞ "–ò–≤–∞–Ω–æ–≤ –ò.–ò." –∏–ª–∏ "–ò–≤–∞–Ω–æ–≤ –ò–≤–∞–Ω –ò–≤–∞–Ω–æ–≤–∏—á"
    fio_pattern = re.compile(
        r"\b([–ê-–Ø–Å][–∞-—è—ë]+(?:\s+[–ê-–Ø–Å][–∞-—è—ë]+){0,2}\s*(?:[–ê-–Ø–Å]\.[–ê-–Ø–Å]\.)?)\b"
    )

    persons = _extract_all_matches(fio_pattern, merged_text, max_items=20)
    if persons:
        metadata["possible_persons"] = persons

    # -----------------------------------------
    # 7) –í–æ–∑–º–æ–∂–Ω—ã–µ —Å—É–º–º—ã (—Ç–µ–Ω–≥–µ / —Ä—É–± / $ / USDT)
    # -----------------------------------------
    amount_pattern = re.compile(
        r"\b\d{1,3}(?:[ \u00A0]\d{3})*(?:[.,]\d+)?\s*(?:—Ç–µ–Ω–≥–µ|—Ç–≥|‚Ç∏|—Ä—É–±(?:–ª–µ–π|\.?)?|‚ÇΩ|usd|\$|usdt)\b",
        re.IGNORECASE,
    )
    amounts = _extract_all_matches(amount_pattern, merged_text, max_items=20)
    if amounts:
        metadata["possible_amounts"] = amounts

    # -----------------------------------------
    # 8) –í–æ–∑–º–æ–∂–Ω—ã–µ —Å—á–µ—Ç–∞ / –∫–∞—Ä—Ç—ã / –∫–æ—à–µ–ª—å–∫–∏
    # -----------------------------------------
    # –ü—Ä–∏–º–µ—Ä—ã: KZ..., 16-20 —Ü–∏—Ñ—Ä –ø–æ–¥—Ä—è–¥, USDT –∞–¥—Ä–µ—Å–∞ (–æ—á–µ–Ω—å –≥—Ä—É–±–æ)
    account_pattern = re.compile(r"\bKZ[0-9A-Z]{10,}\b")
    card_pattern = re.compile(r"\b\d{4}[ \-]?\d{4}[ \-]?\d{4}[ \-]?\d{4}\b")

    accounts = _extract_all_matches(account_pattern, merged_text, max_items=20)
    cards = _extract_all_matches(card_pattern, merged_text, max_items=20)

    if accounts:
        metadata["possible_accounts"] = accounts
    if cards:
        metadata["possible_cards"] = cards

    # -----------------------------------------
    # 9) –¢–∏–ø–æ–≤—ã–µ –º–∞—Ä–∫–µ—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–ø–æ–¥—Å–∫–∞–∑–∫–∞ –¥–ª—è document_classifier)
    # -----------------------------------------
    markers = []
    for kw in ["–ø—Ä–æ—Ç–æ–∫–æ–ª –¥–æ–ø—Ä–æ—Å–∞", "—Ä–∞–ø–æ—Ä—Ç", "–ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ", "–∑–∞—è–≤–ª–µ–Ω–∏–µ", "–≤—ã–ø–∏—Å–∫–∞", "–¥–æ–≥–æ–≤–æ—Ä"]:
        if re.search(kw, merged_text, re.IGNORECASE):
            markers.append(kw)
    if markers:
        metadata["content_markers"] = markers

    logger.debug(f"üìë extract_metadata({filename}): {metadata}")
    return metadata
