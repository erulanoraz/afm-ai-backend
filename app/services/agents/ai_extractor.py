# app/services/agents/ai_extractor.py
"""
AI Extractor 3.1 ‚Äî —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –ø–æ–¥ ChatGPT-style RAG —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é.

–ì–ª–∞–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏:
    1. –û—á–∏—Å—Ç–∏—Ç—å —Ç–µ–∫—Å—Ç (SUPER PRE-FILTER 3.1)
    2. –†–∞–∑–±–∏—Ç—å –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    3. –ü—Ä–∏–º–µ–Ω–∏—Ç—å FactTokenizer ‚Üí –ø–æ–ª—É—á–∏—Ç—å LegalFact
    4. –°–æ–±—Ä–∞—Ç—å —Ñ–∞–∫—Ç—ã —á–µ—Ä–µ–∑ FactGraph (—Å–ª–∏—è–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤)
    5. –í–µ—Ä–Ω—É—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ LegalFact

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
    ‚Ä¢ –∞–∫–∫—É—Ä–∞—Ç–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω–æ–≥–æ –º—É—Å–æ—Ä–∞
    ‚Ä¢ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–æ –≤ —Ñ–∞–±—É–ª—É
    ‚Ä¢ —á–∏—Å—Ç—ã–µ —Ç–æ–∫–µ–Ω—ã (amount, date, person, action, ‚Ä¶)
    ‚Ä¢ —Å—Ç—Ä–æ–≥–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ LegalFact
"""

import logging
import re
from typing import List

from app.services.facts.fact_models import LegalFact
from app.services.facts.fact_tokenizer import FactTokenizer
from app.services.facts.fact_graph import FactGraph

logger = logging.getLogger(__name__)


# =====================================================================
# üî• SUPER PRE-FILTER 3.1 ‚Äì –∞–∫–∫—É—Ä–∞—Ç–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω–æ–≥–æ –º—É—Å–æ—Ä–∞
# =====================================================================

DIALOG_PATTERNS = [
    r"–≤–æ–ø—Ä–æ—Å[:\s].*",
    r"–æ—Ç–≤–µ—Ç[:\s].*",
    r"–≤–æ–ø—Ä–æ—Å —Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª[—è–π].*",
]

TECH_MATERIAL = [
    r"–¥–æ–ø—Ä–æ—Å –æ–∫–æ–Ω—á–µ–Ω.*",
    r"–ø—Ä–∏–ª–æ–∂–µ–Ω–∏[–µ—è].*",
    r"–ø—Ä–æ—Ç–æ–∫–æ–ª –¥–æ–ø—Ä–æ—Å–∞.*",
    r"–æ—Ä–¥–µ—Ä ‚Ññ.*",
    r"–¥–∞—Ç–∞ –ø–µ—á–∞—Ç–∏.*",
    r"–ø—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–æ.*",
    r"—ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç.*",
    r"–ø–æ–¥–ø–∏—Å—å –Ω–∞–ª–æ–∂–µ–Ω–∞.*",
    r"–ø–æ—è—Å–Ω–∏–ª.*",
    r"–æ–±—ä—è—Å–Ω–∏–ª.*",
]

PERSON_FORM = [
    r"—Ñ–∞–º–∏–ª–∏—è[:\s].*",
    r"–∏–º—è[:\s].*",
    r"–æ—Ç—á–µ—Å—Ç–≤–æ[:\s].*",
    r"–º–µ—Å—Ç–æ —Ä–æ–∂–¥–µ–Ω–∏—è.*",
    r"–º–µ—Å—Ç–æ –∂–∏—Ç–µ–ª—å—Å—Ç–≤–∞.*",
    r"–¥–∞—Ç–∞ —Ä–æ–∂–¥–µ–Ω–∏—è.*",
    r"–Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç.*",
    r"–≥—Ä–∞–∂–¥–∞–Ω—Å—Ç–≤.*",
]

def super_pre_filter(text: str) -> str:
    """
    –£–¥–∞–ª—è–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–π –º—É—Å–æ—Ä, –Ω–æ –ù–ï —Ç—Ä–æ–≥–∞–µ—Ç —Ñ–∞–±—É–ª—É.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç.
    """
    t = text.strip()
    if not t:
        return ""

    # –°–Ω–∞—á–∞–ª–∞ —É–¥–∞–ª—è–µ–º –∫—Ä—É–ø–Ω—ã–µ –±–ª–æ–∫–∏
    for p in DIALOG_PATTERNS + TECH_MATERIAL + PERSON_FORM:
        t = re.sub(p, "", t, flags=re.IGNORECASE)

    # –£–±–∏—Ä–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –ø—Ä–æ–±–µ–ª—ã
    t = re.sub(r"\s+", " ", t, flags=re.IGNORECASE)

    return t.strip()


# =====================================================================
# üîç –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
# =====================================================================

def split_sentences(text: str) -> List[str]:
    if not text:
        return []
    parts = re.split(r"(?<=[.!?])\s+", text)
    return [p.strip() for p in parts if len(p.strip()) > 5]


# =====================================================================
# üß† –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø EXTRACT_ALL ‚Äî –ù–û–í–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê
# =====================================================================

def extract_all(docs: List[dict]) -> List[LegalFact]:
    """
    –í—Ö–æ–¥:
        docs = [{ file_id, page, text }]
    –í—ã—Ö–æ–¥:
        List[LegalFact]
    """

    if not docs:
        logger.warning("‚ö† extract_all: docs –ø—É—Å—Ç—ã")
        return []

    tokenizer = FactTokenizer()
    graph = FactGraph()

    cleaned_docs = []

    # ---------------------------------------------------------
    # 1) PRE-FILTER
    # ---------------------------------------------------------
    for d in docs:
        file_id = d.get("file_id")
        page = d.get("page", 1)
        text = d.get("text", "") or ""

        cleaned = super_pre_filter(text)

        if not cleaned or len(cleaned) < 5:
            continue

        cleaned_docs.append({
            "file_id": file_id,
            "page": page,
            "text": cleaned,
        })

    if not cleaned_docs:
        logger.warning("‚ö† extract_all: –ø–æ—Å–ª–µ pre-filter –Ω–∏—á–µ–≥–æ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å")
        return []

    # ---------------------------------------------------------
    # 2) TOKENIZATION
    # ---------------------------------------------------------
    logger.info(f"üü¶ FactTokenizer: –≤—Ö–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ = {len(cleaned_docs)}")

    tokenized_facts = tokenizer.tokenize(cleaned_docs)

    logger.info(f"üü© FactTokenizer: –∏–∑–≤–ª–µ—á–µ–Ω–æ LegalFacts = {len(tokenized_facts)}")

    if not tokenized_facts:
        return []

    # ---------------------------------------------------------
    # 3) FACT GRAPH MERGE
    # ---------------------------------------------------------
    merged_facts = graph.build(tokenized_facts)

    logger.info(f"üüß FactGraph: –ø–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è = {len(merged_facts)} —Ñ–∞–∫—Ç–æ–≤")

    return merged_facts
