# app/services/agents/ai_qualifier.py
from __future__ import annotations

import logging
import uuid
import json
import re
from typing import List, Optional, Dict, Any
from datetime import datetime

from app.services.llm_client import LLMClient
from app.services.facts.fact_models import LegalFact
from app.services.facts.fact_tokenizer import FactTokenizer
from app.services.facts.fact_graph import FactGraph
from app.services.facts.fact_filter import FactFilter
from app.services.rag_router import RAGRouter
from app.services.validation.verifier import (
    run_full_verification,
    verify_sentence_token_alignment,
)
from app.services.agents import prompts
from app.services.agents.crime_classifier import (
    classify_by_tokens,
    format_classification_debug,
)
from app.utils.sentence_splitter import split_into_sentences
from app.utils.utils_v4 import validate_docs

logger = logging.getLogger(__name__)

llm = LLMClient()

MODEL_VERSION = "qualifier-llm-4.5.0"


# ============================================================
# ðŸ§  Ð’ÑÐ¿Ð¾Ð¼Ð¾Ð³Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ LLM
# ============================================================

def ask_llm(system_prompt: str, user_prompt: str) -> str:
    """
    ÐžÐ±Ñ‘Ñ€Ñ‚ÐºÐ° Ð½Ð°Ð´ LLMClient Ñ Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ Ð¸ Ð·Ð°Ñ‰Ð¸Ñ‚Ð¾Ð¹ Ð¾Ñ‚ Ð¿Ð°Ð´ÐµÐ½Ð¸Ð¹.
    """
    try:
        resp = llm.chat(
            [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ]
        )
        if resp is None:
            logger.error("LLM ERROR: Ð¾Ñ‚Ð²ÐµÑ‚ None")
            return "[LLM_ERROR]"
        if isinstance(resp, dict):
            # Ð•ÑÐ»Ð¸ LLMClient Ð²ÐµÑ€Ð½ÑƒÐ» dict (ÑÑ‚Ð¸Ð»ÑŒ OpenAI), Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ Ð²Ñ‹Ñ‚Ð°Ñ‰Ð¸Ñ‚ÑŒ Ñ‚ÐµÐºÑÑ‚
            try:
                content = resp["choices"][0]["message"]["content"]
                return (content or "").strip()
            except Exception:
                logger.error(f"LLM ERROR: Ð½ÐµÐ¾Ð¶Ð¸Ð´Ð°Ð½Ð½Ñ‹Ð¹ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ dict-Ð¾Ñ‚Ð²ÐµÑ‚Ð°: {resp}")
                return "[LLM_ERROR]"
        return str(resp).strip()
    except Exception as e:
        logger.error(f"LLM ERROR: {e}")
        return "[LLM_ERROR]"


# ============================================================
# ðŸ”§ Ð£ÑÑ‚Ð¾Ð¹Ñ‡Ð¸Ð²Ñ‹Ð¹ JSON-Ð¿Ð°Ñ€ÑÐµÑ€ (Ñ Ð°Ð²Ñ‚Ð¾-Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸ÐµÐ¼)
# ============================================================

def safe_json_loads(raw: str) -> Optional[dict]:
    """
    JSON Recovery Layer â€” AI_Qualifier 4.5
    Ð˜ÑÐ¿Ñ€Ð°Ð²Ð»ÑÐµÑ‚ Ñ‡Ð°ÑÑ‚Ð¸Ñ‡Ð½Ð¾ ÑÐ»Ð¾Ð¼Ð°Ð½Ð½Ñ‹Ðµ JSON-ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñ‹ Ð¾Ñ‚ LLM.

    ÐŸÐ¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÑ‚:
    - ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ðµ ```json ... ``` Ð¾Ð±Ð¾Ð»Ð¾Ñ‡ÐµÐº;
    - ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ðµ Ð»Ð¸ÑˆÐ½Ð¸Ñ… Ð·Ð°Ð¿ÑÑ‚Ñ‹Ñ… Ð¿ÐµÑ€ÐµÐ´ ] Ð¸ };
    - Ð´Ð¾Ð±Ð¸Ð²Ð°Ð½Ð¸Ðµ Ð½ÐµÐ´Ð¾ÑÑ‚Ð°ÑŽÑ‰Ð¸Ñ… ÑÐºÐ¾Ð±Ð¾Ðº;
    - Ð¿Ð¾Ð¸ÑÐº Ð¿ÐµÑ€Ð²Ð¾Ð¹ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ð¾Ð¹ { ... } ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñ‹ Ð²Ð½ÑƒÑ‚Ñ€Ð¸ Ñ‚ÐµÐºÑÑ‚Ð°.
    """
    if not raw:
        return None

    cleaned = raw.strip()

    # 1) ÑƒÐ´Ð°Ð»ÑÐµÐ¼ markdown-Ð¾Ð±Ð¾Ð»Ð¾Ñ‡ÐºÑƒ ```json ... ```
    cleaned = re.sub(r"```[a-zA-Z0-9]*", "", cleaned).strip("` \n\r\t")

    # 2) ÑƒÐ´Ð°Ð»ÑÐµÐ¼ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾Ðµ ÑÐ»Ð¾Ð²Ð¾ "json" Ð² Ð½Ð°Ñ‡Ð°Ð»Ðµ
    cleaned = re.sub(r"^json\s*", "", cleaned, flags=re.IGNORECASE).strip()

    # 3) ÑƒÐ±Ð¸Ñ€Ð°ÐµÐ¼ Ð²Ð¸ÑÑÑ‰Ð¸Ðµ Ð·Ð°Ð¿ÑÑ‚Ñ‹Ðµ Ð¿ÐµÑ€ÐµÐ´ Ð·Ð°ÐºÑ€Ñ‹Ð²Ð°ÑŽÑ‰Ð¸Ð¼Ð¸ ÑÐºÐ¾Ð±ÐºÐ°Ð¼Ð¸
    cleaned = re.sub(r",\s*]", "]", cleaned)
    cleaned = re.sub(r",\s*}", "}", cleaned)

    # 4) Ð´Ð¾Ð±Ð¸Ð²Ð°ÐµÐ¼ Ð½ÐµÐ´Ð¾ÑÑ‚Ð°ÑŽÑ‰Ð¸Ðµ Ñ„Ð¸Ð³ÑƒÑ€Ð½Ñ‹Ðµ ÑÐºÐ¾Ð±ÐºÐ¸
    open_braces = cleaned.count("{")
    close_braces = cleaned.count("}")
    if open_braces > close_braces:
        cleaned += "}" * (open_braces - close_braces)

    # 5) Ð´Ð¾Ð±Ð¸Ð²Ð°ÐµÐ¼ Ð½ÐµÐ´Ð¾ÑÑ‚Ð°ÑŽÑ‰Ð¸Ðµ ÐºÐ²Ð°Ð´Ñ€Ð°Ñ‚Ð½Ñ‹Ðµ ÑÐºÐ¾Ð±ÐºÐ¸
    open_arr = cleaned.count("[")
    close_arr = cleaned.count("]")
    if open_arr > close_arr:
        cleaned += "]" * (open_arr - close_arr)

    # 6) Ð¿ÐµÑ€Ð²Ð°Ñ Ð¿Ð¾Ð¿Ñ‹Ñ‚ÐºÐ° parse
    try:
        return json.loads(cleaned)
    except Exception:
        pass

    # 7) fallback: Ð²Ñ‹Ñ‚Ð°Ñ‰Ð¸Ñ‚ÑŒ Ð¿ÐµÑ€Ð²ÑƒÑŽ {...} ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ
    m = re.search(r"\{.*\}", cleaned, flags=re.DOTALL)
    if m:
        try:
            return json.loads(m.group(0))
        except Exception:
            return None

    return None


# ============================================================
# ðŸ§  Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ token_id
# ============================================================

def _extract_token_ids_from_fact(fact: LegalFact) -> List[str]:
    """
    Ð£Ð½Ð¸Ñ„Ð¸Ñ†Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ðµ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ token_id/token_ids Ð¸Ð· LegalFact,
    Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ Ð»Ð¾Ð²Ð¸Ñ‚ÑŒ 'method' object is not iterable.
    """
    token_ids: List[str] = []

    # Ð’Ð°Ñ€Ð¸Ð°Ð½Ñ‚ 1: ÐµÐ´Ð¸Ð½ÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹ token_id
    single = getattr(fact, "token_id", None)
    if isinstance(single, str) and single:
        token_ids.append(single)

    # Ð’Ð°Ñ€Ð¸Ð°Ð½Ñ‚ 2: Ð¿Ð¾Ð»Ðµ Ð¸Ð»Ð¸ Ð¼ÐµÑ‚Ð¾Ð´ token_ids
    attr = getattr(fact, "token_ids", None)
    if attr:
        try:
            value = attr() if callable(attr) else attr
            if isinstance(value, (list, tuple, set)):
                for v in value:
                    if isinstance(v, str) and v:
                        token_ids.append(v)
        except Exception as e:
            logger.warning(
                f"âš  ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¸Ð·Ð²Ð»ÐµÑ‡ÑŒ token_ids Ð¸Ð· Ñ„Ð°ÐºÑ‚Ð° {getattr(fact, 'id', None)}: {e}"
            )

    # ÑƒÐ±Ð¸Ñ€Ð°ÐµÐ¼ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ñ‹
    return list(sorted(set(token_ids)))


# ============================================================
# ðŸ”§ Auto-clean routed facts (person-only, Ð¼ÑƒÑÐ¾Ñ€Ð½Ñ‹Ðµ Ð¸Ð¼ÐµÐ½Ð°)
# ============================================================

_BAD_PERSON_TOKENS = {
    "Ð¿Ð¾ÑÐ»Ðµ",
    "ÐºÑ€Ð¾Ð¼Ðµ",
    "Ð´Ð°Ð»ÐµÐµ",
    "Ð½Ðµ",
    "Ð½ÐµÑ‚",
    "Ð²Ñ‹",
    "Ð¾",
    "Ð°",
    "Ð´Ð»Ñ",
    "Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð½Ð¾",
    "Ð½Ð°Ð·Ð°Ð´",
    "Ð¾Ð´Ð½Ð°ÐºÐ¾",
}


def _cleanup_routed_facts(facts: List[LegalFact]) -> List[LegalFact]:
    """
    Ð£Ð´Ð°Ð»ÑÐµÑ‚:
    - Ñ„Ð°ÐºÑ‚Ñ‹, ÑÐ¾ÑÑ‚Ð¾ÑÑ‰Ð¸Ðµ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¸Ð· person-Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²;
    - person-Ñ‚Ð¾ÐºÐµÐ½Ñ‹ Ñ "ÑÐ»ÑƒÐ¶ÐµÐ±Ð½Ñ‹Ð¼Ð¸" Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸ÑÐ¼Ð¸ (ÐŸÐ¾ÑÐ»Ðµ, ÐšÑ€Ð¾Ð¼Ðµ, Ð’Ñ‹, ÐÐµ, ÐÐµÑ‚ Ð¸ Ñ‚.Ð¿.).
    """
    cleaned: List[LegalFact] = []

    for f in facts:
        tokens = getattr(f, "tokens", []) or []
        persons = [t for t in tokens if t.type == "person"]
        other_tokens = [t for t in tokens if t.type != "person"]

        # ÐŸÐ¾Ð»Ð½Ð¾ÑÑ‚ÑŒÑŽ Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ñ„Ð°ÐºÑ‚Ñ‹, Ð³Ð´Ðµ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ "person" Ð¸ Ð½ÐµÑ‚ Ð´Ñ€ÑƒÐ³Ð¸Ñ… ÑÑƒÑ‰Ð½Ð¾ÑÑ‚ÐµÐ¹
        if persons and not other_tokens:
            continue

        # Ð§Ð¸ÑÑ‚Ð¸Ð¼ "Ð¿Ð»Ð¾Ñ…Ð¸Ðµ" person-Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ
        filtered_tokens = []
        for t in tokens:
            if t.type == "person":
                if t.value and t.value.strip().lower() in _BAD_PERSON_TOKENS:
                    continue
            filtered_tokens.append(t)

        f.tokens = filtered_tokens
        cleaned.append(f)

    return cleaned


# ============================================================
# ðŸ”§ Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ñ„Ð°ÐºÑ‚Ð¾Ð² Ð¿ÐµÑ€ÐµÐ´ LLM
# ============================================================

def _validate_facts_for_llm(facts: List[LegalFact]) -> List[LegalFact]:
    """
    ÐžÑ‚Ð±Ñ€Ð°ÑÑ‹Ð²Ð°ÐµÑ‚ Ð¿ÑƒÑÑ‚Ñ‹Ðµ / ÑÑ‚Ñ€Ð°Ð½Ð½Ñ‹Ðµ Ñ„Ð°ÐºÑ‚Ñ‹ Ð¿ÐµÑ€ÐµÐ´ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²ÐºÐ¾Ð¹ Ð² LLM.
    """
    valid: List[LegalFact] = []
    for f in facts:
        tokens = getattr(f, "tokens", None)
        if not tokens or not isinstance(tokens, list):
            continue
        if len(tokens) == 0:
            continue
        valid.append(f)
    return valid


# ============================================================
# ðŸ”§ Ð¡Ñ‚Ñ€Ð¾Ð³Ð¸Ð¹ sentence/token alignment Ð¿Ð¾Ð²ÐµÑ€Ñ… Ð±Ð°Ð·Ð¾Ð²Ð¾Ð³Ð¾
# ============================================================

def _strict_sentence_token_alignment(
    sentence_map: List[Dict[str, Any]],
    used_tokens: List[str],
    all_token_ids: List[str],
) -> Dict[str, Any]:
    """
    Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¹ ÑÑ‚Ñ€Ð¾Ð³Ð¸Ð¹ ÐºÐ¾Ð½Ñ‚Ñ€Ð¾Ð»ÑŒ:
    - Ð²ÑÐµ tokens, Ð½Ð° ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ ÑÑÑ‹Ð»Ð°ÐµÑ‚ÑÑ LLM, Ð´Ð¾Ð»Ð¶Ð½Ñ‹ Ð±Ñ‹Ñ‚ÑŒ Ð² all_token_ids;
    - alignment_ok = False, ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ Ð½ÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ñ‹Ðµ Ñ‚Ð¾ÐºÐµÐ½Ñ‹.
    """
    all_set = set(all_token_ids)
    used_set = set(used_tokens)

    unknown = sorted(list(used_set - all_set))
    missing = sorted(list(all_set - used_set))

    return {
        "unknown_tokens": unknown,
        "missing_tokens": missing,
        "alignment_ok": len(unknown) == 0,
    }


# ============================================================
# â­ ÐžÑÐ½Ð¾Ð²Ð½Ð°Ñ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ ÐºÐ²Ð°Ð»Ð¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸
# ============================================================

def qualify_documents(
    case_id: str,
    docs: List[Dict[str, Any]],
    city: str = "Ð³. ÐŸÐ°Ð²Ð»Ð¾Ð´Ð°Ñ€",
    investigator_fio: str = "ÐÐµ ÑƒÐºÐ°Ð·Ð°Ð½",
    investigator_line: str = "Ð¡Ð»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ",
    date_str: Optional[str] = None,
) -> Dict[str, Any]:

    # Ð±Ð°Ð·Ð¾Ð²Ð°Ñ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ñ… Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²
    validate_docs(docs)

    if not date_str:
        date_str = datetime.now().strftime("%d.%m.%Y")

    logger.info(
        f"â–¶ï¸ QUALIFIER 4.5 (token-json): case_id={case_id}, docs={len(docs)}"
    )

    # =====================================================================
    # 1) Tokenizer
    # =====================================================================
    tokenizer = FactTokenizer()
    tokenized_facts: List[LegalFact] = tokenizer.tokenize(docs)
    logger.info(f"ðŸ“˜ Tokenizer: Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¾ Ñ„Ð°ÐºÑ‚Ð¾Ð² = {len(tokenized_facts)}")

    if not tokenized_facts:
        return _empty_result(
            case_id,
            "Ð¤Ð°ÐºÑ‚Ñ‹ Ð½Ðµ Ð¾Ð±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ñ‹ Ð¿Ð¾ÑÐ»Ðµ Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ†Ð¸Ð¸.",
            investigator_fio,
            investigator_line,
        )

    # =====================================================================
    # 2) FactGraph (merge)
    # =====================================================================
    graph = FactGraph()
    merged: List[LegalFact] = graph.build(tokenized_facts)
    logger.info(f"ðŸ“— FactGraph: Ð¿Ð¾ÑÐ»Ðµ Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ñ = {len(merged)}")

    if not merged:
        return _empty_result(
            case_id,
            "ÐŸÐ¾ÑÐ»Ðµ Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ñ Ñ„Ð°ÐºÑ‚Ð¾Ð² (FactGraph) Ð½Ð¸Ñ‡ÐµÐ³Ð¾ Ð½Ðµ Ð¾ÑÑ‚Ð°Ð»Ð¾ÑÑŒ.",
            investigator_fio,
            investigator_line,
        )

    # =====================================================================
    # 2.1) FactFilter â€” Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ° Ð¿Ñ€Ð¾Ñ†ÐµÑÑÑƒÐ°Ð»ÐºÐ¸ Ð¸ Ð¼ÑƒÑÐ¾Ñ€Ð°
    # =====================================================================
    fact_filter = FactFilter()
    filtered_facts: List[LegalFact] = fact_filter.filter_for_qualifier(merged)
    logger.info(f"ðŸ“™ FactFilter: Ð¿Ð¾ÑÐ»Ðµ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ = {len(filtered_facts)}")

    if not filtered_facts:
        return _empty_result(
            case_id,
            "ÐÐµÑ‚ Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ñ‹Ñ… Ñ„Ð°ÐºÑ‚Ð¾Ð² Ð´Ð»Ñ ÐºÐ²Ð°Ð»Ð¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸ Ð¿Ð¾ÑÐ»Ðµ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸Ð¸.",
            investigator_fio,
            investigator_line,
        )

    # =====================================================================
    # 3) RAG Router
    # =====================================================================
    router = RAGRouter()
    routed_facts: List[LegalFact] = router.route_for_qualifier(filtered_facts)
    logger.info(f"ðŸ“™ RAG Router: ÐºÐ°Ð½Ð´Ð¸Ð´Ð°Ñ‚Ð¾Ð² Ð´Ð¾ Ð°Ð²Ñ‚Ð¾-Ñ‡Ð¸ÑÑ‚ÐºÐ¸ = {len(routed_facts)}")

    if not routed_facts:
        return _empty_result(
            case_id,
            "RAG Router Ð½Ðµ Ð½Ð°ÑˆÑ‘Ð» Ñ„Ð°ÐºÑ‚Ð¾Ð² Ð´Ð»Ñ ÐºÐ²Ð°Ð»Ð¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸.",
            investigator_fio,
            investigator_line,
        )

    # 3.1) Auto-clean routed facts (ÑƒÐ±Ð¸Ñ€Ð°ÐµÐ¼ person-only Ð¼ÑƒÑÐ¾Ñ€)
    routed_facts = _cleanup_routed_facts(routed_facts)
    routed_facts = _validate_facts_for_llm(routed_facts)

    logger.info(f"ðŸ“™ RAG Router: Ð¿Ð¾ÑÐ»Ðµ Ð°Ð²Ñ‚Ð¾-Ñ‡Ð¸ÑÑ‚ÐºÐ¸ = {len(routed_facts)}")

    if not routed_facts:
        return _empty_result(
            case_id,
            "ÐŸÐ¾ÑÐ»Ðµ Ð°Ð²Ñ‚Ð¾-Ñ‡Ð¸ÑÑ‚ÐºÐ¸ Ñ„Ð°ÐºÑ‚Ð¾Ð² Ð½Ðµ Ð¾ÑÑ‚Ð°Ð»Ð¾ÑÑŒ Ð´Ð»Ñ ÐºÐ²Ð°Ð»Ð¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸.",
            investigator_fio,
            investigator_line,
        )

    # ============================================================
    # 3.2) Crime Classification (Ð¿Ð¾ LegalFact)
    # ============================================================
    cls_input = [f for f in routed_facts if getattr(f, "role", "") != "generic_fact"]
    if not cls_input:
        cls_input = routed_facts

    classification = classify_by_tokens(cls_input)
    logger.info("âš– Crime classification:\n" + format_classification_debug(classification))

    primary_article = classification.get("primary")
    secondary_articles = classification.get("secondary", [])

    # =====================================================================
    # 4) ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° payload Ñ„Ð°ÐºÑ‚Ð¾Ð² Ð´Ð»Ñ LLM (JSON strict)
    # =====================================================================
    facts_payload: List[Dict[str, Any]] = []
    for f in routed_facts:
        d = f.model_dump()
        # Ð´Ð»Ñ Ð²ÐµÑ€Ð¸Ñ„Ð¸ÐºÐ°Ñ‚Ð¾Ñ€Ð°: Ð³Ð°Ñ€Ð°Ð½Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ð¾Ð»Ðµ sources, Ð´Ð°Ð¶Ðµ ÐµÑÐ»Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð½Ð°Ð·Ñ‹Ð²Ð°ÐµÑ‚ ÐµÐ³Ð¾ Ð¸Ð½Ð°Ñ‡Ðµ
        if "sources" not in d and "source_refs" in d:
            d["sources"] = d.get("source_refs") or []
        facts_payload.append(d)

    # =====================================================================
    # 5) Ð’Ñ‹Ð·Ð¾Ð² LLM Ð´Ð»Ñ Â«Ð£Ð¡Ð¢ÐÐÐžÐ’Ð˜Ð›Â» (P_UST_TOKENS_JSON)
    # =====================================================================
    system_prompt = prompts.P_UST_TOKENS_JSON
    user_payload = {"facts": facts_payload}
    user_prompt = json.dumps(user_payload, ensure_ascii=False, indent=2)

    response = ask_llm(system_prompt, user_prompt)

    # =====================================================================
    # 6) ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ JSON-Ð¾Ñ‚Ð²ÐµÑ‚Ð° LLM Ð¿Ð¾ Â«Ð£Ð¡Ð¢ÐÐÐžÐ’Ð˜Ð›Â»
    # =====================================================================
    try:
        if response.startswith("[LLM_ERROR]"):
            raise ValueError("LLM returned error marker")

        parsed = safe_json_loads(response)
        if not parsed:
            raise ValueError("JSON parse failed")

        ustanovil_text = (parsed.get("ustanovil") or "").strip()
        sentence_map = parsed.get("sentences", []) or []
        used_tokens = sorted({t for s in sentence_map for t in s.get("tokens", [])})

        logger.info(
            f"ðŸ“˜ SENTENCEâ€“TOKEN alignment Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½: {len(sentence_map)} Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¹"
        )
        logger.info(f"ðŸ“˜ USED TOKENS Ð¾Ñ‚ LLM: {used_tokens}")

    except Exception as e:
        logger.error(f"âŒ ÐÐµÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ñ‹Ð¹ JSON Ð¾Ñ‚ LLM (USTANOVIL): {e}")
        logger.error(
            f"âŒ Ð¡Ñ‹Ñ€Ð¾Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚ LLM (ÑƒÑÐµÑ‡Ñ‘Ð½ Ð´Ð¾ 1000 ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²): {str(response)[:1000]}"
        )
        ustanovil_text = _fallback_ustanovil(routed_facts)
        sentence_map = []

        used_tokens = []
        for f in routed_facts:
            used_tokens.extend(_extract_token_ids_from_fact(f))

    # Ð•ÑÐ»Ð¸ LLM Ð²ÐµÑ€Ð½ÑƒÐ» Ð¿ÑƒÑÑ‚Ð¾Ð¹ Â«Ð£Ð¡Ð¢ÐÐÐžÐ’Ð˜Ð›Â» â€” fallback Ð¿Ð¾ Ñ„Ð°ÐºÑ‚Ð°Ð¼
    if not ustanovil_text:
        ustanovil_text = _fallback_ustanovil(routed_facts)
        if not used_tokens:
            used_tokens = []
            for f in routed_facts:
                used_tokens.extend(_extract_token_ids_from_fact(f))

    # ------------------------------------------------------------
    # 6.1. Ð Ð°Ð·Ð±Ð¸ÐµÐ½Ð¸Ðµ Â«Ð£Ð¡Ð¢ÐÐÐžÐ’Ð˜Ð›Â» Ð½Ð° Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ (Ð´Ð»Ñ Ð»Ð¾Ð³Ð¾Ð²)
    # ------------------------------------------------------------
    _sentences_plain = split_into_sentences(ustanovil_text)
    logger.info(f"ðŸ“˜ USTANOVIL: Ñ€Ð°Ð·Ð±Ð¸ÐµÐ½Ð¸Ðµ Ð½Ð° Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ = {len(_sentences_plain)}")

    # ------------------------------------------------------------
    # 6.2. Ð¡Ð¾Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð²ÑÐµ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ñ‹Ðµ token_id Ð¸Ð· Ñ„Ð°ÐºÑ‚Ð¾Ð²
    # ------------------------------------------------------------
    all_token_ids = set()
    for f in routed_facts:
        all_token_ids.update(_extract_token_ids_from_fact(f))

    # ------------------------------------------------------------
    # 6.3. Anti-hallucination: sentence â†” token alignment
    # ------------------------------------------------------------
    base_alignment = verify_sentence_token_alignment(
        sentence_map=sentence_map,
        used_tokens=list(used_tokens),
        all_token_ids=list(all_token_ids),
    )

    strict_al = _strict_sentence_token_alignment(
        sentence_map=sentence_map,
        used_tokens=list(used_tokens),
        all_token_ids=list(all_token_ids),
    )

    # ÑÐ»Ð¸Ð²Ð°ÐµÐ¼ ÑÑ‚Ñ€Ð¾Ð³Ð¸Ð¹ alignment Ð² Ð¾ÑÐ½Ð¾Ð²Ð½Ð¾Ð¹
    if isinstance(base_alignment, dict):
        alignment = {**base_alignment, **strict_al}
    else:
        alignment = strict_al

    # =====================================================================
    # 7) ÐŸÐžÐ¡Ð¢ÐÐÐžÐ’Ð˜Ð› â€” LLM (Ð¾Ð±Ñ‹Ñ‡Ð½Ñ‹Ð¹ Ñ€ÐµÐ¶Ð¸Ð¼, Ð½Ð¾ JSON-Ð²Ñ…Ð¾Ð´)
    # =====================================================================
    post_system = prompts.P_POST
    post_payload = {
        "ustanovil_text": ustanovil_text,
        "primary_article": primary_article,
        "secondary_articles": secondary_articles,
    }
    post_user = json.dumps(post_payload, ensure_ascii=False, indent=2)

    post_text = ask_llm(post_system, post_user)
    if post_text.startswith("[LLM_ERROR]"):
        post_text = _fallback_postanovil(ustanovil_text)

    # =====================================================================
    # 8) Verification (token anti-hallucination + Ñ‚ÐµÐºÑÑ‚Ñ‹ + Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸)
    # =====================================================================
    verification = run_full_verification(
        {
            "facts": facts_payload,
            "ustanovil": ustanovil_text,
            "established_text": ustanovil_text,
            "final_postanovlenie": post_text,
            "used_tokens": used_tokens,
            "sentences": sentence_map,
        }
    )

    # =====================================================================
    # 9) Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°
    # =====================================================================
    result = {
        # Ð°Ð²Ñ‚Ð¾-ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ ÑÐ¾ÑÑ‚Ð°Ð²Ð°
        "auto_classification": classification,
        "primary_article": primary_article,
        "secondary_articles": secondary_articles,

        # Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸
        "generation_id": str(uuid.uuid4()),
        "model_version": MODEL_VERSION,
        "case_id": case_id,

        # Ñ„Ð°ÐºÑ‚Ð¾Ð»Ð¾Ð³Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð±Ð°Ð·Ð°
        "facts_used": facts_payload,
        "used_tokens": used_tokens,

        # Ñ‚ÐµÐºÑÑ‚ Ð¿Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ
        "established_text": ustanovil_text.strip(),
        "final_postanovlenie": post_text.strip(),

        # Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ°
        "verification": verification,

        # Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ â†’ Ñ‚Ð¾ÐºÐµÐ½Ñ‹
        "sentence_map": sentence_map,
        "sentence_alignment": alignment,
        "verification_sentences": verification.get("sentences"),

        # ÑÐ»ÑƒÐ¶ÐµÐ±Ð½Ñ‹Ðµ Ð¿Ð¾Ð»Ñ
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "investigator_fio": investigator_fio,
        "investigator_line": investigator_line,
        "city": city,
        "date": date_str,
    }

    logger.info(
        f"âœ” QUALIFIER 4.5 Ð·Ð°Ð²ÐµÑ€ÑˆÑ‘Ð½: facts={len(facts_payload)}, used={len(used_tokens)}"
    )
    return result


# ============================================================
# ðŸ”§ Fallback Â«Ð£Ð¡Ð¢ÐÐÐžÐ’Ð˜Ð›Â» (ÑƒÐ¼Ð½Ñ‹Ð¹, Ð±ÐµÐ· Ð¼ÑƒÑÐ¾Ñ€Ð°)
# ============================================================

def _fallback_ustanovil(facts: List[LegalFact]) -> str:
    """
    Ð£Ð¼Ð½Ñ‹Ð¹ fallback: ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ ÐºÑ€Ð°Ñ‚ÐºÑƒÑŽ ÑŽÑ€Ð¸Ð´Ð¸Ñ‡ÐµÑÐºÑƒÑŽ Ñ„Ð°Ð±ÑƒÐ»Ñƒ Ð¸Ð· Ñ„Ð°ÐºÑ‚Ð¾Ð²,
    Ð±ÐµÐ· Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð², Ð±ÐµÐ· Ð¼ÑƒÑÐ¾Ñ€Ð°, Ð±ÐµÐ· source_refs.
    Ð¡Ð¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÐµÑ‚ Ð»Ð¾Ð³Ð¸ÐºÐµ ÑÑ‚. 204 Ð£ÐŸÐš Ð Ðš (Ð¾Ð±Ñ‰Ð°Ñ Ñ„Ð°Ð±ÑƒÐ»Ð°).
    """

    persons = set()
    amounts = []
    actions = set()
    dates = set()
    platforms = set()

    for f in facts:
        txt = (getattr(f, "text", "") or "").lower()
        tokens = getattr(f, "tokens", []) or []

        for t in tokens:
            if t.type == "person" and t.value:
                v = t.value.strip()
                if len(v) > 2 and v.lower() not in _BAD_PERSON_TOKENS:
                    persons.add(v)

            if t.type == "amount":
                amounts.append(t.value)

            if t.type == "date":
                dates.add(t.value)

        if any(w in txt for w in ["Ð¿ÐµÑ€ÐµÐ²ÐµÐ»", "Ð¿ÐµÑ€ÐµÐ²ÐµÐ»Ð°", "Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð¸Ð»", "Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð¸Ð»Ð°", "Ð²Ð½ÐµÑ", "Ð²Ð½ÐµÑÐ»Ð°", "Ð¿Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ð»", "Ð¿Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ð»Ð°"]):
            actions.add("Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ñ‹ Ð´ÐµÐ½ÐµÐ¶Ð½Ñ‹Ñ… ÑÑ€ÐµÐ´ÑÑ‚Ð²")

        if any(w in txt for w in ["Ð¾Ð±Ð¼Ð°Ð½", "Ð¾Ð±Ð¼Ð°Ð½Ð½Ñ‹Ð¼ Ð¿ÑƒÑ‚ÐµÐ¼", "Ð²Ð²ÐµÐ» Ð² Ð·Ð°Ð±Ð»ÑƒÐ¶Ð´ÐµÐ½Ð¸Ðµ", "Ð²Ð²ÐµÐ»Ð° Ð² Ð·Ð°Ð±Ð»ÑƒÐ¶Ð´ÐµÐ½Ð¸Ðµ", "Ð²Ð²ÐµÐ»Ð¸ Ð² Ð·Ð°Ð±Ð»ÑƒÐ¶Ð´ÐµÐ½Ð¸Ðµ"]):
            actions.add("Ð²Ð²ÐµÐ´ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ñ‚ÐµÑ€Ð¿ÐµÐ²ÑˆÐ¸Ñ… Ð² Ð·Ð°Ð±Ð»ÑƒÐ¶Ð´ÐµÐ½Ð¸Ðµ")

        if "usdt" in txt or "okx" in txt or "binance" in txt:
            platforms.add("ÐºÑ€Ð¸Ð¿Ñ‚Ð¾Ð²Ð°Ð»ÑŽÑ‚Ð½Ñ‹Ðµ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸")

    lines: List[str] = []
    lines.append("ÐŸÐ¾ Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ð°Ð¼ Ð´ÐµÐ»Ð° ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¾ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰ÐµÐµ.")

    if actions:
        lines.append(f"Ð—Ð°Ñ„Ð¸ÐºÑÐ¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ, ÑÐ²ÑÐ·Ð°Ð½Ð½Ñ‹Ðµ Ñ {', '.join(sorted(actions))}.")

    if persons:
        lines.append(
            f"Ð’ Ð´ÐµÐ»Ðµ Ñ„Ð¸Ð³ÑƒÑ€Ð¸Ñ€ÑƒÑŽÑ‚ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ðµ ÑƒÑ‡Ð°ÑÑ‚Ð½Ð¸ÐºÐ¸: {', '.join(sorted(persons))}."
        )

    if amounts:
        try:
            # Ð³Ñ€ÑƒÐ±Ð°Ñ Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð´Ð»Ñ min/max: ÑƒÐ±Ð¸Ñ€Ð°ÐµÐ¼ Ð½ÐµÑ†Ð¸Ñ„Ñ€Ð¾Ð²Ñ‹Ðµ
            normalized = []
            for a in amounts:
                digits = re.sub(r"[^\d]", "", a)
                if digits:
                    normalized.append(int(digits))
            if normalized:
                min_v = min(normalized)
                max_v = max(normalized)
                lines.append(
                    f"ÐžÑ‚Ð¼ÐµÑ‡ÐµÐ½Ñ‹ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð½Ð° Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ ÑÑƒÐ¼Ð¼Ñ‹, Ð¾Ñ€Ð¸ÐµÐ½Ñ‚Ð¸Ñ€Ð¾Ð²Ð¾Ñ‡Ð½Ð¾ Ð¾Ñ‚ {min_v} Ð´Ð¾ {max_v} Ñ‚ÐµÐ½Ð³Ðµ."
                )
        except Exception:
            # ÐµÑÐ»Ð¸ Ð½Ðµ ÑÐ¼Ð¾Ð³Ð»Ð¸ Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð¾Ð²Ð°Ñ‚ÑŒ â€” Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð¿ÐµÑ€ÐµÑ‡Ð¸ÑÐ»Ð¸Ð¼ ÑÑƒÐ¼Ð¼Ñ‹
            lines.append(
                f"ÐžÑ‚Ð¼ÐµÑ‡ÐµÐ½Ñ‹ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð½Ð° ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ðµ ÑÑƒÐ¼Ð¼Ñ‹: {', '.join(amounts)}."
            )

    if platforms:
        lines.append("Ð˜Ð¼ÐµÑŽÑ‚ÑÑ ÑÐ²ÐµÐ´ÐµÐ½Ð¸Ñ Ð¾Ð± Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸ÑÑ…, ÑÐ²ÑÐ·Ð°Ð½Ð½Ñ‹Ñ… Ñ ÐºÑ€Ð¸Ð¿Ñ‚Ð¾Ð²Ð°Ð»ÑŽÑ‚Ð½Ñ‹Ð¼Ð¸ Ð¿Ð»Ð°Ñ‚Ñ„Ð¾Ñ€Ð¼Ð°Ð¼Ð¸.")

    if dates:
        lines.append(f"Ð¡Ð¾Ð±Ñ‹Ñ‚Ð¸Ñ Ð¾Ñ‚Ð½Ð¾ÑÑÑ‚ÑÑ Ðº Ð´Ð°Ñ‚Ð°Ð¼: {', '.join(sorted(dates))}.")

    lines.append(
        "Ð£ÐºÐ°Ð·Ð°Ð½Ð½Ñ‹Ðµ Ð¾Ð±ÑÑ‚Ð¾ÑÑ‚ÐµÐ»ÑŒÑÑ‚Ð²Ð° Ð² ÑÐ¾Ð²Ð¾ÐºÑƒÐ¿Ð½Ð¾ÑÑ‚Ð¸ ÑÐ²Ð¸Ð´ÐµÑ‚ÐµÐ»ÑŒÑÑ‚Ð²ÑƒÑŽÑ‚ Ð¾ ÑÐ¾Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ð¸ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ð¹ Ð¸Ð¼ÑƒÑ‰ÐµÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ð³Ð¾ Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð° Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ Ð²Ð²ÐµÐ´ÐµÐ½Ð¸Ñ Ð² Ð·Ð°Ð±Ð»ÑƒÐ¶Ð´ÐµÐ½Ð¸Ðµ Ð¸ Ð¿Ñ€Ð¸Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ Ð´ÐµÐ½ÐµÐ¶Ð½Ñ‹Ñ… ÑÑ€ÐµÐ´ÑÑ‚Ð² Ð¿Ð¾Ñ‚ÐµÑ€Ð¿ÐµÐ²ÑˆÐ¸Ñ…."
    )

    return " ".join(lines).strip()


def _fallback_postanovil(ustanovil_text: str) -> str:
    return (
        "ÐŸÐžÐ¡Ð¢ÐÐÐžÐ’Ð˜Ð›:\n"
        "ÐÐ° Ð¾ÑÐ½Ð¾Ð²Ð°Ð½Ð¸Ð¸ Ð¸Ð·Ð»Ð¾Ð¶ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð² Ñ€Ð°Ð·Ð´ÐµÐ»Ðµ Â«Ð£Ð¡Ð¢ÐÐÐžÐ’Ð˜Ð›Â»,\n"
        "Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ÑÑ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð¾ÐºÐ¾Ð½Ñ‡Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð¹ ÐºÐ²Ð°Ð»Ð¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸.\n"
    )


# ============================================================
# ðŸ”§ ÐŸÑƒÑÑ‚Ð¾Ð¹ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚
# ============================================================

def _empty_result(case_id: str, msg: str, fio: str, line: str) -> Dict[str, Any]:
    return {
        "generation_id": None,
        "model_version": MODEL_VERSION,
        "case_id": case_id,
        "established_text": msg,
        "final_postanovlenie": msg,
        "facts_used": [],
        "used_tokens": [],
        "verification": {"error": msg, "overall_ok": False},
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "investigator_fio": fio,
        "investigator_line": line,
        "city": None,
        "date": None,
    }
