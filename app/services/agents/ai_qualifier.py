# app/services/agents/ai_qualifier.py
from __future__ import annotations

import logging
import uuid
import json
import re
from collections import Counter
from typing import List, Optional, Dict, Any
from datetime import datetime

from app.services.llm_client import LLMClient
from app.services.facts.fact_models import LegalFact
from app.services.facts.fact_tokenizer import FactTokenizer
from app.services.facts.fact_graph import FactGraph
from app.services.facts.fact_filter import FactFilter
from app.services.rag_router import RAGRouter
from app.services.validation.verifier import (
    run_full_verification,
    verify_sentence_token_alignment,
)
from app.services.agents import prompts
from app.services.agents.crime_classifier import (
    classify_by_tokens,
    format_classification_debug,
)
from app.utils.sentence_splitter import split_into_sentences
from app.utils.utils_v4 import validate_docs

logger = logging.getLogger(__name__)

llm = LLMClient()

# Ð’Ð•Ð Ð¡Ð˜Ð® ÐžÐ‘ÐÐžÐ’Ð˜Ð›Ð˜, Ð§Ð¢ÐžÐ‘Ð« Ð’Ð˜Ð”ÐÐž Ð‘Ð«Ð›Ðž, Ð§Ð¢Ðž Ð›ÐžÐ“Ð˜ÐšÐ ÐŸÐ•Ð Ð•Ð ÐÐ‘ÐžÐ¢ÐÐÐ
MODEL_VERSION = "qualifier-llm-6.0.2"


# ============================================================
# ðŸ§  Ð’ÑÐ¿Ð¾Ð¼Ð¾Ð³Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ LLM
# ============================================================

def ask_llm(system_prompt: str, user_prompt: str) -> str:
    """
    ÐžÐ±Ñ‘Ñ€Ñ‚ÐºÐ° Ð½Ð°Ð´ LLMClient Ñ Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ Ð¸ Ð·Ð°Ñ‰Ð¸Ñ‚Ð¾Ð¹ Ð¾Ñ‚ Ð¿Ð°Ð´ÐµÐ½Ð¸Ð¹.
    """
    try:
        resp = llm.chat(
            [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ]
        )
        if resp is None:
            logger.error("LLM ERROR: Ð¾Ñ‚Ð²ÐµÑ‚ None")
            return "[LLM_ERROR]"
        if isinstance(resp, dict):
            # Ð•ÑÐ»Ð¸ LLMClient Ð²ÐµÑ€Ð½ÑƒÐ» dict (ÑÑ‚Ð¸Ð»ÑŒ OpenAI), Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ Ð²Ñ‹Ñ‚Ð°Ñ‰Ð¸Ñ‚ÑŒ Ñ‚ÐµÐºÑÑ‚
            try:
                content = resp["choices"][0]["message"]["content"]
                return (content or "").strip()
            except Exception:
                logger.error(f"LLM ERROR: Ð½ÐµÐ¾Ð¶Ð¸Ð´Ð°Ð½Ð½Ñ‹Ð¹ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ dict-Ð¾Ñ‚Ð²ÐµÑ‚Ð°: {resp}")
                return "[LLM_ERROR]"
        return str(resp).strip()
    except Exception as e:
        logger.error(f"LLM ERROR: {e}")
        return "[LLM_ERROR]"


# ============================================================
# ðŸ”§ Ð£ÑÑ‚Ð¾Ð¹Ñ‡Ð¸Ð²Ñ‹Ð¹ JSON-Ð¿Ð°Ñ€ÑÐµÑ€ (Ñ Ð°Ð²Ñ‚Ð¾-Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸ÐµÐ¼)
# ============================================================

def safe_json_loads(raw: str) -> Optional[dict]:
    """
    JSON Recovery Layer â€” AI_Qualifier 6.0+
    Ð˜ÑÐ¿Ñ€Ð°Ð²Ð»ÑÐµÑ‚ Ñ‡Ð°ÑÑ‚Ð¸Ñ‡Ð½Ð¾ ÑÐ»Ð¾Ð¼Ð°Ð½Ð½Ñ‹Ðµ JSON-ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñ‹ Ð¾Ñ‚ LLM.

    ÐŸÐ¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÑ‚:
    - ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ðµ ```json ... ``` Ð¾Ð±Ð¾Ð»Ð¾Ñ‡ÐµÐº;
    - ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ðµ Ð»Ð¸ÑˆÐ½Ð¸Ñ… Ð·Ð°Ð¿ÑÑ‚Ñ‹Ñ… Ð¿ÐµÑ€ÐµÐ´ ] Ð¸ };
    - Ð´Ð¾Ð±Ð¸Ð²Ð°Ð½Ð¸Ðµ Ð½ÐµÐ´Ð¾ÑÑ‚Ð°ÑŽÑ‰Ð¸Ñ… ÑÐºÐ¾Ð±Ð¾Ðº;
    - Ð¿Ð¾Ð¸ÑÐº Ð¿ÐµÑ€Ð²Ð¾Ð¹ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ð¾Ð¹ { ... } ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñ‹ Ð²Ð½ÑƒÑ‚Ñ€Ð¸ Ñ‚ÐµÐºÑÑ‚Ð°.
    """
    if not raw:
        return None

    cleaned = raw.strip()

    # 1) ÑƒÐ´Ð°Ð»ÑÐµÐ¼ markdown-Ð¾Ð±Ð¾Ð»Ð¾Ñ‡ÐºÑƒ ```json ... ```
    cleaned = re.sub(r"```[a-zA-Z0-9]*", "", cleaned).strip("` \n\r\t")

    # 2) ÑƒÐ´Ð°Ð»ÑÐµÐ¼ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾Ðµ ÑÐ»Ð¾Ð²Ð¾ "json" Ð² Ð½Ð°Ñ‡Ð°Ð»Ðµ
    cleaned = re.sub(r"^json\s*", "", cleaned, flags=re.IGNORECASE).strip()

    # 3) ÑƒÐ±Ð¸Ñ€Ð°ÐµÐ¼ Ð²Ð¸ÑÑÑ‰Ð¸Ðµ Ð·Ð°Ð¿ÑÑ‚Ñ‹Ðµ Ð¿ÐµÑ€ÐµÐ´ Ð·Ð°ÐºÑ€Ñ‹Ð²Ð°ÑŽÑ‰Ð¸Ð¼Ð¸ ÑÐºÐ¾Ð±ÐºÐ°Ð¼Ð¸
    cleaned = re.sub(r",\s*]", "]", cleaned)
    cleaned = re.sub(r",\s*}", "}", cleaned)

    # 4) Ð´Ð¾Ð±Ð¸Ð²Ð°ÐµÐ¼ Ð½ÐµÐ´Ð¾ÑÑ‚Ð°ÑŽÑ‰Ð¸Ðµ Ñ„Ð¸Ð³ÑƒÑ€Ð½Ñ‹Ðµ ÑÐºÐ¾Ð±ÐºÐ¸
    open_braces = cleaned.count("{")
    close_braces = cleaned.count("}")
    if open_braces > close_braces:
        cleaned += "}" * (open_braces - close_braces)

    # 5) Ð´Ð¾Ð±Ð¸Ð²Ð°ÐµÐ¼ Ð½ÐµÐ´Ð¾ÑÑ‚Ð°ÑŽÑ‰Ð¸Ðµ ÐºÐ²Ð°Ð´Ñ€Ð°Ñ‚Ð½Ñ‹Ðµ ÑÐºÐ¾Ð±ÐºÐ¸
    open_arr = cleaned.count("[")
    close_arr = cleaned.count("]")
    if open_arr > close_arr:
        cleaned += "]" * (open_arr - close_arr)

    # 6) Ð¿ÐµÑ€Ð²Ð°Ñ Ð¿Ð¾Ð¿Ñ‹Ñ‚ÐºÐ° parse
    try:
        parsed = json.loads(cleaned)
        # Ð¾Ð¶Ð¸Ð´Ð°ÐµÐ¼ dict; ÐµÑÐ»Ð¸ LLM Ð²ÐµÑ€Ð½ÑƒÐ» Ð¼Ð°ÑÑÐ¸Ð² Ñ Ð¾Ð´Ð½Ð¸Ð¼ Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð¼ â€” Ð±ÐµÑ€Ñ‘Ð¼ Ð¿ÐµÑ€Ð²Ñ‹Ð¹
        if isinstance(parsed, list) and parsed and isinstance(parsed[0], dict):
            return parsed[0]
        if isinstance(parsed, dict):
            return parsed
    except Exception:
        pass

    # 7) fallback: Ð²Ñ‹Ñ‚Ð°Ñ‰Ð¸Ñ‚ÑŒ Ð¿ÐµÑ€Ð²ÑƒÑŽ {...} ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ
    m = re.search(r"\{.*\}", cleaned, flags=re.DOTALL)
    if m:
        try:
            return json.loads(m.group(0))
        except Exception:
            return None

    return None


# ============================================================
# ðŸ§  Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ token_id
# ============================================================

def _extract_token_ids_from_fact(fact: LegalFact) -> List[str]:
    """
    Ð£Ð½Ð¸Ñ„Ð¸Ñ†Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ðµ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ token_id/token_ids Ð¸Ð· LegalFact,
    Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ Ð»Ð¾Ð²Ð¸Ñ‚ÑŒ 'method' object is not iterable.
    """
    token_ids: List[str] = []

    # Ð’Ð°Ñ€Ð¸Ð°Ð½Ñ‚ 1: ÐµÐ´Ð¸Ð½ÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹ token_id
    single = getattr(fact, "token_id", None)
    if isinstance(single, str) and single:
        token_ids.append(single)

    # Ð’Ð°Ñ€Ð¸Ð°Ð½Ñ‚ 2: Ð¿Ð¾Ð»Ðµ Ð¸Ð»Ð¸ Ð¼ÐµÑ‚Ð¾Ð´ token_ids
    attr = getattr(fact, "token_ids", None)
    if attr:
        try:
            value = attr() if callable(attr) else attr
            if isinstance(value, (list, tuple, set)):
                for v in value:
                    if isinstance(v, str) and v:
                        token_ids.append(v)
        except Exception as e:
            logger.warning(
                f"âš  ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¸Ð·Ð²Ð»ÐµÑ‡ÑŒ token_ids Ð¸Ð· Ñ„Ð°ÐºÑ‚Ð° {getattr('id', None)}: {e}"
            )

    # ÑƒÐ±Ð¸Ñ€Ð°ÐµÐ¼ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ñ‹
    return list(sorted(set(token_ids)))


# ============================================================
# ðŸ”§ Auto-clean routed facts (person-only, Ð¼ÑƒÑÐ¾Ñ€Ð½Ñ‹Ðµ Ð¸Ð¼ÐµÐ½Ð°)
# ============================================================

_BAD_PERSON_TOKENS = {
    "Ð¿Ð¾ÑÐ»Ðµ",
    "ÐºÑ€Ð¾Ð¼Ðµ",
    "Ð´Ð°Ð»ÐµÐµ",
    "Ð½Ðµ",
    "Ð½ÐµÑ‚",
    "Ð²Ñ‹",
    "Ð¾",
    "Ð°",
    "Ð´Ð»Ñ",
    "Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð½Ð¾",
    "Ð½Ð°Ð·Ð°Ð´",
    "Ð¾Ð´Ð½Ð°ÐºÐ¾",
}


def _cleanup_routed_facts(facts: List[LegalFact]) -> List[LegalFact]:
    """
    Ð£Ð´Ð°Ð»ÑÐµÑ‚:
    - Ñ„Ð°ÐºÑ‚Ñ‹, ÑÐ¾ÑÑ‚Ð¾ÑÑ‰Ð¸Ðµ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¸Ð· person-Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²;
    - person-Ñ‚Ð¾ÐºÐµÐ½Ñ‹ Ñ "ÑÐ»ÑƒÐ¶ÐµÐ±Ð½Ñ‹Ð¼Ð¸" Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸ÑÐ¼Ð¸ (ÐŸÐ¾ÑÐ»Ðµ, ÐšÑ€Ð¾Ð¼Ðµ, Ð’Ñ‹, ÐÐµ, ÐÐµÑ‚ Ð¸ Ñ‚.Ð¿.).
    """
    cleaned: List[LegalFact] = []

    for f in facts:
        tokens = getattr(f, "tokens", []) or []
        persons = [t for t in tokens if t.type == "person"]
        other_tokens = [t for t in tokens if t.type != "person"]

        # ÐŸÐ¾Ð»Ð½Ð¾ÑÑ‚ÑŒÑŽ Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ñ„Ð°ÐºÑ‚Ñ‹, Ð³Ð´Ðµ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ "person" Ð¸ Ð½ÐµÑ‚ Ð´Ñ€ÑƒÐ³Ð¸Ñ… ÑÑƒÑ‰Ð½Ð¾ÑÑ‚ÐµÐ¹
        if persons and not other_tokens:
            continue

        # Ð§Ð¸ÑÑ‚Ð¸Ð¼ "Ð¿Ð»Ð¾Ñ…Ð¸Ðµ" person-Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ
        filtered_tokens = []
        for t in tokens:
            if t.type == "person":
                if t.value and t.value.strip().lower() in _BAD_PERSON_TOKENS:
                    continue
            filtered_tokens.append(t)

        f.tokens = filtered_tokens
        cleaned.append(f)

    return cleaned


# ============================================================
# ðŸ”§ Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ñ„Ð°ÐºÑ‚Ð¾Ð² Ð¿ÐµÑ€ÐµÐ´ LLM
# ============================================================

def _validate_facts_for_llm(facts: List[LegalFact]) -> List[LegalFact]:
    """
    ÐžÑ‚Ð±Ñ€Ð°ÑÑ‹Ð²Ð°ÐµÑ‚ Ð¿ÑƒÑÑ‚Ñ‹Ðµ / ÑÑ‚Ñ€Ð°Ð½Ð½Ñ‹Ðµ Ñ„Ð°ÐºÑ‚Ñ‹ Ð¿ÐµÑ€ÐµÐ´ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²ÐºÐ¾Ð¹ Ð² LLM.
    """
    valid: List[LegalFact] = []
    for f in facts:
        tokens = getattr(f, "tokens", None)
        if not tokens or not isinstance(tokens, list):
            continue
        if len(tokens) == 0:
            continue
        valid.append(f)
    return valid


# ============================================================
# ðŸ”§ ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¤Ð˜Ðž Ð¸ Ð¸Ð¼ÐµÐ½Ð¸ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°/Ð¾Ñ€Ð³Ð°Ð½Ð¸Ð·Ð°Ñ†Ð¸Ð¸
# ============================================================

def _normalize_person_name(name: str) -> str:
    """
    Ð£Ð½Ð¸Ñ„Ð¸Ñ†Ð¸Ñ€ÑƒÐµÑ‚ Ð¤Ð˜Ðž: ÑƒÐ±Ð¸Ñ€Ð°ÐµÑ‚ Ð»Ð¸ÑˆÐ½Ð¸Ðµ Ð¿Ñ€Ð¾Ð±ÐµÐ»Ñ‹, Ð¿Ñ€Ð¸Ð²Ð¾Ð´Ð¸Ñ‚ Ðº Ð°ÐºÐºÑƒÑ€Ð°Ñ‚Ð½Ð¾Ð¼Ñƒ Ð²Ð¸Ð´Ñƒ.
    ÐÐµ Ð»ÐµÐ·ÐµÑ‚ Ð² Ð»Ð¾Ð³Ð¸ÐºÑƒ Ð¿Ð¾Ð»Ð°/Ð¿Ð°Ð´ÐµÐ¶ÐµÐ¹ â€” Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚.
    """
    if not name:
        return ""
    n = re.sub(r"\s+", " ", name).strip()
    if not n:
        return ""
    parts = n.split(" ")
    return " ".join(p[:1].upper() + p[1:] for p in parts)


def _normalize_project_name(name: str) -> str:
    """
    Ð£Ð½Ð¸Ñ„Ð¸Ñ†Ð¸Ñ€ÑƒÐµÑ‚ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ðµ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°/Ð¾Ñ€Ð³Ð°Ð½Ð¸Ð·Ð°Ñ†Ð¸Ð¸/Ð¿Ð»Ð°Ñ‚Ñ„Ð¾Ñ€Ð¼Ñ‹:
    ÑƒÐ±Ð¸Ñ€Ð°ÐµÑ‚ ÐºÐ°Ð²Ñ‹Ñ‡ÐºÐ¸, Ð»Ð¸ÑˆÐ½Ð¸Ðµ Ð¿Ñ€Ð¾Ð±ÐµÐ»Ñ‹, ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÑ‚ Ð¾Ð±Ñ‰Ð¸Ð¹ Ð²Ð¸Ð´.
    """
    if not name:
        return ""
    n = name.strip().strip("Â«Â»\"'â€œâ€â€žâ€œ")
    n = re.sub(r"\s+", " ", n)
    return n.strip()


# ============================================================
# ðŸ”§ Ð¡Ð±Ð¾Ñ€ Ð¼ÐµÑ‚Ð°-Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸ Ð¿Ð¾ Ð´ÐµÐ»Ñƒ
#      (suspects, victims, organizations, platforms, amounts)
# ============================================================

def _collect_case_meta(facts: List[LegalFact]) -> Dict[str, Any]:
    """
    Ð¡Ð¾Ð±Ð¸Ñ€Ð°ÐµÑ‚ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð´ÐµÐ»Ð° Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ LegalFact:
    - project_name
    - suspects (Ð¤Ð˜Ðž)
    - victims (Ð¤Ð˜Ðž)
    - organizations (Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ñ)
    - platforms (Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ñ)
    - all_persons
    - amounts_summary (min/max/total Ð¿Ð¾ Ñ‡Ð¸ÑÐ»Ð°Ð¼ Ð¸Ð· amount)
    - participants_formatted (Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ 2: Â«Ð»Ð¸Ñ†Ð¾, ÑƒÐºÐ°Ð·Ð°Ð½Ð½Ð¾Ðµ Ð² Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ð°Ñ… ÐºÐ°Ðº ...Â»)
    """
    suspects: set[str] = set()
    victims: set[str] = set()
    all_persons: set[str] = set()
    organizations: set[str] = set()
    platforms: set[str] = set()
    project_candidates: List[str] = []
    amount_values: List[int] = []

    for f in facts:
        txt_raw = getattr(f, "text", "") or ""
        txt = txt_raw.lower()
        tokens = getattr(f, "tokens", []) or []
        role = (getattr(f, "role", "") or "").lower()

        # role_label Ñ‚Ð¾ÐºÐµÐ½Ñ‹ (victim/suspect/organizer/witness ...)
        role_labels = {t.value for t in tokens if t.type == "role_label" and t.value}

        # PERSONS
        persons_in_fact = [t.value for t in tokens if t.type == "person" and t.value]
        norm_persons: List[str] = []
        for p in persons_in_fact:
            n = _normalize_person_name(p)
            if n:
                norm_persons.append(n)
                all_persons.add(n)

        # Heuristics Ð´Ð»Ñ Ð¿Ð¾Ð´Ð¾Ð·Ñ€ÐµÐ²Ð°ÐµÐ¼Ñ‹Ñ…
        is_suspect_fact = False
        if role in ("suspect_action", "fraud_action", "fraud_event"):
            is_suspect_fact = True
        if "Ð¿Ð¾Ð´Ð¾Ð·Ñ€ÐµÐ²Ð°ÐµÐ¼" in txt:
            is_suspect_fact = True
        if any("suspect" in str(lbl).lower() for lbl in role_labels):
            is_suspect_fact = True

        if is_suspect_fact:
            for p in norm_persons:
                suspects.add(p)

        # Heuristics Ð´Ð»Ñ Ð¿Ð¾Ñ‚ÐµÑ€Ð¿ÐµÐ²ÑˆÐ¸Ñ…
        is_victim_fact = False
        if "Ð¿Ð¾Ñ‚ÐµÑ€Ð¿ÐµÐ²Ñˆ" in txt:
            is_victim_fact = True
        if any("victim" in str(lbl).lower() for lbl in role_labels):
            is_victim_fact = True

        if is_victim_fact:
            for p in norm_persons:
                victims.add(p)

        # AMOUNTS
        for t in tokens:
            if t.type == "amount" and t.value:
                digits = re.sub(r"[^\d]", "", t.value)
                if digits:
                    try:
                        amount_values.append(int(digits))
                    except Exception:
                        pass

        # ORGANIZATIONS / PROJECTS / PLATFORMS â€” Ñ‡ÐµÑ€ÐµÐ· Ñ‚Ð¾ÐºÐµÐ½Ñ‹
        for t in tokens:
            t_type = getattr(t, "type", None)
            t_val = getattr(t, "value", None) or ""
            if not t_type or not t_val:
                continue

            if t_type in ("project", "project_name"):
                name_norm = _normalize_project_name(t_val)
                if name_norm:
                    project_candidates.append(name_norm)

            if t_type in ("organization", "company"):
                name_norm = _normalize_project_name(t_val)
                if name_norm:
                    organizations.add(name_norm)
                    project_candidates.append(name_norm)

            if t_type == "platform":
                name_norm = _normalize_project_name(t_val)
                if name_norm:
                    platforms.add(name_norm)

        # ORGANIZATIONS â€” Ñ‡ÐµÑ€ÐµÐ· Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ðµ ÑˆÐ°Ð±Ð»Ð¾Ð½Ñ‹
        for m in re.findall(
            r"(Ð¿Ñ€Ð¾ÐµÐºÑ‚|ÐºÐ¾Ð¼Ð¿Ð°Ð½Ð¸Ñ|Ð¾Ñ€Ð³Ð°Ð½Ð¸Ð·Ð°Ñ†Ð¸Ñ)\s+Â«([^Â»]{2,80})Â»",
            txt_raw,
            flags=re.IGNORECASE,
        ):
            name_norm = _normalize_project_name(m[1])
            if name_norm:
                organizations.add(name_norm)
                project_candidates.append(name_norm)

        # PLATFORMS â€” Ñ‡ÐµÑ€ÐµÐ· Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ðµ ÑˆÐ°Ð±Ð»Ð¾Ð½Ñ‹
        for m in re.findall(
            r"(Ð¿Ð»Ð°Ñ‚Ñ„Ð¾Ñ€Ð¼Ð°|ÑÐ¸ÑÑ‚ÐµÐ¼Ð°)\s+Â«([^Â»]{2,80})Â»",
            txt_raw,
            flags=re.IGNORECASE,
        ):
            name_norm = _normalize_project_name(m[1])
            if name_norm:
                platforms.add(name_norm)

    project_name = None
    if project_candidates:
        freq = Counter(project_candidates)
        project_name = freq.most_common(1)[0][0]

    amounts_summary = None
    if amount_values:
        try:
            amounts_summary = {
                "count": len(amount_values),
                "min": min(amount_values),
                "max": max(amount_values),
                "total": sum(amount_values),
            }
        except Exception:
            amounts_summary = {
                "count": len(amount_values),
            }

    # Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚ 2: ÑŽÑ€Ð¸Ð´Ð¸Ñ‡ÐµÑÐºÐ¸ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ñ‹Ðµ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ñ ÑƒÑ‡Ð°ÑÑ‚Ð½Ð¸ÐºÐ¾Ð²
    participants_formatted: Dict[str, List[str]] = {}

    if suspects:
        participants_formatted["suspects"] = [
            f"Ð¿Ð¾Ð´Ð¾Ð·Ñ€ÐµÐ²Ð°ÐµÐ¼Ñ‹Ð¹, ÑƒÐºÐ°Ð·Ð°Ð½Ð½Ñ‹Ð¹ Ð² Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ð°Ñ… ÐºÐ°Ðº {s}"
            for s in sorted(suspects)
        ]

    if victims:
        participants_formatted["victims"] = [
            f"Ð¿Ð¾Ñ‚ÐµÑ€Ð¿ÐµÐ²ÑˆÐ¸Ð¹, ÑƒÐºÐ°Ð·Ð°Ð½Ð½Ñ‹Ð¹ Ð² Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ð°Ñ… ÐºÐ°Ðº {v}"
            for v in sorted(victims)
        ]

    if organizations:
        participants_formatted["organizations"] = [
            f"Ð¾Ñ€Ð³Ð°Ð½Ð¸Ð·Ð°Ñ†Ð¸Ñ, Ñ„Ð¸Ð³ÑƒÑ€Ð¸Ñ€ÑƒÑŽÑ‰Ð°Ñ Ð² Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ð°Ñ… ÐºÐ°Ðº Â«{o}Â»"
            for o in sorted(organizations)
        ]

    if platforms:
        participants_formatted["platforms"] = [
            f"Ð¿Ð»Ð°Ñ‚Ñ„Ð¾Ñ€Ð¼Ð°, Ð¾Ð±Ð¾Ð·Ð½Ð°Ñ‡ÐµÐ½Ð½Ð°Ñ Ð² Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ð°Ñ… ÐºÐ°Ðº Â«{p}Â»"
            for p in sorted(platforms)
        ]

    meta: Dict[str, Any] = {
        "project_name": project_name,
        "suspects": sorted(suspects),
        "victims": sorted(victims),
        "organizations": sorted(organizations),
        "platforms": sorted(platforms),
        "victims_count": len(victims),
        "all_persons": sorted(all_persons),
        "amounts_summary": amounts_summary,
        "participants_formatted": participants_formatted,
    }

    return meta


# ============================================================
# ðŸ”§ Ð¡Ñ‚Ñ€Ð¾Ð³Ð¸Ð¹ sentence/token alignment Ð¿Ð¾Ð²ÐµÑ€Ñ… Ð±Ð°Ð·Ð¾Ð²Ð¾Ð³Ð¾
# ============================================================

def _strict_sentence_token_alignment(
    sentence_map: List[Dict[str, Any]],
    used_tokens: List[str],
    all_token_ids: List[str],
) -> Dict[str, Any]:
    """
    Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¹ ÑÑ‚Ñ€Ð¾Ð³Ð¸Ð¹ ÐºÐ¾Ð½Ñ‚Ñ€Ð¾Ð»ÑŒ:
    - Ð²ÑÐµ tokens, Ð½Ð° ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ ÑÑÑ‹Ð»Ð°ÐµÑ‚ÑÑ LLM, Ð´Ð¾Ð»Ð¶Ð½Ñ‹ Ð±Ñ‹Ñ‚ÑŒ Ð² all_token_ids;
    - alignment_ok = False, ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ Ð½ÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ñ‹Ðµ Ñ‚Ð¾ÐºÐµÐ½Ñ‹.
    """
    all_set = set(all_token_ids)
    used_set = set(used_tokens)

    unknown = sorted(list(used_set - all_set))
    missing = sorted(list(all_set - used_set))

    return {
        "unknown_tokens": unknown,
        "missing_tokens": missing,
        "alignment_ok": len(unknown) == 0,
    }


# ============================================================
# ðŸ”§ ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° Ñ‚ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð²ÑÑ‚Ð°Ð²Ð¾Ðº (token-id, UUID Ð¸ Ñ‚.Ð¿.)
# ============================================================

_TECH_TOKEN_RE = re.compile(r"\(token [^)]+\)", re.IGNORECASE)
_UUID_RE = re.compile(
    r"\b[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}\b"
)
_TOKEN_WORD_UUID_RE = re.compile(r"token\s+[0-9a-fA-F\-]{8,}", re.IGNORECASE)


def _strip_technical_tokens(text: str) -> str:
    """
    Ð£Ð±Ð¸Ñ€Ð°ÐµÑ‚ Ð¸Ð· Ñ‚ÐµÐºÑÑ‚Ð° ÑÐ»ÑƒÐ¶ÐµÐ±Ð½Ñ‹Ðµ ÐºÐ¾Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ð¸:
    â€¢ '(token XXXXX-...)'
    â€¢ Ñ‡Ð¸ÑÑ‚Ñ‹Ðµ UUID
    â€¢ Ñ„Ñ€Ð°Ð·Ñ‹ Ð²Ð¸Ð´Ð° 'token XXXXX-...'
    Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ ÐºÐ°Ðº Ð·Ð°Ñ‰Ð¸Ñ‚Ð½Ñ‹Ð¹ ÑÐ»Ð¾Ð¹, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð² Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ð¼ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ðµ
    Ð½Ðµ Ð±Ñ‹Ð»Ð¾ Ð²Ð½ÑƒÑ‚Ñ€ÐµÐ½Ð½Ð¸Ñ… Ð¸Ð´ÐµÐ½Ñ‚Ð¸Ñ„Ð¸ÐºÐ°Ñ‚Ð¾Ñ€Ð¾Ð².
    """
    if not text:
        return text

    cleaned = _TECH_TOKEN_RE.sub("", text)
    cleaned = _UUID_RE.sub("", cleaned)
    cleaned = _TOKEN_WORD_UUID_RE.sub("", cleaned)
    cleaned = re.sub(r"\s{2,}", " ", cleaned)
    return cleaned.strip()


# ============================================================
# ðŸ” ÐÐ²Ñ‚Ð¾-Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð³Ð¾Ñ€Ð¾Ð´Ð° Ð¿Ð¾ Ñ‚ÐµÐºÑÑ‚Ð°Ð¼
# ============================================================

def _detect_city_from_docs(docs: List[dict]) -> str:
    """
    ÐžÑ‡ÐµÐ½ÑŒ Ð¼ÑÐ³ÐºÐ¾Ðµ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð³Ð¾Ñ€Ð¾Ð´Ð°:
    - Ð¸Ñ‰ÐµÐ¼ Ð³. ÐÐ»Ð¼Ð°Ñ‚Ñ‹, ÐÑÑ‚Ð°Ð½Ð°, Ð¨Ñ‹Ð¼ÐºÐµÐ½Ñ‚, ÐŸÐ°Ð²Ð»Ð¾Ð´Ð°Ñ€, ÐšÐ°Ñ€Ð°Ð³Ð°Ð½Ð´Ð°, ÐšÐ¾ÑÑ‚Ð°Ð½Ð°Ð¹, ÐÐºÑ‚Ð°Ñƒ, ÐÐºÑ‚Ð¾Ð±Ðµ Ð¸ Ð´Ñ€.
    - ÐµÑÐ»Ð¸ Ð½ÐµÑ‚ â€” Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ Ð¿ÑƒÑÑ‚Ð¾
    """
    if not docs:
        return ""

    cities = [
        "ÐÐ»Ð¼Ð°Ñ‚Ñ‹",
        "ÐÑÑ‚Ð°Ð½Ð°",
        "ÐÑƒÑ€-Ð¡ÑƒÐ»Ñ‚Ð°Ð½",
        "Ð¨Ñ‹Ð¼ÐºÐµÐ½Ñ‚",
        "ÐŸÐ°Ð²Ð»Ð¾Ð´Ð°Ñ€",
        "ÐšÐ°Ñ€Ð°Ð³Ð°Ð½Ð´Ð°",
        "ÐšÐ¾ÑÑ‚Ð°Ð½Ð°Ð¹",
        "ÐÐºÑ‚Ð°Ñƒ",
        "ÐÐºÑ‚Ð¾Ð±Ðµ",
        "Ð¢Ð°Ñ€Ð°Ð·",
        "Ð£ÑÑ‚ÑŒ-ÐšÐ°Ð¼ÐµÐ½Ð¾Ð³Ð¾Ñ€ÑÐº",
        "Ð¡ÐµÐ¼ÐµÐ¹",
        "ÐšÐ¾ÐºÑˆÐµÑ‚Ð°Ñƒ",
    ]

    merged_text = " ".join((d.get("text") or "").lower() for d in docs)

    for c in cities:
        if c.lower() in merged_text:
            return c

    return ""


def _count_words(text: str) -> int:
    if not text:
        return 0
    return len(re.findall(r"\w+", text, flags=re.UNICODE))


# ============================================================
# â­ ÐžÑÐ½Ð¾Ð²Ð½Ð°Ñ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ ÐºÐ²Ð°Ð»Ð¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸
# ============================================================

def qualify_documents(
    case_id: Optional[str],
    docs: List[Dict[str, Any]],
    city: Optional[str] = None,
    investigator_fio: str = "ÐÐµ ÑƒÐºÐ°Ð·Ð°Ð½",
    investigator_line: str = "Ð¡Ð»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ",
    date_str: Optional[str] = None,
) -> Dict[str, Any]:

    # Ð±Ð°Ð·Ð¾Ð²Ð°Ñ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ñ… Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²
    validate_docs(docs)

    if not date_str:
        date_str = datetime.now().strftime("%d.%m.%Y")

    logger.info(
        f"â–¶ï¸ QUALIFIER 6.0.2 (token-json): docs={len(docs)}, case_id={case_id or '-'}"
    )

    # ------------------------------------------------------------
    # 0) ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð³Ð¾Ñ€Ð¾Ð´Ð°
    # ------------------------------------------------------------
    auto_city = _detect_city_from_docs(docs)
    if auto_city:
        city = auto_city
    else:
        city = city or ""

    # =====================================================================
    # 1) Tokenizer
    # =====================================================================
    tokenizer = FactTokenizer()
    tokenized_facts: List[LegalFact] = tokenizer.tokenize(docs)
    logger.info(f"ðŸ“˜ Tokenizer: Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¾ Ñ„Ð°ÐºÑ‚Ð¾Ð² = {len(tokenized_facts)}")

    if not tokenized_facts:
        return _empty_result(
            case_id,
            "Ð¤Ð°ÐºÑ‚Ñ‹ Ð½Ðµ Ð¾Ð±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ñ‹ Ð¿Ð¾ÑÐ»Ðµ Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ†Ð¸Ð¸.",
            investigator_fio,
            investigator_line,
        )

    # =====================================================================
    # 2) FactGraph (merge)
    # =====================================================================
    graph = FactGraph()
    merged: List[LegalFact] = graph.build(tokenized_facts)
    logger.info(f"ðŸ“— FactGraph: Ð¿Ð¾ÑÐ»Ðµ Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ñ = {len(merged)}")

    if not merged:
        return _empty_result(
            case_id,
            "ÐŸÐ¾ÑÐ»Ðµ Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ñ Ñ„Ð°ÐºÑ‚Ð¾Ð² (FactGraph) Ð½Ð¸Ñ‡ÐµÐ³Ð¾ Ð½Ðµ Ð¾ÑÑ‚Ð°Ð»Ð¾ÑÑŒ.",
            investigator_fio,
            investigator_line,
        )

    # =====================================================================
    # 2.1) FactFilter â€” Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ° Ð¿Ñ€Ð¾Ñ†ÐµÑÑÑƒÐ°Ð»ÐºÐ¸ Ð¸ Ð¼ÑƒÑÐ¾Ñ€Ð°
    # =====================================================================
    fact_filter = FactFilter()
    filtered_facts: List[LegalFact] = fact_filter.filter_for_qualifier(merged)
    logger.info(f"ðŸ“™ FactFilter: Ð¿Ð¾ÑÐ»Ðµ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ = {len(filtered_facts)}")

    if not filtered_facts:
        return _empty_result(
            case_id,
            "ÐÐµÑ‚ Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ñ‹Ñ… Ñ„Ð°ÐºÑ‚Ð¾Ð² Ð´Ð»Ñ ÐºÐ²Ð°Ð»Ð¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸ Ð¿Ð¾ÑÐ»Ðµ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸Ð¸.",
            investigator_fio,
            investigator_line,
        )

    # =====================================================================
    # 2.2) Pre-crime classification (Ñ‡Ð¸ÑÑ‚Ð¾ Ð´Ð¸Ð°Ð³Ð½Ð¾ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ)
    # =====================================================================
    pre_cls_input = [f for f in filtered_facts if getattr(f, "role", "") != "generic_fact"]
    if not pre_cls_input:
        pre_cls_input = filtered_facts

    pre_classification = classify_by_tokens(pre_cls_input)
    logger.info(
        "âš– Pre-crime classification (Ð´Ð»Ñ Ð¾Ñ‚Ð»Ð°Ð´ÐºÐ¸):\n"
        + format_classification_debug(pre_classification)
    )

    # =====================================================================
    # 3) RAG Router (Ð‘Ð•Ð— target_article â€” ÑƒÐ½Ð¸Ð²ÐµÑ€ÑÐ°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ€ÐµÐ¶Ð¸Ð¼)
    # =====================================================================
    router = RAGRouter()
    routed_facts: List[LegalFact] = router.route_for_qualifier(
        filtered_facts,
        target_article=None,  # ÐÐ• Ð½Ð°Ð²ÑÐ·Ñ‹Ð²Ð°ÐµÐ¼ ÑÐ¾ÑÑ‚Ð°Ð², Ñ€Ð¾ÑƒÑ‚ÐµÑ€ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ ÑƒÐ½Ð¸Ð²ÐµÑ€ÑÐ°Ð»ÑŒÐ½Ð¾
    )
    logger.info(f"ðŸ“™ RAG Router: ÐºÐ°Ð½Ð´Ð¸Ð´Ð°Ñ‚Ð¾Ð² (ÑÑ‹Ñ€Ð¾Ð¹ Ð²Ñ‹Ð²Ð¾Ð´) = {len(routed_facts)}")

    if not routed_facts:
        return _empty_result(
            case_id,
            "RAG Router Ð½Ðµ Ð½Ð°ÑˆÑ‘Ð» Ñ„Ð°ÐºÑ‚Ð¾Ð² Ð´Ð»Ñ ÐºÐ²Ð°Ð»Ð¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸.",
            investigator_fio,
            investigator_line,
        )

    # 3.1) Auto-clean routed facts (ÑƒÐ±Ð¸Ñ€Ð°ÐµÐ¼ person-only Ð¼ÑƒÑÐ¾Ñ€)
    routed_facts = _cleanup_routed_facts(routed_facts)
    routed_facts = _validate_facts_for_llm(routed_facts)

    if not routed_facts:
        return _empty_result(
            case_id,
            "ÐŸÐ¾ÑÐ»Ðµ Ð°Ð²Ñ‚Ð¾-Ñ‡Ð¸ÑÑ‚ÐºÐ¸ Ñ„Ð°ÐºÑ‚Ð¾Ð² Ð½Ðµ Ð¾ÑÑ‚Ð°Ð»Ð¾ÑÑŒ Ð´Ð»Ñ ÐºÐ²Ð°Ð»Ð¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸.",
            investigator_fio,
            investigator_line,
        )

    # 3.2) Ð“Ñ€ÑƒÐ¿Ð¿Ð¸Ñ€Ð¾Ð²ÐºÐ° Ð¿Ð¾ routing_group (primary / secondary / reserve)
    primary_facts: List[LegalFact] = []
    secondary_facts: List[LegalFact] = []
    reserve_facts: List[LegalFact] = []

    for f in routed_facts:
        grp = getattr(f, "routing_group", None)
        if grp == "secondary":
            secondary_facts.append(f)
        elif grp == "reserve":
            reserve_facts.append(f)
        else:
            primary_facts.append(f)

    routed_facts = primary_facts + secondary_facts + reserve_facts

    logger.info(
        "ðŸ“™ RAG Router: Ð³Ñ€ÑƒÐ¿Ð¿Ð¸Ñ€Ð¾Ð²ÐºÐ° Ð¿Ð¾ÑÐ»Ðµ Ð°Ð²Ñ‚Ð¾-Ñ‡Ð¸ÑÑ‚ÐºÐ¸ â†’ "
        f"primary={len(primary_facts)}, "
        f"secondary={len(secondary_facts)}, "
        f"reserve={len(reserve_facts)}, "
        f"total={len(routed_facts)}"
    )

    # 3.3) Ð¡Ð±Ð¾Ñ€ Ð¼ÐµÑ‚Ð°-Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸ Ð¿Ð¾ Ð´ÐµÐ»Ñƒ (project_name, suspects, victims, ÑÑƒÐ¼Ð¼Ñ‹, Ð¾Ñ€Ð³Ð°Ð½Ð¸Ð·Ð°Ñ†Ð¸Ð¸, Ð¿Ð»Ð°Ñ‚Ñ„Ð¾Ñ€Ð¼Ñ‹)
    case_meta = _collect_case_meta(routed_facts)
    logger.info(f"ðŸ“Œ Case meta: {case_meta}")

    # ============================================================
    # 3.4) Crime Classification (Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ð°Ñ, Ð¿Ð¾ routed_facts)
    #      â€” Ñ‡Ð¸ÑÑ‚Ð¾ Ð´Ð»Ñ auto_classification, ÐÐ• Ð½Ð°Ð²ÑÐ·Ñ‹Ð²Ð°ÐµÐ¼ LLM ÑÑ‚Ð°Ñ‚ÑŒÐ¸
    # ============================================================
    cls_input = [f for f in routed_facts if getattr(f, "role", "") != "generic_fact"]
    if not cls_input:
        cls_input = routed_facts

    classification = classify_by_tokens(cls_input)

    logger.info(
        "âš– Crime classification (Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ð°Ñ Ð¿Ð¾ routed_facts):\n"
        + format_classification_debug(classification)
    )

    primary_article = classification.get("primary")
    secondary_articles = classification.get("secondary", []) or []

    # Ð£Ð½Ð¸Ð²ÐµÑ€ÑÐ°Ð»ÑŒÐ½Ñ‹Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº Ð²ÑÐµÑ… Ð²Ñ‹ÑÐ²Ð»ÐµÐ½Ð½Ñ‹Ñ… ÑÑ‚Ð°Ñ‚ÐµÐ¹ (Ñ‡Ñ‚Ð¾Ð±Ñ‹ 217 ÐÐ• Ð²Ñ‹Ð³Ð»ÑÐ´ÐµÐ»Ð° Â«Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ð¾Â»)
    articles_all: List[str] = []
    if primary_article:
        articles_all.append(primary_article)
    for a in secondary_articles:
        if a and a not in articles_all:
            articles_all.append(a)

    logger.info(
        f"âš– Ð˜Ñ‚Ð¾Ð³Ð¾Ð²Ð°Ñ Ð°Ð²Ñ‚Ð¾-ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ: primary={primary_article}, "
        f"secondary={secondary_articles}, all={articles_all}"
    )

    # =====================================================================
    # 4) ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° payload Ñ„Ð°ÐºÑ‚Ð¾Ð² Ð´Ð»Ñ LLM (JSON strict, Ñ Ð³Ñ€ÑƒÐ¿Ð¿Ð°Ð¼Ð¸)
    # =====================================================================
    facts_payload: List[Dict[str, Any]] = []
    for f in routed_facts:
        d = f.model_dump()

        # Ð´Ð»Ñ Ð²ÐµÑ€Ð¸Ñ„Ð¸ÐºÐ°Ñ‚Ð¾Ñ€Ð°: Ð³Ð°Ñ€Ð°Ð½Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ð¾Ð»Ðµ sources, Ð´Ð°Ð¶Ðµ ÐµÑÐ»Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð½Ð°Ð·Ñ‹Ð²Ð°ÐµÑ‚ ÐµÐ³Ð¾ Ð¸Ð½Ð°Ñ‡Ðµ
        if "sources" not in d and "source_refs" in d:
            d["sources"] = d.get("source_refs") or []

        # Ð¿Ð¾Ð¼ÐµÑ‡Ð°ÐµÐ¼ routing_group, ÐµÑÐ»Ð¸ Ð¾Ð½ ÐµÑÑ‚ÑŒ Ñƒ Ñ„Ð°ÐºÑ‚Ð°
        grp = getattr(f, "routing_group", None)
        if grp:
            d["routing_group"] = grp

        facts_payload.append(d)

    logger.info(
        f"ðŸ“Š Facts payload Ð´Ð»Ñ LLM: Ð²ÑÐµÐ³Ð¾={len(facts_payload)}, "
        f"primaryâ‰ˆ{sum(1 for x in facts_payload if x.get('routing_group') == 'primary') or len(facts_payload)}, "
        f"secondary={sum(1 for x in facts_payload if x.get('routing_group') == 'secondary')}, "
        f"reserve={sum(1 for x in facts_payload if x.get('routing_group') == 'reserve')}"
    )

    # =====================================================================
    # 5) Ð’Ñ‹Ð·Ð¾Ð² LLM Ð´Ð»Ñ Â«Ð£Ð¡Ð¢ÐÐÐžÐ’Ð˜Ð›Â» (P_UST_TOKENS_JSON)
    #    â€” Ð‘Ð•Ð— Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‡Ð¸ ÑÑ‚Ð°Ñ‚ÐµÐ¹, Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ„Ð°ÐºÑ‚Ñ‹ + meta Ñ ÑƒÑ‡Ð°ÑÑ‚Ð½Ð¸ÐºÐ°Ð¼Ð¸
    # =====================================================================
    system_prompt = prompts.P_UST_TOKENS_JSON

    user_payload = {
        "facts": facts_payload,
        "meta": case_meta,  # project_name, suspects, victims, organizations, platforms, ÑÑƒÐ¼Ð¼Ñ‹, participants_formatted
        # Ð’ÐÐ–ÐÐž: ÐÐ˜ÐšÐÐšÐ˜Ð¥ primary_article / secondary_articles Ð·Ð´ÐµÑÑŒ Ð½ÐµÑ‚.
    }

    user_prompt = json.dumps(user_payload, ensure_ascii=False, indent=2)

    response = ask_llm(system_prompt, user_prompt)

    # =====================================================================
    # 6) ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ JSON-Ð¾Ñ‚Ð²ÐµÑ‚Ð° LLM Ð¿Ð¾ Â«Ð£Ð¡Ð¢ÐÐÐžÐ’Ð˜Ð›Â»
    # =====================================================================
    try:
        if response.startswith("[LLM_ERROR]"):
            raise ValueError("LLM returned error marker")

        parsed = safe_json_loads(response)
        if not parsed or not isinstance(parsed, dict):
            raise ValueError("JSON parse failed")

        ustanovil_text = (parsed.get("ustanovil") or "").strip()
        sentence_map = parsed.get("sentences", []) or []
        used_tokens = sorted({t for s in sentence_map for t in s.get("tokens", [])})

        logger.info(
            f"ðŸ“˜ SENTENCEâ€“TOKEN alignment Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½: {len(sentence_map)} Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¹"
        )
        logger.info(f"ðŸ“˜ USED TOKENS Ð¾Ñ‚ LLM: {used_tokens}")

    except Exception as e:
        logger.error(f"âŒ ÐÐµÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ñ‹Ð¹ JSON Ð¾Ñ‚ LLM (USTANOVIL): {e}")
        logger.error(
            f"âŒ Ð¡Ñ‹Ñ€Ð¾Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚ LLM (ÑƒÑÐµÑ‡Ñ‘Ð½ Ð´Ð¾ 1000 ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²): {str(response)[:1000]}"
        )
        ustanovil_text = _fallback_ustanovil(routed_facts)
        sentence_map = []

        used_tokens = []
        for f in routed_facts:
            used_tokens.extend(_extract_token_ids_from_fact(f))

    # Ð•ÑÐ»Ð¸ LLM Ð²ÐµÑ€Ð½ÑƒÐ» Ð¿ÑƒÑÑ‚Ð¾Ð¹ Â«Ð£Ð¡Ð¢ÐÐÐžÐ’Ð˜Ð›Â» â€” fallback Ð¿Ð¾ Ñ„Ð°ÐºÑ‚Ð°Ð¼
    if not ustanovil_text:
        ustanovil_text = _fallback_ustanovil(routed_facts)
        if not used_tokens:
            used_tokens = []
            for f in routed_facts:
                used_tokens.extend(_extract_token_ids_from_fact(f))

    # ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° Ð¾Ñ‚ Ñ‚ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð²ÑÑ‚Ð°Ð²Ð¾Ðº (token-id, UUID Ð¸ Ñ‚.Ð¿.)
    ustanovil_text = _strip_technical_tokens(ustanovil_text)

    # ------------------------------------------------------------
    # 6.1. Ð Ð°Ð·Ð±Ð¸ÐµÐ½Ð¸Ðµ Â«Ð£Ð¡Ð¢ÐÐÐžÐ’Ð˜Ð›Â» Ð½Ð° Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ Ð¸ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°
    # ------------------------------------------------------------
    _sentences_plain = split_into_sentences(ustanovil_text)
    logger.info(f"ðŸ“˜ USTANOVIL: Ñ€Ð°Ð·Ð±Ð¸ÐµÐ½Ð¸Ðµ Ð½Ð° Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ = {len(_sentences_plain)}")

    ustanovil_word_count = _count_words(ustanovil_text)
    logger.info(f"ðŸ“˜ USTANOVIL: Ð´Ð»Ð¸Ð½Ð° ~ {ustanovil_word_count} ÑÐ»Ð¾Ð²")

    # ------------------------------------------------------------
    # 6.2. Ð¡Ð¾Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð²ÑÐµ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ñ‹Ðµ token_id Ð¸Ð· Ñ„Ð°ÐºÑ‚Ð¾Ð²
    # ------------------------------------------------------------
    all_token_ids = set()
    for f in routed_facts:
        all_token_ids.update(_extract_token_ids_from_fact(f))

    # ------------------------------------------------------------
    # 6.3. Anti-hallucination: sentence â†” token alignment
    # ------------------------------------------------------------
    base_alignment = verify_sentence_token_alignment(
        sentence_map=sentence_map,
        used_tokens=list(used_tokens),
        all_token_ids=list(all_token_ids),
    )

    strict_al = _strict_sentence_token_alignment(
        sentence_map=sentence_map,
        used_tokens=list(used_tokens),
        all_token_ids=list(all_token_ids),
    )

    if isinstance(base_alignment, dict):
        alignment = {**base_alignment, **strict_al}
    else:
        alignment = strict_al

    # =====================================================================
    # 7) ÐŸÐžÐ¡Ð¢ÐÐÐžÐ’Ð˜Ð› â€” LLM (Ð¾Ð±Ñ‹Ñ‡Ð½Ñ‹Ð¹ Ñ€ÐµÐ¶Ð¸Ð¼, Ð½Ð¾ JSON-Ð²Ñ…Ð¾Ð´)
    #    â€” ÑÑ‚Ð°Ñ‚ÑŒÐ¸ Ð£Ðš/Ð£ÐŸÐš Ð˜Ð˜ Ð²Ñ‹Ð²Ð¾Ð´Ð¸Ñ‚ ÑÐ°Ð¼ Ð¸Ð· ustanovil_text, Ð¼Ñ‹ ÐÐ• Ð¿Ð¾Ð´ÑÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð½Ð¾Ð¼ÐµÑ€Ð°
    # =====================================================================
    post_system = prompts.P_POST
    post_payload = {
        "ustanovil_text": ustanovil_text,
        "meta": case_meta,
        # ÐÐ•Ð¢ primary_article/secondary_articles â€” Ð˜Ð˜ ÑÐ°Ð¼ Ñ€ÐµÑˆÐ°ÐµÑ‚, ÐºÐ°ÐºÐ¸Ðµ ÑÑ‚Ð°Ñ‚ÑŒÐ¸ ÑƒÐºÐ°Ð·Ð°Ñ‚ÑŒ.
    }
    post_user = json.dumps(post_payload, ensure_ascii=False, indent=2)

    post_text = ask_llm(post_system, post_user)
    if post_text.startswith("[LLM_ERROR]"):
        post_text = _fallback_postanovil(ustanovil_text)

    post_text = _strip_technical_tokens(post_text)

    # =====================================================================
    # 8) Verification (token anti-hallucination + Ñ‚ÐµÐºÑÑ‚Ñ‹ + Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸)
    # =====================================================================
    verification = run_full_verification(
        {
            "facts": facts_payload,
            "ustanovil": ustanovil_text,
            "established_text": ustanovil_text,
            "final_postanovlenie": post_text,
            "used_tokens": used_tokens,
            "sentences": sentence_map,
        }
    )

    # =====================================================================
    # 9) Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°
    # =====================================================================
    result = {
        # Ð°Ð²Ñ‚Ð¾-ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ ÑÐ¾ÑÑ‚Ð°Ð²Ð° (Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ñ‹Ð¹ ÑÐ»Ð¾Ð¹)
        "auto_classification": classification,
        "primary_article": primary_article,
        "secondary_articles": secondary_articles,
        "articles_all": articles_all,

        # Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸
        "generation_id": str(uuid.uuid4()),
        "model_version": MODEL_VERSION,
        "case_id": case_id,

        # Ñ„Ð°ÐºÑ‚Ð¾Ð»Ð¾Ð³Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð±Ð°Ð·Ð°
        "facts_used": facts_payload,
        "used_tokens": used_tokens,
        "case_meta": case_meta,

        # Ñ‚ÐµÐºÑÑ‚ Ð¿Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ
        "established_text": ustanovil_text.strip(),
        "final_postanovlenie": post_text.strip(),

        # ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð¿Ð¾ Ð´Ð»Ð¸Ð½Ðµ ÐºÐ²Ð°Ð»Ð¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸
        "ustanovil_word_count": ustanovil_word_count,
        "ustanovil_sentence_count": len(_sentences_plain),

        # Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ°
        "verification": verification,
        "sentence_map": sentence_map,
        "sentence_alignment": alignment,
        "verification_sentences": verification.get("sentences"),

        # ÑÐ»ÑƒÐ¶ÐµÐ±Ð½Ñ‹Ðµ Ð¿Ð¾Ð»Ñ
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "investigator_fio": investigator_fio,
        "investigator_line": investigator_line,
        "city": city,
        "date": date_str,
    }

    logger.info(
        f"âœ” QUALIFIER 6.0.2 Ð·Ð°Ð²ÐµÑ€ÑˆÑ‘Ð½: facts={len(facts_payload)}, used={len(used_tokens)}, "
        f"ustanovil_words={ustanovil_word_count}"
    )
    return result


# ============================================================
# ðŸ”§ Fallback Â«Ð£Ð¡Ð¢ÐÐÐžÐ’Ð˜Ð›Â» (Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ 2, Ð±ÐµÐ· Ð¼Ð¾Ñ‚Ð¸Ð²Ð¾Ð²)
# ============================================================

def _fallback_ustanovil(facts: List[LegalFact]) -> str:
    """
    Ð£Ð¼Ð½Ñ‹Ð¹ fallback: ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ ÐºÑ€Ð°Ñ‚ÐºÑƒÑŽ ÑŽÑ€Ð¸Ð´Ð¸Ñ‡ÐµÑÐºÑƒÑŽ Ñ„Ð°Ð±ÑƒÐ»Ñƒ Ð¸Ð· Ñ„Ð°ÐºÑ‚Ð¾Ð²,
    Ð±ÐµÐ· Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð², Ð±ÐµÐ· source_refs, Ð±ÐµÐ· Ð¼Ð¾Ñ‚Ð¸Ð²Ð¾Ð² Ð¸ Ð¾Ñ†ÐµÐ½Ð¾Ðº.
    Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ 2 Ð´Ð»Ñ ÑƒÑ‡Ð°ÑÑ‚Ð½Ð¸ÐºÐ¾Ð²:
    Â«Ð»Ð¸Ñ†Ð¾, ÑƒÐºÐ°Ð·Ð°Ð½Ð½Ð¾Ðµ Ð² Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ð°Ñ… ÐºÐ°Ðº ...Â» / Â«Ð¾Ñ€Ð³Ð°Ð½Ð¸Ð·Ð°Ñ†Ð¸Ñ, Ñ„Ð¸Ð³ÑƒÑ€Ð¸Ñ€ÑƒÑŽÑ‰Ð°Ñ ÐºÐ°Ðº ...Â».
    """

    meta = _collect_case_meta(facts)

    suspects = meta.get("suspects") or []
    victims = meta.get("victims") or []
    organizations = meta.get("organizations") or []
    platforms_named = meta.get("platforms") or []
    project_name = meta.get("project_name")

    persons_other = set()
    amounts = []
    actions = set()
    dates = set()
    platform_flags = set()

    for f in facts:
        txt = (getattr(f, "text", "") or "")
        low = txt.lower()
        tokens = getattr(f, "tokens", []) or []

        for t in tokens:
            if t.type == "person" and t.value:
                v = t.value.strip()
                if len(v) > 2 and v.lower() not in _BAD_PERSON_TOKENS:
                    norm = _normalize_person_name(v)
                    if norm and norm not in suspects and norm not in victims:
                        persons_other.add(norm)

            if t.type == "amount" and t.value:
                amounts.append(t.value)

            if t.type == "date" and t.value:
                dates.add(t.value)

        if any(
            w in low
            for w in [
                "Ð¿ÐµÑ€ÐµÐ²ÐµÐ»",
                "Ð¿ÐµÑ€ÐµÐ²ÐµÐ»Ð°",
                "Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð¸Ð»",
                "Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð¸Ð»Ð°",
                "Ð²Ð½ÐµÑ",
                "Ð²Ð½ÐµÑÐ»Ð°",
                "Ð¿Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ð»",
                "Ð¿Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ð»Ð°",
            ]
        ):
            actions.add("Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð°Ð¼Ð¸ Ð¸ Ð¸Ð½Ñ‹Ð¼Ð¸ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸ÑÐ¼Ð¸ Ñ Ð´ÐµÐ½ÐµÐ¶Ð½Ñ‹Ð¼Ð¸ ÑÑ€ÐµÐ´ÑÑ‚Ð²Ð°Ð¼Ð¸")

        if "usdt" in low or "okx" in low or "binance" in low:
            platform_flags.add("Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸ÑÐ¼Ð¸, ÑÐ²ÑÐ·Ð°Ð½Ð½Ñ‹Ð¼Ð¸ Ñ Ñ†Ð¸Ñ„Ñ€Ð¾Ð²Ñ‹Ð¼Ð¸ ÑÐµÑ€Ð²Ð¸ÑÐ°Ð¼Ð¸ Ð¸ ÐºÑ€Ð¸Ð¿Ñ‚Ð¾Ð²Ð°Ð»ÑŽÑ‚Ð½Ñ‹Ð¼Ð¸ Ð¿Ð»Ð°Ñ‚Ñ„Ð¾Ñ€Ð¼Ð°Ð¼Ð¸")

    lines: List[str] = []
    lines.append("ÐŸÐ¾ Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ð°Ð¼ Ð´Ð¾ÑÑƒÐ´ÐµÐ±Ð½Ð¾Ð³Ð¾ Ñ€Ð°ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ñ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¾ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰ÐµÐµ.")

    # ÐžÑ€Ð³Ð°Ð½Ð¸Ð·Ð°Ñ†Ð¸Ñ / Ð¿Ñ€Ð¾ÐµÐºÑ‚
    org_source_names: List[str] = []
    if project_name:
        org_source_names.append(project_name)
    if organizations:
        for o in organizations:
            if o not in org_source_names:
                org_source_names.append(o)

    if org_source_names:
        main_org = org_source_names[0]
        lines.append(
            f"Ð’ Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ð°Ñ… Ñ„Ð¸Ð³ÑƒÑ€Ð¸Ñ€ÑƒÐµÑ‚ Ð¾Ñ€Ð³Ð°Ð½Ð¸Ð·Ð°Ñ†Ð¸Ñ (Ð¿Ñ€Ð¾ÐµÐºÑ‚), Ð¾Ð±Ð¾Ð·Ð½Ð°Ñ‡ÐµÐ½Ð½Ð°Ñ Ð² Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ… ÐºÐ°Ðº Â«{main_org}Â»."
        )

    # ÐŸÐ¾Ð´Ð¾Ð·Ñ€ÐµÐ²Ð°ÐµÐ¼Ñ‹Ðµ
    if suspects:
        formatted = ", ".join(
            f"Ð»Ð¸Ñ†Ð¾, ÑƒÐºÐ°Ð·Ð°Ð½Ð½Ð¾Ðµ Ð² Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ð°Ñ… ÐºÐ°Ðº {s}" for s in sorted(suspects)
        )
        lines.append(
            f"Ð’ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ðµ Ð¿Ð¾Ð´Ð¾Ð·Ñ€ÐµÐ²Ð°ÐµÐ¼Ñ‹Ñ… Ð² Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ð°Ñ… ÑƒÐºÐ°Ð·Ð°Ð½Ñ‹ {formatted}."
        )

    # ÐŸÐ¾Ñ‚ÐµÑ€Ð¿ÐµÐ²ÑˆÐ¸Ðµ
    if victims:
        formatted = ", ".join(
            f"Ð»Ð¸Ñ†Ð¾, ÑƒÐºÐ°Ð·Ð°Ð½Ð½Ð¾Ðµ Ð² Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ð°Ñ… ÐºÐ°Ðº {v}" for v in sorted(victims)
        )
        lines.append(
            f"Ð’ Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ð°Ñ… Ð¾Ñ‚Ñ€Ð°Ð¶ÐµÐ½Ñ‹ Ð¿Ð¾Ñ‚ÐµÑ€Ð¿ÐµÐ²ÑˆÐ¸Ðµ, ÑƒÐºÐ°Ð·Ð°Ð½Ð½Ñ‹Ðµ Ð² Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ð°Ñ… ÐºÐ°Ðº {formatted}."
        )

    # Ð˜Ð½Ñ‹Ðµ ÑƒÑ‡Ð°ÑÑ‚Ð½Ð¸ÐºÐ¸
    if persons_other:
        lines.append(
            "Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð² Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ð°Ñ… ÑƒÐ¿Ð¾Ð¼Ð¸Ð½Ð°ÑŽÑ‚ÑÑ Ð¸Ð½Ñ‹Ðµ ÑƒÑ‡Ð°ÑÑ‚Ð½Ð¸ÐºÐ¸, Ð¾Ð±Ð¾Ð·Ð½Ð°Ñ‡ÐµÐ½Ð½Ñ‹Ðµ Ð² Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ… ÐºÐ°Ðº: "
            + ", ".join(sorted(persons_other))
            + "."
        )

    # Ð”ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ / Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸
    if actions:
        lines.append(
            f"Ð—Ð°Ñ„Ð¸ÐºÑÐ¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ, ÑÐ²ÑÐ·Ð°Ð½Ð½Ñ‹Ðµ Ñ {', '.join(sorted(actions))}."
        )

    # Ð¡ÑƒÐ¼Ð¼Ñ‹
    if amounts:
        try:
            normalized = []
            for a in amounts:
                digits = re.sub(r"[^\d]", "", a)
                if digits:
                    normalized.append(int(digits))
            if normalized:
                min_v = min(normalized)
                max_v = max(normalized)
                lines.append(
                    f"ÐžÐ¿ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ñ Ð´ÐµÐ½ÐµÐ¶Ð½Ñ‹Ð¼Ð¸ ÑÑ€ÐµÐ´ÑÑ‚Ð²Ð°Ð¼Ð¸ Ð¾Ñ‚Ñ€Ð°Ð¶ÐµÐ½Ñ‹ Ð½Ð° ÑÑƒÐ¼Ð¼Ñ‹ Ð¾Ñ‚ {min_v} Ð´Ð¾ {max_v} Ñ‚ÐµÐ½Ð³Ðµ."
                )
        except Exception:
            lines.append(
                "Ð’ Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ð°Ñ… ÑƒÐºÐ°Ð·Ð°Ð½Ñ‹ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ñ Ð´ÐµÐ½ÐµÐ¶Ð½Ñ‹Ð¼Ð¸ ÑÑ€ÐµÐ´ÑÑ‚Ð²Ð°Ð¼Ð¸ Ð½Ð° ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ðµ ÑÑƒÐ¼Ð¼Ñ‹: "
                + ", ".join(amounts)
                + "."
            )

    # ÐŸÐ»Ð°Ñ‚Ñ„Ð¾Ñ€Ð¼Ñ‹ Ð¿Ð¾ Ñ‚Ð¾ÐºÐµÐ½Ð°Ð¼ (Ð¸Ð¼ÐµÐ½Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ)
    if platforms_named:
        lines.append(
            "Ð’ Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ð°Ñ… ÑƒÐ¿Ð¾Ð¼ÑÐ½ÑƒÑ‚Ñ‹ Ð¿Ð»Ð°Ñ‚Ñ„Ð¾Ñ€Ð¼Ñ‹ Ð¸ Ñ†Ð¸Ñ„Ñ€Ð¾Ð²Ñ‹Ðµ ÑÐµÑ€Ð²Ð¸ÑÑ‹, Ð¾Ð±Ð¾Ð·Ð½Ð°Ñ‡ÐµÐ½Ð½Ñ‹Ðµ Ð² Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ… ÐºÐ°Ðº: "
            + ", ".join(f"Â«{p}Â»" for p in sorted(platforms_named))
            + "."
        )

    # ÐŸÐ»Ð°Ñ‚Ñ„Ð¾Ñ€Ð¼Ñ‹ Ð¿Ð¾ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ð¼ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ°Ð¼
    if platform_flags:
        lines.append(
            "ÐžÑ‚Ð¼ÐµÑ‡ÐµÐ½Ñ‹ Ñ‚Ð°ÐºÐ¶Ðµ ÑÐ²ÐµÐ´ÐµÐ½Ð¸Ñ Ð¾Ð± Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸ÑÑ…, ÑÐ²ÑÐ·Ð°Ð½Ð½Ñ‹Ñ… Ñ Ñ†Ð¸Ñ„Ñ€Ð¾Ð²Ñ‹Ð¼Ð¸ ÑÐµÑ€Ð²Ð¸ÑÐ°Ð¼Ð¸ Ð¸ ÐºÑ€Ð¸Ð¿Ñ‚Ð¾Ð²Ð°Ð»ÑŽÑ‚Ð½Ñ‹Ð¼Ð¸ Ð¿Ð»Ð°Ñ‚Ñ„Ð¾Ñ€Ð¼Ð°Ð¼Ð¸."
        )

    # Ð”Ð°Ñ‚Ñ‹
    if dates:
        lines.append(
            "Ð¡Ð¾Ð±Ñ‹Ñ‚Ð¸Ñ, Ð¸Ð·Ð»Ð¾Ð¶ÐµÐ½Ð½Ñ‹Ðµ Ð² Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ð°Ñ…, ÑÐ¾Ð¾Ñ‚Ð½Ð¾ÑÑÑ‚ÑÑ ÑÐ¾ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¼Ð¸ Ð´Ð°Ñ‚Ð°Ð¼Ð¸: "
            + ", ".join(sorted(dates))
            + "."
        )

    # Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ðµ Ð¾Ð±Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð±ÐµÐ· Ð¼Ð¾Ñ‚Ð¸Ð²Ð¾Ð²/Ð²Ñ‹Ð²Ð¾Ð´Ð¾Ð²
    lines.append(
        "ÐŸÐµÑ€ÐµÑ‡Ð¸ÑÐ»ÐµÐ½Ð½Ñ‹Ðµ ÑÐ²ÐµÐ´ÐµÐ½Ð¸Ñ Ð² ÑÐ¾Ð²Ð¾ÐºÑƒÐ¿Ð½Ð¾ÑÑ‚Ð¸ Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð¸Ð·ÑƒÑŽÑ‚ Ñ„Ð°ÐºÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð¾Ð±ÑÑ‚Ð¾ÑÑ‚ÐµÐ»ÑŒÑÑ‚Ð²Ð°, "
        "Ð¾Ñ‚Ñ€Ð°Ð¶Ñ‘Ð½Ð½Ñ‹Ðµ Ð² Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ð°Ñ… Ð´Ð¾ÑÑƒÐ´ÐµÐ±Ð½Ð¾Ð³Ð¾ Ñ€Ð°ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ñ, Ð±ÐµÐ· Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð¸Ñ… ÑŽÑ€Ð¸Ð´Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ ÐºÐ²Ð°Ð»Ð¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸."
    )

    return " ".join(lines).strip()


def _fallback_postanovil(ustanovil_text: str) -> str:
    return (
        "ÐŸÐžÐ¡Ð¢ÐÐÐžÐ’Ð˜Ð›:\n"
        "ÐÐ° Ð¾ÑÐ½Ð¾Ð²Ð°Ð½Ð¸Ð¸ Ð¸Ð·Ð»Ð¾Ð¶ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð² Ñ€Ð°Ð·Ð´ÐµÐ»Ðµ Â«Ð£Ð¡Ð¢ÐÐÐžÐ’Ð˜Ð›Â» Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ÑÑ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… "
        "Ð´Ð»Ñ Ð¾ÐºÐ¾Ð½Ñ‡Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð¹ Ð¿Ñ€Ð°Ð²Ð¾Ð²Ð¾Ð¹ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð´ÐµÑÐ½Ð¸Ñ.\n"
    )


# ============================================================
# ðŸ”§ ÐŸÑƒÑÑ‚Ð¾Ð¹ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚
# ============================================================

def _empty_result(case_id: Optional[str], msg: str, fio: str, line: str) -> Dict[str, Any]:
    return {
        "generation_id": None,
        "model_version": MODEL_VERSION,
        "case_id": case_id,
        "established_text": msg,
        "final_postanovlenie": msg,
        "facts_used": [],
        "used_tokens": [],
        "case_meta": {},
        "verification": {"error": msg, "overall_ok": False},
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "investigator_fio": fio,
        "investigator_line": line,
        "city": None,
        "date": None,
        "ustanovil_word_count": 0,
        "ustanovil_sentence_count": 0,
    }
