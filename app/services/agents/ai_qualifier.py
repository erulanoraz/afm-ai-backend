# app/services/agents/ai_qualifier.py
from __future__ import annotations

import logging
import re
import uuid
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple

from fastapi import HTTPException

from app.services.llm_client import LLMClient
from app.services.agents.ai_laws import ALL_AFM_LAWS
from app.services.agents.ai_extractor import extract_all, super_pre_filter
from app.services.reranker import LLMReranker
from app.services.validation.verifier import run_full_verification
from app.services.agents import prompts

logger = logging.getLogger(__name__)

# ============================================================
# ‚öôÔ∏è –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ / –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã
# ============================================================

MODEL_VERSION = "qualifier-llm-3.0"
MIN_FACT_CONFIDENCE = 0.5
CONTEXT_RADIUS = 60

_llm_client = LLMClient()


# ============================================================
# üß© –ö–∞—Å—Ç–æ–º–Ω—ã–µ –æ—à–∏–±–∫–∏
# ============================================================

class LLMUnavailableError(Exception):
    """–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –∏–ª–∏ –æ—à–∏–±–∫–µ LLM."""
    pass


# ============================================================
# üîå –û–±—ë—Ä—Ç–∫–∞ –Ω–∞–¥ LLMClient (–µ–¥–∏–Ω–∞—è —Ç–æ—á–∫–∞ –≤—ã–∑–æ–≤–∞)
# ============================================================

def _ask_llm(prompt: str, system_prompt: Optional[str] = None) -> str:
    """
    –í—ã–∑–æ–≤ LLM —á–µ—Ä–µ–∑ –æ–±—â–∏–π –∫–ª–∏–µ–Ω—Ç.
    –ï—Å–ª–∏ LLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω / –≤–µ—Ä–Ω—É–ª –æ—à–∏–±–∫—É ‚Äî –ø–æ–¥–Ω–∏–º–∞–µ–º LLMUnavailableError.
    """
    messages: List[Dict[str, str]] = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": prompt})

    try:
        content = _llm_client.chat(messages)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ LLM: {e}")
        raise LLMUnavailableError(str(e))

    if not content or (isinstance(content, str) and content.startswith("[LLM ERROR]")):
        raise LLMUnavailableError(content or "–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç LLM")

    return str(content).strip()


# ============================================================
# üßÆ –†–µ–≥—É–ª—è—Ä–∫–∏ –¥–ª—è –ø–µ—Ä–≤–∏—á–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
# ============================================================

PERSON_RX = re.compile(
    r"\b([–ê-–Ø–Å][–∞-—è—ë]+(?:\s+[–ê-–Ø–Å]\.){1,2}|[–ê-–Ø–Å][–∞-—è—ë]+(?:\s+[–ê-–Ø–Å][–∞-—è—ë]+){1,2})\b"
)
DATE_RX = re.compile(
    r"\b(\d{1,2}[./-]\d{1,2}[./-]\d{2,4}|\d{4}-\d{2}-\d{2})\b"
)
MONEY_RX = re.compile(
    r"(?:(\d{1,3}(?:\s?\d{3})+|\d+)(?:[.,]\d{1,2})?)\s?(?:—Ç–≥|—Ç–µ–Ω–≥–µ|KZT|‚Ç∏)",
    re.IGNORECASE,
)
ART_RX = re.compile(
    r"(—Å—Ç\.?|—Å—Ç–∞—Ç—å[—å—è–∏])\s*([0-9]{1,3}(?:[-‚Äì][0-9]+)?)(?:\s*(–£–ö|–£–ü–ö|–ì–ö)\s*–†–ö)?",
    re.IGNORECASE,
)

SENTENCE_SPLIT_RX = re.compile(r"(?<=[\.\?\!])\s+")


def _split_sentences(text: str) -> List[str]:
    if not text:
        return []
    parts = SENTENCE_SPLIT_RX.split(text)
    return [p.strip() for p in parts if p.strip()]


# ============================================================
# üîé –§–∏–ª—å—Ç—Ä —á–∏—Å—Ç–æ –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ / –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
# ============================================================

def _is_procedural_sentence(t: str) -> bool:
    lt = t.lower()

    blocked = [
        "–ø–æ—Ç–µ—Ä–ø–µ–≤—à–∏–π –∏–º–µ–µ—Ç –ø—Ä–∞–≤–æ",
        "–ø–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º—ã–π –∏–º–µ–µ—Ç –ø—Ä–∞–≤–æ",
        "—Ä–∞–∑—ä—è—Å–Ω–µ–Ω—ã –ø—Ä–∞–≤–∞",
        "–µ–º—É —Ä–∞–∑—ä—è—Å–Ω–µ–Ω—ã –ø—Ä–∞–≤–∞",
        "–µ–π —Ä–∞–∑—ä—è—Å–Ω–µ–Ω—ã –ø—Ä–∞–≤–∞",
        "—Ä–∞–∑—ä—è—Å–Ω–µ–Ω—ã –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏",
        "—É–≥–æ–ª–æ–≤–Ω–æ-–ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–π –∫–æ–¥–µ–∫—Å",
        "—Å—Ç. 64 —É–ø–∫ —Ä–∫",
        "—Å—Ç. 71 —É–ø–∫ —Ä–∫",
        "—Å—Ç. 73 —É–ø–∫ —Ä–∫",
        "–≤–æ–ø—Ä–æ—Å:", "–æ—Ç–≤–µ—Ç:",
        "–¥–æ–ø—Ä–æ—Å –Ω–∞—á–∞—Ç", "–¥–æ–ø—Ä–æ—Å –æ–∫–æ–Ω—á–µ–Ω",
        "—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ –Ω–∞—á–∞—Ç–æ",
        "—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ –æ–∫–æ–Ω—á–µ–Ω–æ",
        "–∫–∞–±–∏–Ω–µ—Ç ‚Ññ", "—Å–ª—É–∂–µ–±–Ω—ã–π –∫–∞–±–∏–Ω–µ—Ç",
        "–∞—É–¥–∏–æ–∑–∞–ø–∏—Å—å", "–≤–∏–¥–µ–æ–∑–∞–ø–∏—Å—å",
        "–∑–≤—É–∫–æ- –∏ (–∏–ª–∏) –≤–∏–¥–µ–æ–∑–∞–ø–∏—Å—å",
    ]

    tech = [
        "qr-–∫–æ–¥", "qr –∫–æ–¥",
        "—ç—Ü–ø", "ecp",
        "—ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç",
        "–¥–∞—Ç–∞ –∏ –≤—Ä–µ–º—è –ø–æ–¥–ø–∏—Å–∞–Ω–∏—è",
        "–ø–æ–¥–ø–∏—Å—å –Ω–∞–ª–æ–∂–µ–Ω–∞",
    ]

    return any(b in lt for b in blocked) or any(tk in lt for tk in tech)


def _is_fact_sentence(t: str) -> bool:
    """
    –ö—Ä–∏—Ç–µ—Ä–∏–π: –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –°–£–©–ï–°–¢–í–ï–ù–ù–´–ô —Ñ–∞–∫—Ç (–¥–µ–Ω—å–≥–∏, –¥–µ–π—Å—Ç–≤–∏—è, —Ä–æ–ª—å, —É—â–µ—Ä–± –∏ —Ç.–ø.).
    """
    if not t:
        return False

    lt = t.lower().strip()
    if len(lt) < 15:
        return False

    # –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª–∫—É –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ–º
    if _is_procedural_sentence(lt):
        return False

    # 1) –£–ø–æ–º–∏–Ω–∞–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ä–æ–ª–µ–π
    if any(w in lt for w in ["–ø–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º", "–æ–±–≤–∏–Ω—è–µ–º", "–ø–æ—Ç–µ—Ä–ø–µ–≤—à", "—Å–≤–∏–¥–µ—Ç–µ–ª"]):
        return True

    # 2) –ü—Ä—è–º—ã–µ –¥–µ–π—Å—Ç–≤–∏—è —Å –¥–µ–Ω—å–≥–∞–º–∏
    money_actions = [
        "–≤–Ω–µ—Å", "–≤–Ω–µ—Å–ª–∞", "–≤–Ω–µ—Å–µ–Ω—ã",
        "–ø–µ—Ä–µ–≤–µ–ª", "–ø–µ—Ä–µ–≤–µ–ª–∞", "–ø–µ—Ä–µ—á–∏—Å–ª–∏–ª", "–ø–µ—Ä–µ—á–∏—Å–ª–∏–ª–∞",
        "–ø–µ—Ä–µ–¥–∞–ª", "–ø–µ—Ä–µ–¥–∞–ª–∞",
        "–æ—Ç–ø—Ä–∞–≤–∏–ª", "–æ—Ç–ø—Ä–∞–≤–∏–ª–∞",
        "–ø–æ–ø–æ–ª–Ω–∏–ª", "–ø–æ–ø–æ–ª–Ω–∏–ª–∞",
        "—Å–Ω—è–ª", "—Å–Ω—è–ª–∞", "–≤—ã–≤–µ–ª", "–≤—ã–≤–µ–ª–∞",
        "–ø–æ–ª—É—á–∏–ª", "–ø–æ–ª—É—á–∏–ª–∞",
    ]
    if any(w in lt for w in money_actions):
        return True

    # 3) –£—â–µ—Ä–± / –Ω–µ–≤–æ–∑–≤—Ä–∞—Ç
    if any(w in lt for w in ["—É—â–µ—Ä–±", "–¥–µ–Ω–µ–≥ –Ω–µ –≤–µ—Ä–Ω—É–ª–∏", "–¥–µ–Ω—å–≥–∏ –ø—Ä–æ–ø–∞–ª–∏", "–Ω–µ –≤–µ—Ä–Ω—É–ª–∏ –¥–µ–Ω—å–≥–∏"]):
        return True

    # 4) –û–±–º–∞–Ω / –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏
    if any(w in lt for w in ["–æ–±–º–∞–Ω", "–≤–≤–µ–ª –≤ –∑–∞–±–ª—É–∂–¥–µ–Ω–∏–µ", "–≤–ª–æ–∂–∏–ª", "–≤–ª–æ–∂–∏–ª–∞", "–∏–Ω–≤–µ—Å—Ç–∏—Ü", "–¥–æ—Ö–æ–¥", "–≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω"]):
        return True

    # 5) –Ø–≤–Ω–∞—è —Å—É–º–º–∞
    if MONEY_RX.search(t):
        return True

    # 6) –•—Ä–æ–Ω–æ–ª–æ–≥–∏—è / —Å–æ–±—ã—Ç–∏—è
    if any(w in lt for w in ["–ø—Ä–æ–∏–∑–æ—à–ª–æ", "—Å–ª—É—á–∏–ª–æ—Å—å", "–ø–æ—Å–ª–µ —ç—Ç–æ–≥–æ", "–≤ –¥–∞–ª—å–Ω–µ–π—à–µ–º", "–≤ —Ç–æ—Ç –∂–µ –¥–µ–Ω—å"]):
        return True

    return False


# ============================================================
# üîé –ò–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–≤ –¥–æ–ø—Ä–æ—Å–∞
# ============================================================

def _looks_like_interrogation_doc(doc: Dict[str, Any]) -> bool:
    name = (doc.get("filename") or "").lower()
    t0 = (doc.get("text") or "").lower()[:400]

    return (
        any(k in name for k in ["–¥–æ–ø—Ä–æ—Å", "–æ–ø—Ä–æ—Å", "–æ–±—ä—è—Å–Ω", "–ø–æ—è—Å–Ω–µ–Ω"])
        or "–ø—Ä–æ—Ç–æ–∫–æ–ª –¥–æ–ø—Ä–æ—Å–∞" in t0
        or "–ø—Ä–æ—Ç–æ–∫–æ–ª –æ–ø—Ä–æ—Å–∞" in t0
    )


def _clean_interrogation_text(raw: str) -> str:
    """
    –£–±–∏—Ä–∞–µ—Ç —à–∞–ø–∫—É, –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –∏ –≤–æ–ø—Ä–æ—Å—ã –∏–∑ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞ –¥–æ–ø—Ä–æ—Å–∞.
    –û—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –æ—Ç–≤–µ—Ç—ã –∏ –ø–æ–≤–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ.
    """
    lines = raw.splitlines()
    cleaned: List[str] = []
    in_body = False

    for line in lines:
        l = line.strip()
        if not l:
            continue

        low = l.lower()

        # —Å–ª—É–∂–µ–±–Ω–æ–µ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        if any(k in low for k in [
            "–ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º —Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è",
            "—Ä–∞–∑—ä—è—Å–Ω–µ–Ω—ã –ø—Ä–∞–≤–∞",
            "–ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω", "–ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∞",
            "–æ–± –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ç.",
            "–µ–º—É —Ä–∞–∑—ä—è—Å–Ω–µ–Ω–æ", "–µ–π —Ä–∞–∑—ä—è—Å–Ω–µ–Ω–æ",
            "–∫–æ–ø–∏—é –ø—Ä–æ—Ç–æ–∫–æ–ª–∞ –ø–æ–ª—É—á–∏–ª",
        ]):
            continue

        # —É–±–∏—Ä–∞–µ–º –≤–æ–ø—Ä–æ—Å—ã
        if low.startswith("–≤–æ–ø—Ä–æ—Å:") or low.startswith("–≤–æ–ø—Ä–æ—Å ‚Ññ"):
            continue

        # –Ω–∞—á–∞–ª–æ —Ç–µ–ª–∞
        if not in_body and any(k in low for k in ["–ø–æ—è—Å–Ω–∏–ª", "–ø–æ—è—Å–Ω–∏–ª–∞", "—Å–æ–æ–±—â–∏–ª", "—Å–æ–æ–±—â–∏–ª–∞", "–ø–æ–∫–∞–∑–∞–ª", "–ø–æ–∫–∞–∑–∞–ª–∞"]):
            in_body = True

        if in_body:
            cleaned.append(l)

    return "\n".join(cleaned) if cleaned else raw


# ============================================================
# üîé –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∞–∫—Ç–æ–≤ –∏ –±–∞–∑–æ–≤—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ docs
# ============================================================

def _extract_facts_and_sources(
    docs: List[Dict[str, Any]]
) -> Tuple[List[Dict[str, Any]], List[str], List[str], List[str], List[Dict[str, Any]]]:
    """
    FACT-BUILDER:
    ‚Ä¢ –ö–∞–∂–¥—ã–π —Ñ–∞–∫—Ç = –æ–¥–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∏–∑ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
    ‚Ä¢ –£ –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–∫—Ç–∞ –µ—Å—Ç—å –º–∏–Ω–∏–º—É–º –æ–¥–∏–Ω –∏—Å—Ç–æ—á–Ω–∏–∫ {file_id, page}.
    """
    facts: List[Dict[str, Any]] = []
    persons: List[str] = []
    dates: List[str] = []
    amounts: List[str] = []
    sources: List[Dict[str, Any]] = []
    fact_id = 1

    for d in docs:
        text = (d.get("text") or "").strip()
        file_id = d.get("file_id")
        page = d.get("page")

        if not text:
            continue

        # –æ—á–∏—Å—Ç–∫–∞ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞ –¥–æ–ø—Ä–æ—Å–∞
        if _looks_like_interrogation_doc(d):
            text = _clean_interrogation_text(text)
            if not text.strip():
                continue

        src = {"file_id": file_id, "page": page}
        if file_id:
            sources.append(src)

        # —Å—É—â–Ω–æ—Å—Ç–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ –∫—É—Å–∫–∞
        for m in PERSON_RX.finditer(text):
            p = m.group(1)
            if p and len(p) > 2 and not any(x in p for x in ["–ê–û", "–¢–û–û", "–ò–ü", "–û–û–û"]):
                if p not in persons:
                    persons.append(p)

        for m in DATE_RX.finditer(text):
            dt = m.group(1)
            if dt not in dates:
                dates.append(dt)

        for m in MONEY_RX.finditer(text):
            amt = m.group(0)
            if amt not in amounts:
                amounts.append(amt)

        # –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è ‚Üí —Ñ–∞–∫—Ç—ã
        for sent in _split_sentences(text):
            sent_clean = sent.strip()
            if not sent_clean:
                continue
            if not _is_fact_sentence(sent_clean):
                continue

            low = sent_clean.lower()
            if "–ø–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º" in low and "–ø—Ä–∏–∑–Ω–∞—Ç—å" in low:
                fact_type = "status"
            else:
                fact_type = "event"

            facts.append(
                {
                    "fact_id": f"f{fact_id}",
                    "type": fact_type,
                    "text": sent_clean[:600],
                    "confidence": _conf_from_signal(sent_clean),
                    "sources": [src] if file_id else [],
                }
            )
            fact_id += 1

    # fallback: –µ—Å–ª–∏ –Ω–µ—Ç —Ñ–∞–∫—Ç–æ–≤, –Ω–æ –µ—Å—Ç—å —Å—É—â–Ω–æ—Å—Ç–∏
    if not facts and (persons or dates or amounts):
        base_parts = []
        if persons:
            base_parts.append(f"–£—á–∞—Å—Ç–Ω–∏–∫–∏: {', '.join(persons[:5])}")
        if dates:
            base_parts.append(f"–î–∞—Ç—ã: {', '.join(dates[:5])}")
        if amounts:
            base_parts.append(f"–°—É–º–º—ã: {', '.join(amounts[:5])}")

        if base_parts:
            facts.append(
                {
                    "fact_id": f"f{fact_id}",
                    "type": "context",
                    "text": "; ".join(base_parts),
                    "confidence": 0.55,
                    "sources": [sources[0]] if sources else [],
                }
            )

    uniq_sources = _dedup_sources(sources)
    return facts, persons, dates, amounts, uniq_sources


# ============================================================
# üß† –û–±–æ–≥–∞—â–µ–Ω–∏–µ —Ñ–∞–∫—Ç–æ–≤ —Ä–æ–ª—è–º–∏ / –¥–µ–π—Å—Ç–≤–∏—è–º–∏
# ============================================================

def enrich_facts_with_roles(facts: List[dict]) -> List[dict]:
    ROLE_HINTS = {
        "–ø–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º": "–ø–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º—ã–π",
        "–æ–±–≤–∏–Ω—è": "–æ–±–≤–∏–Ω—è–µ–º—ã–π",
        "—Å–≤–∏–¥–µ—Ç–µ–ª": "—Å–≤–∏–¥–µ—Ç–µ–ª—å",
        "–ø–æ—Ç–µ—Ä–ø–µ–≤—à": "–ø–æ—Ç–µ—Ä–ø–µ–≤—à–∏–π",
        "—Å–æ—É—á–∞—Å—Ç": "—Å–æ—É—á–∞—Å—Ç–Ω–∏–∫",
        "–æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä": "–æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä",
    }
    ACTION_HINTS = [
        "–ø–µ—Ä–µ–≤—ë–ª", "–ø–µ—Ä–µ–¥–∞–ª", "–ø–æ–ª—É—á–∏–ª", "–ø—Ä–∏–Ω—è–ª", "–ø—Ä–µ–¥–ª–æ–∂–∏–ª",
        "–æ–±–º–∞–Ω—É–ª", "–≤–≤–µ–ª –≤ –∑–∞–±–ª—É–∂–¥–µ–Ω–∏–µ", "—Å–æ–≤–µ—Ä—à–∏–ª", "–ø—Ä–∏—Å–≤–æ–∏–ª", "–≤—ã–º–æ–≥–∞–ª",
        "–æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–ª", "–∑–∞–∫–ª—é—á–∏–ª –¥–æ–≥–æ–≤–æ—Ä", "–ø–æ–ª—É—á–∏–ª –¥–æ—Å—Ç—É–ø", "—Å–Ω—è–ª –¥–µ–Ω—å–≥–∏",
    ]

    for f in facts:
        txt = (f.get("text") or "").lower()
        f["role"] = next((r for k, r in ROLE_HINTS.items() if k in txt), "–Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ")
        f["action"] = next((a for a in ACTION_HINTS if a in txt), None)
        f["time"] = next(
            (d for d in re.findall(r"\d{1,2}[./]\d{1,2}[./]\d{2,4}", txt)), None
        )
    return facts


# ============================================================
# üß± –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –¥–∞–Ω–Ω—ã—Ö –æ –ø–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º–æ–º
# ============================================================

def validate_facts_completeness(docs: List[Dict[str, Any]]) -> None:
    """
    –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å—Ç—å –ª–∏ –í–û–û–ë–©–ï –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –ø–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º–æ–≥–æ.
    –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –∏–∑ API-—Ä–æ—É—Ç–µ—Ä–∞ –î–û –∑–∞–ø—É—Å–∫–∞ –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞.
    """
    if not docs:
        raise HTTPException(
            status_code=400,
            detail="‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, –∑–∞–≥—Ä—É–∂–µ–Ω—ã –ª–∏ —Ñ–∞–π–ª—ã –ø–æ –¥–µ–ª—É.",
        )

    has_suspect = any("–ø–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º" in (d.get("text") or "").lower() for d in docs)
    if not has_suspect:
        raise HTTPException(
            status_code=404,
            detail=(
                "‚ùå –í —Ç–µ–∫—Å—Ç–∞—Ö –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Å–≤–µ–¥–µ–Ω–∏—è –æ –ø–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º–æ–º. "
                "–¢—Ä–µ–±—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∏—Ç—å OCR –∏ –ø–æ–ª–Ω–æ—Ç—É –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤."
            ),
        )


# ============================================================
# ‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–ª–Ω–æ—Ç—ã –ø–æ —Å—Ç. 204 –£–ü–ö –†–ö
# ============================================================

def _check_204_completeness(
    facts: List[Dict[str, Any]],
    persons: List[str],
    dates: List[str],
    amounts: List[str],
    roles: Optional[Dict[str, Any]] = None,
    events: Optional[List[Dict[str, Any]]] = None,
    legal_facts: Optional[Dict[str, Any]] = None,
    timeline: Optional[List[Dict[str, Any]]] = None,
) -> Dict[str, Any]:
    roles = roles or {}
    events = events or []
    legal_facts = legal_facts or {}

    def present(x: Any) -> bool:
        return bool(x)

    checklist = [
        {
            "item": "–£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ª–∏—á–Ω–æ—Å—Ç—å –ø–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º–æ–≥–æ",
            "present": present(roles.get("suspect")),
        },
        {
            "item": "–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ —Ä–æ–ª—å –ø–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º–æ–≥–æ",
            "present": present(roles.get("suspect_role")),
        },
        {
            "item": "–ï—Å—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ —Å–æ–±—ã—Ç–∏–π",
            "present": present(events),
        },
        {
            "item": "–ï—Å—Ç—å –¥–∞—Ç—ã —Å–æ–±—ã—Ç–∏–π",
            "present": present(dates),
        },
        {
            "item": "–ï—Å—Ç—å —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Å–≤–µ–¥–µ–Ω–∏—è",
            "present": present(amounts),
        },
        {
            "item": "–í—ã–¥–µ–ª–µ–Ω—ã —é—Ä–∏–¥–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏",
            "present": present(legal_facts),
        },
        {
            "item": "–ï—Å—Ç—å —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ (–≤—ã–¥–µ—Ä–∂–∫–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)",
            "present": present(facts),
        },
    ]

    missing = [x["item"] for x in checklist if not x["present"]]

    return {
        "article": "204 –£–ü–ö –†–ö",
        "checklist": checklist,
        "missing": missing,
        "enough_for_draft": len(missing) <= 3,
    }


# ============================================================
# ‚öñÔ∏è –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–µ–π –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤
# ============================================================

def _extract_articles(docs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    arts: List[Dict[str, Any]] = []
    for d in docs:
        text = d.get("text") or ""
        file_id = d.get("file_id")
        page = d.get("page")
        for m in ART_RX.finditer(text):
            art_num = m.group(2)
            code = (m.group(3) or "–£–ö/–£–ü–ö/–ì–ö?").upper()
            arts.append(
                {
                    "code": (code.replace(" ", "") + " –†–ö") if "–†–ö" not in code else code,
                    "article": art_num,
                    "context": _context_snippet(text, m.start(), m.end()),
                    "source": {"file_id": file_id, "page": page},
                }
            )
    return _dedup_articles(arts)


def _resolve_law_context(article_num: str) -> str:
    data = ALL_AFM_LAWS.get(article_num)
    if not data:
        return f"–°—Ç–∞—Ç—å—è {article_num}: [–æ–ø–∏—Å–∞–Ω–∏–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –±–∞–∑–µ AFM]"

    code = data.get("code", "–£–ö/–£–ü–ö –†–ö")
    name = data.get("name", "[–Ω–∞–∑–≤–∞–Ω–∏–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç]")
    official_text = data.get("official_text") or data.get("text") or "[—Ç–µ–∫—Å—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç]"
    category = data.get("category", "–ø—Ä–æ—á–µ–µ")

    return f"{code} —Å—Ç.{article_num} ‚Äî {name}. {official_text} ({category})"


# ============================================================
# üßæ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∞ ¬´–£–°–¢–ê–ù–û–í–ò–õ¬ª (–±–∞–∑–æ–≤—ã–π, –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π)
# ============================================================

def _build_ustanovil_base(
    facts: List[dict],
    completeness: Dict[str, Any],
    suspect: Optional[str] = None,
    suspect_role: Optional[str] = None,
) -> str:
    """
    –ë–∞–∑–æ–≤–∞—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è ¬´–£–°–¢–ê–ù–û–í–ò–õ¬ª:
    - –ø—Ä–æ—Å—Ç–æ –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ñ–∞–∫—Ç–æ–≤
    - –±–µ–∑ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏—è
    - —Å [file_id:page] –≤ –∫–æ–Ω—Ü–µ –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–∫—Ç–∞
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ fallback –∏ –≤—Ö–æ–¥ –¥–ª—è LLM.
    """
    if not facts and not suspect:
        return "–°—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ñ–∞–∫—Ç–æ–≤ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ. –¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞."

    lines: List[str] = ["–£–°–¢–ê–ù–û–í–ò–õ:"]

    # –ë–ª–æ–∫ –æ –ø–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º–æ–º (–µ—Å–ª–∏ –µ—Å—Ç—å)
    if suspect:
        line = f"–ò–∑ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –¥–µ–ª–∞ —É—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è, —á—Ç–æ {suspect}"
        if suspect_role:
            line += f", –≤—ã–ø–æ–ª–Ω—è—è —Ä–æ–ª—å {suspect_role},"
        else:
            line += ","
        line += " —Ñ–∏–≥—É—Ä–∏—Ä—É–µ—Ç –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ª–∏—Ü–∞, –≤ –æ—Ç–Ω–æ—à–µ–Ω–∏–∏ –∫–æ—Ç–æ—Ä–æ–≥–æ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –¥–æ—Å—É–¥–µ–±–Ω–æ–µ —Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ."
        lines.append(line)
        lines.append("")

    # –§–∞–∫—Ç—ã
    for i, f in enumerate(facts, 1):
        src_str = _src_str(f.get("sources"))
        conf = f.get("confidence", 0.5)
        suffix = "" if conf >= 0.75 else " [‚ö†Ô∏è –Ω–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å]"
        lines.append(f"{i}. {f['text']} {src_str}{suffix}")

    # –ù–µ–¥–æ—Å—Ç–∞—é—â–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã
    missing = completeness.get("missing") or []
    if missing:
        lines.append("")
        lines.append("–ù–µ–¥–æ—Å—Ç–∞—é—â–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã –¥–ª—è –ø–æ–ª–Ω–æ–π –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ —Å—Ç. 204 –£–ü–ö –†–ö:")
        for m in missing:
            lines.append(f"‚Ä¢ {m}")

    return "\n".join(lines)


# ============================================================
# üßæ Fallback-–ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ (–±–µ–∑ LLM)
# ============================================================

def _build_postanovlenie_simple(
    city: str,
    date_str: str,
    investigator_line: str,
    case_id: Optional[str],
    ustanovil_text: str,
    mentioned_articles: List[Dict[str, Any]],
    completeness: Dict[str, Any],
    investigator_fio: str = "",
    intro_context: str = "",
) -> str:
    rus_date = _rus_date(date_str)

    # —Å–ø–∏—Å–æ–∫ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π —Å—Ç–∞—Ç–µ–π –£–ö
    if mentioned_articles:
        arts_filtered = [a for a in mentioned_articles if "–£–ö" in a.get("code", "")]
        if arts_filtered:
            arts = sorted({f"{a.get('code', '')} —Å—Ç.{a.get('article', '?')}" for a in arts_filtered})
            arts_line = "–£–ø–æ–º–∏–Ω–∞–Ω–∏—è —Å—Ç–∞—Ç–µ–π –£–ö: " + "; ".join(arts)
        else:
            arts_line = "–£–ø–æ–º–∏–Ω–∞–Ω–∏–π —Å—Ç–∞—Ç–µ–π –£–ö –Ω–µ –≤—ã—è–≤–ª–µ–Ω–æ."
    else:
        arts_line = "–£–ø–æ–º–∏–Ω–∞–Ω–∏–π —Å—Ç–∞—Ç–µ–π –Ω–µ—Ç."

    # —Ä–µ—à–µ–Ω–∏–µ
    if completeness.get("enough_for_draft"):
        decision = "–ö–≤–∞–ª–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –¥–µ—è–Ω–∏–µ –ø–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º–æ–≥–æ –ø–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º —Å—Ç–∞—Ç—å—è–º –£–ö –†–µ—Å–ø—É–±–ª–∏–∫–∏ –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω."
    else:
        decision = "–û–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—É—é –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏—é –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤."

    intro_block = ""
    if intro_context:
        intro_block = intro_context.strip() + "\n\n"

    return f"""–ü–û–°–¢–ê–ù–û–í–õ–ï–ù–ò–ï
–æ –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–µ—è–Ω–∏—è –ø–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º–æ–≥–æ

{city}, {rus_date}

–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –¥–µ–ª–∞ ‚Ññ {case_id}
{arts_line}

{intro_block}{ustanovil_text}

–ü–û–°–¢–ê–ù–û–í–ò–õ:
{decision}

–ü–æ–¥–ø–∏—Å—å:
–°–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å: {investigator_line}
–§–ò–û: {investigator_fio}
______________________
–î–∞—Ç–∞: {rus_date}

–ß–µ—Ä–Ω–æ–≤–∏–∫ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏; –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å –ø–æ—Å–ª–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –ø—Ä–æ–∫—É—Ä–æ—Ä–æ–º.
""".strip()


# ============================================================
# üéØ –§–∏–ª—å—Ç—Ä—ã —é—Ä–∏–¥–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º—ã—Ö —Ñ–∞–∫—Ç–æ–≤
# ============================================================

def _legal_fact_filter(fact_text: str) -> bool:
    if not fact_text:
        return False

    t = fact_text.lower().strip()

    rights_noise = [
        "—Å—Ç. 67 —É–ø–∫ —Ä–∫",
        "—Å—Ç–∞—Ç—å—è 67 —É–ø–∫ —Ä–∫",
        "–ø–æ—Ç–µ—Ä–ø–µ–≤—à–∏–π –∏–º–µ–µ—Ç –ø—Ä–∞–≤–æ",
        "–ø–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º—ã–π –∏–º–µ–µ—Ç –ø—Ä–∞–≤–æ",
        "–ø—Ä–∞–≤–∞ –∏ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏ –ø–æ—Ç–µ—Ä–ø–µ–≤—à–µ–≥–æ",
        "–ø—Ä–∞–≤–∞ –∏ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏ –ø–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º–æ–≥–æ",
        "–æ–Ω –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–¥–≤–µ—Ä–≥–Ω—É—Ç –ø—Ä–∏–≤–æ–¥—É",
        "–Ω–∞–ª–æ–∂–µ–Ω–æ –¥–µ–Ω–µ–∂–Ω–æ–µ –≤–∑—ã—Å–∫–∞–Ω–∏–µ",
        "–ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∞",
    ]
    if any(w in t for w in rights_noise):
        return False

    blocked_pdf = [
        "qr-–∫–æ–¥", "qr –∫–æ–¥", "—Ö–µ—à", "—Ö—ç—à", "hash",
        "ecp", "—ç—Ü–ø", "—ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç",
        "–¥–∞–Ω–Ω—ã–µ —ç—Ü–ø", "–∫–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞", "–≤—Ä–µ–º—è –ø–æ–¥–ø–∏—Å–∞–Ω–∏—è",
        "–¥–∞—Ç–æ–π –∏ –≤—Ä–µ–º–µ–Ω–µ–º –ø–æ–¥–ø–∏—Å–∞–Ω–∏—è", "—ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã–π pdf",
    ]
    if any(w in t for w in blocked_pdf):
        return False

    if "—Ä–∞–∑—ä—è—Å–Ω" in t and "–ø—Ä–∞–≤" in t and "–ø–æ–∫–∞–∑–∞–Ω" not in t and "–ø–æ—è—Å–Ω–∏–ª" not in t:
        return False

    if "—è–≤–∏—Ç—å—Å—è –ø–æ –≤—ã–∑–æ–≤—É" in t or "–Ω–µ —Ä–∞–∑–≥–ª–∞—à–∞—Ç—å —Å–≤–µ–¥–µ–Ω–∏—è" in t:
        return False

    if "–≤–∏–¥–µ–æ–∫–∞–º–µ—Ä–∞" in t or "–≤–∏–¥–µ–æ–∑–∞–ø–∏—Å—å" in t or "–∞—É–¥–∏–æ–∑–∞–ø–∏—Å—å" in t:
        return False

    intro_markers = [
        "–ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –¥–æ—Å—É–¥–µ–±–Ω–æ–µ —Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ",
        "—Ä–∞—Å—Å–º–æ—Ç—Ä–µ–≤ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –¥–æ—Å—É–¥–µ–±–Ω–æ–≥–æ —Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è",
        "—Ä–∞—Å—Å–º–æ—Ç—Ä–µ–≤ –º–∞—Ç–µ—Ä–∏–∞–ª—ã —É–≥–æ–ª–æ–≤–Ω–æ–≥–æ –¥–µ–ª–∞",
        "–º–∞—Ç–µ—Ä–∏–∞–ª—ã –¥–æ—Å—É–¥–µ–±–Ω–æ–≥–æ —Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è ‚Ññ",
        "–º–∞—Ç–µ—Ä–∏–∞–ª—ã —É–≥–æ–ª–æ–≤–Ω–æ–≥–æ –¥–µ–ª–∞ ‚Ññ",
        "–º–∞—Ç–µ—Ä–∏–∞–ª—ã –¥–µ–ª–∞ ‚Ññ",
        "—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å —Å–æ–≥",
        "—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å —Å—É –¥–µ—Ä",
        "—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å —Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ–π-–æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –≥—Ä—É–ø–ø—ã",
    ]
    if any(m in t for m in intro_markers):
        return False

    plan_markers = [
        "—Å–∞–Ω–∫—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ –æ–±—ã—Å–∫–æ–≤—ã—Ö –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π",
        "—Å–∞–Ω–∫—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ –æ–±—ã—Å–∫–∞",
        "–Ω–∞–ø—Ä–∞–≤–∏—Ç—å –≤ —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∞–ª—å–Ω—ã–µ –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç—ã",
        "–ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–æ–±—Ä–µ—Ç–µ–Ω–∏–µ –∏–º—É—â–µ—Å—Ç–≤–∞",
        "–∏—Å—Ç—Ä–µ–±–æ–≤–∞—Ç—å —Å–ø—Ä–∞–≤–∫–∏ –∏ –¥–µ–∫–ª–∞—Ä–∞—Ü–∏–∏ –æ –¥–æ—Ö–æ–¥–∞—Ö",
        "–ø–æ—Ä—É—á–∏—Ç—å –ø—Ä–æ–≤–µ—Å—Ç–∏",
        "–ø—Ä–æ–≤–µ—Å—Ç–∏ –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ-—Ä–æ–∑—ã—Å–∫–Ω—ã–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è",
    ]
    if any(m in t for m in plan_markers):
        return False

    if len(t) < 15:
        return False

    # –µ—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π / —É—â–µ—Ä–±–∞
    crime_keywords = [
        "–ø–∏—Ä–∞–º–∏–¥", "–≤–æ–≤–ª–µ–∫", "–ø—Ä–∏–≤–ª–µ–∫", "–≤–Ω–µ—Å", "–≤–Ω–µ—Å–ª–∞",
        "–≤–ª–æ–∂–∏–ª", "–≤–ª–æ–∂–∏–ª–∞", "–ø–µ—Ä–µ–≤–µ–ª", "–ø–µ—Ä–µ–≤–µ–ª–∞", "–ø–µ—Ä–µ—á–∏—Å–ª–∏–ª",
        "–¥–µ–Ω–µ–∂–Ω", "–¥–µ–Ω—å–≥–∏", "—Å—Ä–µ–¥—Å—Ç–≤–∞", "—É—â–µ—Ä–±", "–±–∞–ª–∞–Ω—Å", "usdt",
        "–ø—Ä–∏–ª–æ–∂–µ–Ω–∏", "–ø–ª–∞—Ç—Ñ–æ—Ä–º", "–∏–Ω–≤–µ—Å—Ç", "–¥–æ—Ö–æ–¥", "–≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ",
    ]
    if any(w in t for w in crime_keywords):
        return True

    action_words = ["—Å–æ–≤–µ—Ä—à", "–æ—Ä–≥–∞–Ω–∏–∑", "—Ä—É–∫–æ–≤–æ–¥", "–ø–æ–ª—É—á–∏–ª", "–ø–æ–ª—É—á–∏–ª–∞", "–ø—Ä–∏—Å–≤–æ", "–æ–±–º–∞–Ω—É–ª"]
    if any(w in t for w in action_words):
        return True

    return True


def _hard_fact_clean(fact_text: str) -> bool:
    if not fact_text:
        return False

    t = fact_text.lower().strip()

    noise = [
        "—è –ª–∏—á–Ω–æ –Ω–∏—á–µ–≥–æ –Ω–µ –ø–æ–º–Ω—é",
        "—è –Ω–µ –∑–Ω–∞—é", "–º—ã –¥—É–º–∞–ª–∏", "–∫–∞–∫-—Ç–æ", "–≤—Ä–æ–¥–µ",
        "–º–∞–º–∞ —Å–∫–∞–∑–∞–ª–∞", "—Å–æ—Å–µ–¥ —Ä–∞—Å—Å–∫–∞–∑–∞–ª",
    ]
    if any(p in t for p in noise):
        return False

    procedural = [
        "—É–ø–∫ —Ä–∫", "—É–≥–æ–ª–æ–≤–Ω–æ-–ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–π –∫–æ–¥–µ–∫—Å",
        "–∏–º–µ–µ—Ç –ø—Ä–∞–≤–æ", "–æ–±—è–∑–∞–Ω", "–æ–±—è–∑–∞–Ω–∞",
        "—Ä–∞–∑—ä—è—Å–Ω–µ–Ω—ã –ø—Ä–∞–≤–∞", "–ø—Ä–∞–≤–∞ –∏ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏",
        "–ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω –æ–± –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏", "–ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∞ –æ–± –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏",
    ]
    if any(p in t for p in procedural) and "–ø–æ—è—Å–Ω–∏–ª" not in t and "–ø–æ—è—Å–Ω–∏–ª–∞" not in t:
        return False

    tech = ["qr-–∫–æ–¥", "qr –∫–æ–¥", "ecp", "—ç—Ü–ø", "pdf", "—Å–∫–∞–Ω-–∫–æ–ø–∏—è"]
    if any(k in t for k in tech):
        return False

    return True


# ============================================================
# ü§ñ –ê–≤—Ç–æ-–∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
# ============================================================

def _auto_qualify(
    facts: List[Dict[str, Any]],
    roles: Dict[str, Any],
    events: List[Dict[str, Any]],
    legal_facts: Dict[str, Any],
) -> Tuple[Optional[str], Optional[str], str]:
    text_all = " ".join(f.get("text", "").lower() for f in facts)

    pyramid_keywords = [
        "–ø–∏—Ä–∞–º–∏–¥–∞", "–≤–ª–æ–∂–∏–ª", "–ø—Ä–∏–≤–ª–µ–∫", "–∑–∞–≤–ª–µ–∫–∞–ª",
        "–≤–æ–≤–ª–µ–∫–∞–ª", "–≤—Å—Ç—É–ø–∏–ª", "—Å—Ö–µ–º–∞", "–¥–æ—Ö–æ–¥ –∑–∞ —Å—á–µ—Ç –Ω–æ–≤—ã—Ö —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤",
        "–ø–µ—Ä–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ", "–∏–Ω–≤–µ—Å—Ç –ø—Ä–æ–µ–∫—Ç –±–µ–∑ –∞–∫—Ç–∏–≤–∞",
    ]
    if any(k in text_all for k in pyramid_keywords):
        return (
            "217",
            "1",
            "–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π –ø–∏—Ä–∞–º–∏–¥—ã (–ø—Ä–∏–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ä–µ–¥—Å—Ç–≤, –ø–µ—Ä–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–∫–ª–∞–¥–æ–≤).",
        )

    fraud_keywords = [
        "–æ–±–º–∞–Ω", "–∑–∞–±–ª—É–∂–¥–µ–Ω–∏", "–º–æ—à–µ–Ω–Ω–∏—á", "–ø—Ä–∏—Å–≤–æ–∏–ª",
        "–∑–∞–≤–µ–¥–æ–º–æ –ª–æ–∂–Ω—ã–µ", "–Ω–µ–∑–∞–∫–æ–Ω–Ω–æ –ø–æ–ª—É—á–∏–ª",
    ]
    if any(k in text_all for k in fraud_keywords):
        return (
            "190",
            "2",
            "–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–∏–∑–Ω–∞–∫–∏ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞ (–æ–±–º–∞–Ω, –≤–≤–µ–¥–µ–Ω–∏–µ –≤ –∑–∞–±–ª—É–∂–¥–µ–Ω–∏–µ, –ø—Ä–∏—Å–≤–æ–µ–Ω–∏–µ).",
        )

    business_keywords = [
        "–±–µ–∑ –ª–∏—Ü–µ–Ω–∑–∏–∏", "–Ω–µ–∑–∞–∫–æ–Ω–Ω–∞—è –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å—Å–∫–∞—è", "–Ω–µ–ª–µ–≥–∞–ª—å–Ω–∞—è –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å",
        "–æ–∫–∞–∑–∞–Ω–∏–µ —É—Å–ª—É–≥ –±–µ–∑ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏", "–±–µ–∑ —É–¥–æ—Å—Ç–æ–≤–µ—Ä–µ–Ω–∏—è", "–Ω–µ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω",
    ]
    if any(k in text_all for k in business_keywords):
        return (
            "214",
            "1",
            "–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–µ–∑–∞–∫–æ–Ω–Ω–æ–π –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å—Å–∫–æ–π –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.",
        )

    laundering_keywords = [
        "–ª–µ–≥–∞–ª–∏–∑", "–æ—Ç–º—ã–≤–∞–ª", "—Å–∫—Ä—ã–≤–∞–ª –ø—Ä–æ–∏—Å—Ö–æ–∂–¥–µ–Ω–∏–µ", "–¥–≤–∏–∂–µ–Ω–∏–µ –¥–µ–Ω–µ–∂–Ω—ã—Ö —Å—Ä–µ–¥—Å—Ç–≤",
    ]
    if any(k in text_all for k in laundering_keywords):
        return (
            "218",
            "1",
            "–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–∏–∑–Ω–∞–∫–∏ –ª–µ–≥–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–æ—Ö–æ–¥–æ–≤ (–æ—Ç–º—ã–≤–∞–Ω–∏–µ –¥–µ–Ω–µ–≥).",
        )

    return None, None, "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–∏."


def classify_crime(facts: List[Dict[str, Any]]) -> Dict[str, Any]:
    text_blob = " ".join(f.get("text", "").lower() for f in facts)

    flags = {
        "190": any(kw in text_blob for kw in [
            "–æ–±–º–∞–Ω", "–∑–∞–±–ª—É–∂–¥–µ–Ω–∏", "–Ω–µ –≤–µ—Ä–Ω—É–ª", "–ø–æ–ª—É—á–∏–ª –¥–µ–Ω—å–≥–∏",
            "–Ω–µ –≤—ã–ø–æ–ª–Ω–∏–ª –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤", "—É—â–µ—Ä–±", "–≤–≤–µ–ª –≤ –∑–∞–±–ª—É–∂–¥",
        ]),
        "217": any(kw in text_blob for kw in [
            "–ø—Ä–∏–≤–ª–µ–∫", "–≤–æ–≤–ª–µ–∫", "–æ–±–µ—â–∞–ª –¥–æ—Ö–æ–¥", "–≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ",
            "—É—á–∞—Å—Ç–Ω–∏–∫", "—Å—Ç—Ä—É–∫—Ç—É—Ä", "–¥–æ—Ö–æ–¥ –∑–∞ —Å—á–µ—Ç –¥—Ä—É–≥–∏—Ö",
            "–º–∞—Å—Å–æ–≤–æ–µ –ø—Ä–∏–≤–ª–µ—á–µ–Ω–∏–µ", "–≤—ã—Å–æ–∫–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç",
        ]),
        "385": any(kw in text_blob for kw in [
            "–±–µ–∑ –ª–∏—Ü–µ–Ω–∑–∏–∏", "–±–µ–∑ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏", "–Ω–µ–∑–∞–∫–æ–Ω–Ω",
            "–ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å—Å–∫–∞—è –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å", "–ø—Ä–∏–±—ã–ª—å –±–µ–∑ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏",
        ]),
        "189": any(kw in text_blob for kw in [
            "–¥–æ–≤–µ—Ä–µ–Ω", "—Ä–∞—Å–ø–æ—Ä—è–¥–∏–ª—Å—è —á—É–∂–∏–º", "–ø—Ä–∏—Å–≤–æ–∏–ª",
            "—Ä–∞—Å—Ç—Ä–∞—Ç–∞", "–∏–º—É—â–µ—Å—Ç–≤–æ –±—ã–ª–æ –ø–µ—Ä–µ–¥–∞–Ω–æ",
        ]),
        "218": any(kw in text_blob for kw in [
            "—Å–∫—Ä—ã—Ç—å –ø—Ä–æ–∏—Å—Ö–æ–∂–¥–µ–Ω–∏–µ", "–æ–±–Ω–∞–ª–∏—á", "–ø–µ—Ä–µ–≤–µ–ª –º–µ–∂–¥—É —Å—á–µ—Ç–∞–º–∏",
            "–ª–µ–≥–∞–ª–∏–∑", "–º–∞—Å–∫–∏—Ä–æ–≤",
        ]),
    }

    primary = None
    if flags["217"]:
        primary = "217"
    elif flags["190"]:
        primary = "190"
    elif flags["189"]:
        primary = "189"
    elif flags["218"]:
        primary = "218"
    elif flags["385"]:
        primary = "385"

    secondary = [art for art, ok in flags.items() if ok and art != primary]

    return {"primary": primary, "secondary": secondary}


# ============================================================
# üß† –ü—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (–¥–æ ¬´–£–°–¢–ê–ù–û–í–ò–õ¬ª)
# ============================================================

def _extract_intro_context(docs: List[Dict[str, Any]]) -> str:
    intro_sentences: List[str] = []

    for d in docs:
        text = d.get("text") or ""
        if not text.strip():
            continue

        for sent in _split_sentences(text):
            lt = sent.lower()

            ban = ["–ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ", "–æ –ø—Ä–∏–∑–Ω–∞–Ω–∏–∏", "–ø–æ—Ç–µ—Ä–ø–µ–≤—à–∏–º"]
            if any(b in lt for b in ban):
                continue

            if "–ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –¥–æ—Å—É–¥–µ–±–Ω–æ–µ —Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ" in lt:
                intro_sentences.append(sent.strip())
                continue

            if "—Ä–∞—Å—Å–º–æ—Ç—Ä–µ–≤ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –¥–æ—Å—É–¥–µ–±–Ω–æ–≥–æ —Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è" in lt or \
               "—Ä–∞—Å—Å–º–æ—Ç—Ä–µ–≤ –º–∞—Ç–µ—Ä–∏–∞–ª—ã —É–≥–æ–ª–æ–≤–Ω–æ–≥–æ –¥–µ–ª–∞" in lt or \
               "—Ä–∞—Å—Å–º–æ—Ç—Ä–µ–≤ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –¥–µ–ª–∞" in lt:
                intro_sentences.append(sent.strip())
                continue

            if "–º–∞—Ç–µ—Ä–∏–∞–ª—ã –¥–æ—Å—É–¥–µ–±–Ω–æ–≥–æ —Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è ‚Ññ" in lt or \
               "–º–∞—Ç–µ—Ä–∏–∞–ª—ã —É–≥–æ–ª–æ–≤–Ω–æ–≥–æ –¥–µ–ª–∞ ‚Ññ" in lt or \
               "–º–∞—Ç–µ—Ä–∏–∞–ª—ã –¥–µ–ª–∞ ‚Ññ" in lt:
                intro_sentences.append(sent.strip())
                continue

            if "—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å —Å–æ–≥" in lt or \
               "—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å —Å—É –¥–µ—Ä" in lt or \
               "—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å —Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ–π –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –≥—Ä—É–ø–ø—ã" in lt:
                intro_sentences.append(sent.strip())
                continue

    seen = set()
    uniq = []
    for s in intro_sentences:
        if s not in seen:
            seen.add(s)
            uniq.append(s)

    return "\n".join(uniq[:2])


# ============================================================
# üß† –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–∏
# ============================================================

def qualify_documents(
    case_id: Optional[str],
    docs: List[Dict[str, Any]],
    city: str = "–≥. –ü–∞–≤–ª–æ–¥–∞—Ä",
    date_str: Optional[str] = None,
    investigator_line: str = "–°–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å –ø–æ –æ—Å–æ–±–æ –≤–∞–∂–Ω—ã–º –¥–µ–ª–∞–º",
    investigator_fio: str = "",
) -> Dict[str, Any]:
    logger.info(f"‚ñ∂Ô∏è –ù–∞—á–∞–ª–æ –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–∏: case_id={case_id}, –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤={len(docs)}")

    if not docs:
        logger.warning("‚ö†Ô∏è –ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return _empty_result(case_id or "", "–î–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç")

    if not date_str:
        date_str = datetime.now().strftime("%d.%m.%Y")

    # 0Ô∏è‚É£ Reranker PRO (–æ—Ç–¥–µ–ª—å–Ω—ã–π —Å–µ—Ä–≤–∏—Å)
    try:
        reranker = LLMReranker()
        QUERY = (
            "—Ñ–∞–∫—Ç—ã –ø—Ä–µ—Å—Ç—É–ø–ª–µ–Ω–∏—è: –ø–µ—Ä–µ–≤–æ–¥—ã –¥–µ–Ω–µ–≥, –≤–ª–æ–∂–µ–Ω–∏—è, –≤—ã–≤–æ–¥ —Å—Ä–µ–¥—Å—Ç–≤, "
            "–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏, –æ–±–º–∞–Ω, –¥–µ–π—Å—Ç–≤–∏—è –ø–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º–æ–≥–æ, —Å–æ–±—ã—Ç–∏—è, —Å—É–º–º—ã, –¥–∞—Ç—ã"
        )

        docs = reranker.rerank(
            query=QUERY,
            items=docs,
            top_k=200,
        )
        logger.info(f"üìä Reranker PRO: –≤—ã–±—Ä–∞–Ω–æ TOP={len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ Reranker: {e}")

    # 1Ô∏è‚É£ SUPER PRE-FILTER + —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç
    for d in docs:
        raw = (d.get("text") or "").strip()
        cleaned_sentences = super_pre_filter(raw)
        d["clean_sentences"] = cleaned_sentences
        if cleaned_sentences:
            d["text"] = " ".join(cleaned_sentences)
        else:
            d["text"] = raw

    # 2Ô∏è‚É£ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∞–∫—Ç–æ–≤ / —Å—É—â–Ω–æ—Å—Ç–µ–π
    try:
        facts, persons, dates, amounts, sources = _extract_facts_and_sources(docs)
    except Exception as e:
        logger.error(f"_extract_facts_and_sources error: {e}")
        return _empty_result(case_id or "", f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ñ–∞–∫—Ç–æ–≤: {e}")

    facts = enrich_facts_with_roles(facts)

    # —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –º—É—Å–æ—Ä–∞
    raw_count = len(facts)
    facts = [
        f for f in facts
        if _legal_fact_filter(f.get("text", "")) and _hard_fact_clean(f.get("text", ""))
    ]
    facts = [f for f in facts if len((f.get("text") or "").split()) >= 3]
    logger.info(f"–§–ò–õ–¨–¢–† –§–ê–ö–¢–û–í: –±—ã–ª–æ={raw_count}, –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞={len(facts)}")

    # 3Ô∏è‚É£ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø—Ä–µ—Å—Ç—É–ø–ª–µ–Ω–∏—è
    crime_class = classify_crime(facts)
    primary_article = crime_class["primary"]
    secondary_articles = crime_class["secondary"]
    logger.info(f"–ê–í–¢–û-–ö–í–ê–õ–ò–§–ò–ö–ê–¶–ò–Ø: primary={primary_article}, secondary={secondary_articles}")

    # 4Ô∏è‚É£ EXTRACTOR 3.0
    extracted = extract_all(facts, persons, dates, amounts)
    roles = extracted.get("roles", {}) or {}
    events = extracted.get("events", []) or []
    timeline = extracted.get("timeline", []) or []
    legal_facts = extracted.get("legal_facts", {}) or {}
    crime_flow = extracted.get("crime_flow", []) or []
    crime_type = extracted.get("crime_type")

    suspects_list = extracted.get("suspects") or roles.get("suspect", []) or []
    suspect = extracted.get("primary_suspect") or (suspects_list[0] if suspects_list else None)
    suspect_role = roles.get("suspect_role")

    logger.info(
        f"[EXTRACTOR] suspect={suspect}, events={len(events)}, crime_type={crime_type}, "
        f"timeline={len(timeline)}, flow={len(crime_flow)}"
    )

    # —É—Å–∏–ª–µ–Ω–∏–µ legal_facts hint'–∞–º–∏
    legal_facts.update(
        {
            "crime_type": crime_type,
            "primary_article_hint": primary_article,
            "secondary_articles_hint": secondary_articles,
            "has_flow": bool(crime_flow),
        }
    )

    # 5Ô∏è‚É£ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∞–≤—Ç–æ-–∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏—è
    auto_article, auto_part, auto_reason = _auto_qualify(
        facts=facts,
        roles=roles,
        events=events,
        legal_facts=legal_facts,
    )
    logger.info(f"–ê–≤—Ç–æ-–∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏—è (auto_qualify): —Å—Ç–∞—Ç—å—è={auto_article}, —á–∞—Å—Ç—å={auto_part} ‚Äî {auto_reason}")

    # 6Ô∏è‚É£ –ü–æ–ª–Ω–æ—Ç–∞ –ø–æ —Å—Ç. 204 –£–ü–ö
    completeness = _check_204_completeness(
        facts=facts,
        persons=persons,
        dates=dates,
        amounts=amounts,
        roles=roles if suspect else {},
        events=events,
        legal_facts=legal_facts,
        timeline=timeline,
    )

    # 7Ô∏è‚É£ –£–ø–æ–º–∏–Ω–∞–Ω–∏—è —Å—Ç–∞—Ç–µ–π + –∫–æ–Ω—Ç–µ–∫—Å—Ç –ê–§–ú
    mentioned_articles = _extract_articles(docs)
    logger.info(f"–£–ø–æ–º–∏–Ω–∞–Ω–∏–π —Å—Ç–∞—Ç–µ–π: {len(mentioned_articles)}")

    law_contexts: List[str] = []
    for art in mentioned_articles:
        num = art.get("article")
        if num and num in ALL_AFM_LAWS:
            law_contexts.append(_resolve_law_context(num))
    law_context_text = "\n".join(law_contexts[:5]) if law_contexts else ""

    # 8Ô∏è‚É£ –ë–∞–∑–æ–≤—ã–π ¬´–£–°–¢–ê–ù–û–í–ò–õ¬ª (—Å —Å—Å—ã–ª–∫–∞–º–∏)
    ustanovil_base = _build_ustanovil_base(
        facts=facts,
        completeness=completeness,
        suspect=suspect,
        suspect_role=suspect_role,
    )

    # 9Ô∏è‚É£ –ü–æ–ø—ã—Ç–∫–∞ —É–ª—É—á—à–∏—Ç—å ¬´–£–°–¢–ê–ù–û–í–ò–õ¬ª —á–µ—Ä–µ–∑ LLM (—é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π —Å—Ç–∏–ª—å –ú–í–î)
    ustanovil_llm = ustanovil_base
    try:
        if facts:
            fact_lines: List[str] = []
            for idx, f in enumerate(facts, 1):
                fact_lines.append(f"{idx}. {f['text']} {_src_str(f.get('sources'))}")

            missing_text = ", ".join(completeness.get("missing", [])) or "–Ω–µ—Ç"

            suspect_block = ""
            if suspects_list:
                if len(suspects_list) == 1:
                    suspect_block = f"–ü–û–î–û–ó–†–ï–í–ê–ï–ú–´–ô: {suspects_list[0]}"
                else:
                    suspect_block = "–ü–û–î–û–ó–†–ï–í–ê–ï–ú–´–ï:\n" + "\n".join(f"- {s}" for s in suspects_list)
            else:
                suspect_block = "–ü–û–î–û–ó–†–ï–í–ê–ï–ú–´–ï: –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã"

            user_prompt = f"""
–ù–∏–∂–µ –ø–µ—Ä–µ–¥–∞–Ω —Å–ø–∏—Å–æ–∫ –û–†–ò–ì–ò–ù–ê–õ–¨–ù–´–• —Ñ–∞–∫—Ç–æ–≤, –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã—Ö –∏–∑ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤
—É–≥–æ–ª–æ–≤–Ω–æ–≥–æ –¥–µ–ª–∞ (—Ä–∞–ø–æ—Ä—Ç—ã, –ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è, –ø—Ä–æ—Ç–æ–∫–æ–ª—ã –¥–æ–ø—Ä–æ—Å–∞ –∏ —Ç.–ø.).

–ö–∞–∂–¥—ã–π —Ñ–∞–∫—Ç:
‚Ä¢ —è–≤–ª—è–µ—Ç—Å—è –≥–æ—Ç–æ–≤—ã–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞;
‚Ä¢ —Å–æ–¥–µ—Ä–∂–∏—Ç —Å—Å—ã–ª–∫—É –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫ [file_id:page];
‚Ä¢ –ù–ï –º–æ–∂–µ—Ç –±—ã—Ç—å –∏–∑–º–µ–Ω—ë–Ω, —Å–æ–∫—Ä–∞—â—ë–Ω –∏–ª–∏ –∏—Å–∫–∞–∂—ë–Ω –ø–æ —Å–º—ã—Å–ª—É.

{suspect_block}

–°–ü–ò–°–û–ö –§–ê–ö–¢–û–í:
{chr(10).join(fact_lines)}

–ù–µ–¥–æ—Å—Ç–∞—é—â–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ —Å—Ç. 204 –£–ü–ö –†–ö: {missing_text}

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
–¢–í–û–Ø –ó–ê–î–ê–ß–ê ‚Äî –°–§–û–†–ú–ò–†–û–í–ê–¢–¨ —Ä–∞–∑–¥–µ–ª ¬´–£–°–¢–ê–ù–û–í–ò–õ¬ª
–≤ —Å—Ç–∏–ª–µ –ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è –ú–í–î.

–¢–†–ï–ë–û–í–ê–ù–ò–Ø:
‚Ä¢ –°—Ç—Ä–æ–≥–∏–π –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å—Ç–∏–ª—å.
‚Ä¢ –†–∞–∑—Ä–µ—à–µ–Ω–æ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π,
  –ù–û –±–µ–∑ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –Ω–æ–≤—ã—Ö —Ñ–∞–∫—Ç–æ–≤, —Å—É–º–º, –¥–∞—Ç, —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –∏ –æ–±—Å—Ç–æ—è—Ç–µ–ª—å—Å—Ç–≤.
‚Ä¢ –ó–∞–ø—Ä–µ—â–µ–Ω–æ –ø—Ä–∏–¥—É–º—ã–≤–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏—è, —ç–ø–∏–∑–æ–¥—ã, –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ —Ñ–∞–∫—Ç–∞—Ö.
‚Ä¢ –ó–∞–ø—Ä–µ—â–µ–Ω–æ —Å—Å—ã–ª–∞—Ç—å—Å—è –Ω–∞ —Å—Ç–∞—Ç—å–∏ –£–ö/–£–ü–ö, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ —Ñ–∞–∫—Ç–∞—Ö –∏ law_context.
‚Ä¢ –°—Å—ã–ª–∫–∏ [file_id:page] –º–æ–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –≤ –∫–æ–Ω—Ü–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.

–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤—ã–≤–æ–¥–∞ (–°–¢–†–û–ì–û):

–£–°–¢–ê–ù–û–í–ò–õ:
<—Å–≤—è–∑–Ω—ã–π —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π —Ç–µ–∫—Å—Ç, –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –¢–û–õ–¨–ö–û –∏–∑ –ø—Ä–∏–≤–µ–¥—ë–Ω–Ω—ã—Ö —Ñ–∞–∫—Ç–æ–≤, –±–µ–∑ –Ω–æ–≤—ã—Ö —Å–≤–µ–¥–µ–Ω–∏–π>

–ù–∏–∫–∞–∫–∏—Ö –∏–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤, –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤, —Å–ª—É–∂–µ–±–Ω—ã—Ö –ø–æ–º–µ—Ç–æ–∫.
"""

            system_prompt = """
–¢—ã ‚Äî —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –º–æ–¥—É–ª—å ¬´AI_Qualifier¬ª –¥–ª—è –æ—Ä–≥–∞–Ω–æ–≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞.
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–æ—Ç–æ–≤—ã—Ö —Ñ–∞–∫—Ç–æ–≤ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–¥–µ–ª ¬´–£–°–¢–ê–ù–û–í–ò–õ¬ª –≤ —Å—Ç–∏–ª–µ –ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è.

–°–¢–†–û–ì–û –ó–ê–ü–†–ï–©–ï–ù–û:
‚Ä¢ –¥–æ–±–∞–≤–ª—è—Ç—å –Ω–æ–≤—ã–µ —Ñ–∞–∫—Ç—ã, —Å—É–º–º—ã, –¥–∞—Ç—ã, –§–ò–û, –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏, —ç–ø–∏–∑–æ–¥—ã;
‚Ä¢ –º–µ–Ω—è—Ç—å –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–≤—è–∑–∏, –µ—Å–ª–∏ –æ–Ω–∏ –ø—Ä—è–º–æ –Ω–µ —Å–ª–µ–¥—É—é—Ç –∏–∑ —Ñ–∞–∫—Ç–æ–≤;
‚Ä¢ –ø—Ä–∏–¥—É–º—ã–≤–∞—Ç—å –º–æ—Ç–∏–≤—ã, —É–º—ã—Å–µ–ª, –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏—é –¥–µ—è–Ω–∏—è;
‚Ä¢ —Å—Å—ã–ª–∞—Ç—å—Å—è –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ —Ñ–∞–∫—Ç–∞—Ö.

–†–ê–ó–†–ï–®–ï–ù–û:
‚Ä¢ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, —Å–æ—Ö—Ä–∞–Ω—è—é —Å–º—ã—Å–ª;
‚Ä¢ –æ–±—ä–µ–¥–∏–Ω—è—Ç—å –±–ª–∏–∑–∫–∏–µ –ø–æ —Å–º—ã—Å–ª—É —Ñ–∞–∫—Ç—ã –≤ –∞–±–∑–∞—Ü—ã;
‚Ä¢ –º–µ–Ω—è—Ç—å –ø–æ—Ä—è–¥–æ–∫, –µ—Å–ª–∏ —ç—Ç–æ –¥–µ–ª–∞–µ—Ç –∏–∑–ª–æ–∂–µ–Ω–∏–µ –ª–æ–≥–∏—á–Ω—ã–º;
‚Ä¢ –æ—Å—Ç–∞–≤–ª—è—Ç—å —Å—Å—ã–ª–∫–∏ [file_id:page] –≤ –∫–æ–Ω—Ü–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.

–ï—Å–ª–∏ —Ñ–∞–∫—Ç–æ–≤ —è–≤–Ω–æ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ ‚Äî —Å—Ñ–æ—Ä–º–∏—Ä—É–π –∫–æ—Ä–æ—Ç–∫–∏–π —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
–æ —Ç–æ–º, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã—Ö –≤—ã–≤–æ–¥–æ–≤.
"""

            ustanovil_llm = _ask_llm(user_prompt, system_prompt)
            logger.info("‚úÖ –†–∞–∑–¥–µ–ª ¬´–£–°–¢–ê–ù–û–í–ò–õ¬ª —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω LLM.")
    except LLMUnavailableError as e:
        logger.warning(f"LLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è —Ä–∞–∑–¥–µ–ª–∞ ¬´–£–°–¢–ê–ù–û–í–ò–õ¬ª: {e}")
        ustanovil_llm = ustanovil_base

    # 10Ô∏è‚É£ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è ¬´–ü–û–°–¢–ê–ù–û–í–ò–õ¬ª
    safe_article = primary_article or auto_article or "[–¢—Ä–µ–±—É–µ—Ç —É—Ç–æ—á–Ω–µ–Ω–∏—è]"

    safe_primary = safe_article or "–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞"
    law_data = ALL_AFM_LAWS.get(safe_primary, {})

    post_prompt = prompts.P_POST.format(
        ustanovil_text = ustanovil_llm,
        primary_article = safe_primary,
        secondary_articles = ", ".join(secondary_articles) if secondary_articles else "–Ω–µ—Ç",
        law_text = law_data.get("text", "–¢–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω"),
        law_commentary = law_data.get("commentary", ""),
    )


    system_for_post = (
        "–¢—ã ‚Äî —É–∑–∫–æ—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–æ–¥—É–ª—å ¬´AI_Qualifier_Post¬ª. "
        "–¢–≤–æ—è –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è –∑–∞–¥–∞—á–∞ ‚Äî —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å —é—Ä–∏–¥–∏—á–µ—Å–∫–∏ –≥—Ä–∞–º–æ—Ç–Ω—ã–π —Ä–∞–∑–¥–µ–ª ¬´–ü–û–°–¢–ê–ù–û–í–ò–õ¬ª "
        "–∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞ —Ä–∞–∑–¥–µ–ª–∞ ¬´–£–°–¢–ê–ù–û–í–ò–õ¬ª.\n\n"
        "–°–¢–†–û–ì–û –ó–ê–ü–†–ï–©–ï–ù–û:\n"
        "- –¥–æ–±–∞–≤–ª—è—Ç—å –Ω–æ–≤—ã–µ —Ñ–∞–∫—Ç—ã, —Å–æ–±—ã—Ç–∏—è, —Å—É–º–º—ã, –¥–∞—Ç—ã, —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤;\n"
        "- –ø—Ä–∏–¥—É–º—ã–≤–∞—Ç—å –æ–±—Å—Ç–æ—è—Ç–µ–ª—å—Å—Ç–≤–∞, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ —Ä–∞–∑–¥–µ–ª–µ ¬´–£–°–¢–ê–ù–û–í–ò–õ¬ª;\n"
        "- —Å—Å—ã–ª–∞—Ç—å—Å—è –Ω–∞ –æ—Ä–≥–∞–Ω—ã –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ —Ç–µ–∫—Å—Ç–µ;\n"
        "- –¥–∞–≤–∞—Ç—å —Å–æ–≤–µ—Ç—ã —Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—é –∏–ª–∏ –æ—Ü–µ–Ω–æ—á–Ω—ã–µ —Å—É–∂–¥–µ–Ω–∏—è.\n\n"
        "–†–∞–∑—Ä–µ—à–µ–Ω–æ —Ç–æ–ª—å–∫–æ –∫—Ä–∞—Ç–∫–æ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å —Ä–µ—à–µ–Ω–∏–µ: "
        "–ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏—è, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –∏ —Ç.–ø.\n"
        "–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏—Ç–æ–≥–æ–≤–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: –£–°–¢–ê–ù–û–í–ò–õ ‚Üí –ü–û–°–¢–ê–ù–û–í–ò–õ ‚Üí –ø–æ–¥–ø–∏—Å—å (–±–µ–∑ —à–∞–ø–æ–∫, –≥–µ—Ä–±–æ–≤ –∏ –ª–∏—à–Ω–∏—Ö –±–ª–æ–∫–æ–≤)."
    )

    intro_context = _extract_intro_context(docs)

    try:
        full_user_prompt = f"""
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã—è–≤–ª–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –£–ö –†–ö (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä): {primary_article or "–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞"}.
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã—è–≤–ª–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –£–ö –†–ö (–∞–Ω–∞–ª–∏–∑ —Ñ–∞–∫—Ç–æ–≤): {auto_article or "–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞"} —á.{auto_part or "-"}.
–ü—Ä–∏—á–∏–Ω–∞: {auto_reason}.

–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π —Ç–∏–ø –ø—Ä–µ—Å—Ç—É–ø–ª–µ–Ω–∏—è (–±–µ–∑ LLM): {crime_type or "–Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ"}.

–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –¥–µ–ª–∞ ‚Ññ {case_id}.
–ú–µ—Å—Ç–æ –≤—ã–Ω–µ—Å–µ–Ω–∏—è: {city}.
–î–∞—Ç–∞: {_rus_date(date_str)}.

–ü—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (–µ–≥–æ –Ω—É–∂–Ω–æ –î–û–°–õ–û–í–ù–û –≤—Å—Ç–∞–≤–∏—Ç—å –ø–µ—Ä–µ–¥ —Ä–∞–∑–¥–µ–ª–æ–º ¬´–£–°–¢–ê–ù–û–í–ò–õ¬ª, –µ—Å–ª–∏ –æ–Ω –Ω–µ –ø—É—Å—Ç–æ–π):
{intro_context or "[–Ω–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞]"}

–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–π —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (–µ—Å–ª–∏ –µ—Å—Ç—å):
{law_context_text or "[–Ω–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫ –∑–∞–∫–æ–Ω–æ–≤]"}

–û—Å–Ω–æ–≤—ã–≤–∞–π—Å—è –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ —Ä–∞–∑–¥–µ–ª–µ ¬´–£–°–¢–ê–ù–û–í–ò–õ¬ª –Ω–∏–∂–µ –∏ –Ω–∞ –±–ª–æ–∫–µ –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤—ã—à–µ.

{post_prompt}

–°—Ç–∞—Ç—å—è –¥–ª—è –æ—Å–Ω–æ–≤–Ω–æ–π –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–µ—Å–ª–∏ —ç—Ç–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç—Å—è —Ñ–∞–∫—Ç–∞–º–∏): {safe_article}.
–ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ ‚Äî –≤ —Ä–∞–∑–¥–µ–ª–µ ¬´–ü–û–°–¢–ê–ù–û–í–ò–õ¬ª –æ—Ç—Ä–∞–∑–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –ø–æ–ª—É—á–µ–Ω–∏—è
–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ –ø–µ—Ä–µ–¥ –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–æ–π –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–µ–π.
"""
        final_postanovlenie_raw = _ask_llm(
            prompt=full_user_prompt,
            system_prompt=system_for_post,
        )
        logger.info("‚úÖ –ü–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ —á–µ—Ä–µ–∑ LLM.")
    except LLMUnavailableError as e:
        logger.warning(f"LLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –ü–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è: {e}")
        final_postanovlenie_raw = _build_postanovlenie_simple(
            city=city,
            date_str=date_str,
            investigator_line=investigator_line,
            case_id=case_id,
            ustanovil_text=ustanovil_llm,
            mentioned_articles=mentioned_articles,
            completeness=completeness,
            investigator_fio=investigator_fio,
            intro_context=intro_context,
        )

    # 11Ô∏è‚É£ –°—Ç—Ä–∞—Ö–æ–≤–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
    lower_body = final_postanovlenie_raw.lower()
    if "—É—Å—Ç–∞–Ω–æ–≤–∏–ª" not in lower_body or "–ø–æ—Å—Ç–∞–Ω–æ–≤–∏–ª" not in lower_body:
        logger.warning("‚ö†Ô∏è LLM –æ—Ç–∫–ª–æ–Ω–∏–ª—Å—è –æ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—ã, –ø—Ä–∏–º–µ–Ω—è—é fallback-—à–∞–±–ª–æ–Ω –ü–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è.")
        final_postanovlenie_raw = _build_postanovlenie_simple(
            city=city,
            date_str=date_str,
            investigator_line=investigator_line,
            case_id=case_id,
            ustanovil_text=ustanovil_llm,
            mentioned_articles=mentioned_articles,
            completeness=completeness,
            investigator_fio=investigator_fio,
            intro_context=intro_context,
        )

    # 12Ô∏è‚É£ –û—Ü–µ–Ω–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
    overall_conf = _overall_confidence(facts, completeness)

    # 13Ô∏è‚É£ –ë–∞–∑–æ–≤—ã–π result (–° –°–°–´–õ–ö–ê–ú–ò ‚Äî –¥–ª—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞)
    result: Dict[str, Any] = {
        "generation_id": str(uuid.uuid4()),
        "model_version": MODEL_VERSION,
        "case_id": case_id,
        "facts": facts,
        "persons": persons,
        "dates": dates,
        "amounts": amounts,
        "mentioned_articles": mentioned_articles,
        "roles": roles,
        "events": events,
        "timeline": timeline,
        "legal_facts": legal_facts,
        "completeness_204": completeness,
        "established_text": ustanovil_llm.strip(),          # ‚Üê —Å [file_id:page]
        "final_postanovlenie": final_postanovlenie_raw.strip(),  # ‚Üê —Ç–æ–∂–µ –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Å—ã–ª–∫–∏
        "sources": sources,
        "confidence": round(overall_conf, 3),
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "investigator_fio": investigator_fio,
        "investigator_line": investigator_line,
        "auto_article": auto_article,
        "auto_part": auto_part,
        "auto_reason": auto_reason,
        "auto_classification": crime_class,
        "suspect": suspect,
        "suspect_role": suspect_role,
        "crime_flow": crime_flow,
        "crime_type": crime_type,
        "warnings": [],
    }

    # 14Ô∏è‚É£ Anti-hallucination –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è (—Ä–∞–±–æ—Ç–∞–µ—Ç –ø–æ —Ç–µ–∫—Å—Ç—É –° –°–°–´–õ–ö–ê–ú–ò)
    try:
        verification = run_full_verification(result)
        if not isinstance(verification, dict):
            raise ValueError("–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏")

        result["verification"] = verification
        result["verdict"] = (
            "OK"
            if verification.get("overall_ok")
            else verification.get("texts", {}).get("verdict", "UNVERIFIED")
        )

        if not verification.get("overall_ok"):
            result["warnings"].append("‚ö†Ô∏è –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –≤—ã—è–≤–∏–ª–∞ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç–∏")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}")
        result["verification"] = {"error": str(e)}
        result["verdict"] = "VERIFICATION_FAILED"
        result["warnings"].append(f"–û—à–∏–±–∫–∞ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏: {str(e)}")

    # 15Ô∏è‚É£ –£–¥–∞–ª—è–µ–º —Å—Å—ã–ª–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤ –ø–µ—Ä–µ–¥ –æ—Ç–¥–∞—á–µ–π –Ω–∞—Ä—É–∂—É (–í–∞—Ä–∏–∞–Ω—Ç B)
    result["established_text"] = _remove_sources(result.get("established_text", ""))
    result["final_postanovlenie"] = _remove_sources(result.get("final_postanovlenie", ""))

    logger.info(
        f"‚úÖ –ö–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. verdict={result.get('verdict')}, "
        f"conf={result.get('confidence'):.2f}, suspect={suspect}"
    )
    return result


# ============================================================
# üîπ Fallback-—Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
# ============================================================

def _empty_result(case_id: str, msg: str,
                  investigator_fio: str = "",
                  investigator_line: str = "") -> Dict[str, Any]:
    return {
        "generation_id": None,
        "model_version": MODEL_VERSION,
        "case_id": case_id,
        "established_text": "",
        "final_postanov–ª–µ–Ωie": f"[–û–®–ò–ë–ö–ê]: {msg}",
        "facts": [],
        "persons": [],
        "dates": [],
        "amounts": [],
        "mentioned_articles": [],
        "roles": {},
        "events": [],
        "timeline": [],
        "legal_facts": {},
        "completeness_204": {},
        "sources": [],
        "confidence": 0.0,
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "investigator_fio": investigator_fio,
        "investigator_line": investigator_line,
        "warnings": [msg],
        "verification": {"overall_ok": False},
        "verdict": "ERROR",
        "suspect": None,
        "suspect_role": None,
        "crime_flow": [],
        "crime_type": None,
    }


# ============================================================
# üß∞ –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —É—Ç–∏–ª–∏—Ç—ã
# ============================================================

def _remove_sources(text: str) -> str:
    """–£–¥–∞–ª—è–µ—Ç —Å—Å—ã–ª–∫–∏ –≤–∏–¥–∞ [uuid:page] –∏–∑ —Ç–µ–∫—Å—Ç–∞."""
    return re.sub(r"\[[0-9a-fA-F\-]{36}:\d+\]", "", text)


def _context_snippet(text: str, start: int, end: int, radius: int = CONTEXT_RADIUS) -> str:
    a, b = max(0, start - radius), min(len(text), end + radius)
    return text[a:b].replace("\n", " ").strip()


def _src_str(sources: Optional[List[Dict[str, Any]]]) -> str:
    if not sources:
        return "[–∏—Å—Ç–æ—á–Ω–∏–∫: –Ω–µ —É–∫–∞–∑–∞–Ω]"
    show = [
        f"[{s.get('file_id', '?')}:{s.get('page', '-')}]"
        for s in sources[:3]
    ]
    if len(sources) > 3:
        show.append(f"(–∏ –µ—â—ë {len(sources) - 3})")
    return " ".join(show)


def _conf_from_signal(sentence: str) -> float:
    score = MIN_FACT_CONFIDENCE
    if DATE_RX.search(sentence):
        score += 0.15
    if MONEY_RX.search(sentence):
        score += 0.15
    if PERSON_RX.search(sentence):
        score += 0.1
    return min(score, 0.95)


def _dedup_sources(sources: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    seen, out = set(), []
    for s in sources:
        key = (s.get("file_id"), s.get("page"))
        if key not in seen:
            seen.add(key)
            out.append(s)
    return out


def _dedup_articles(arts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    seen, out = set(), []
    for a in arts:
        key = (
            a["code"],
            a["article"],
            a["source"].get("file_id"),
            a["source"].get("page"),
        )
        if key not in seen:
            seen.add(key)
            out.append(a)
    return out


def _overall_confidence(facts: List[dict], completeness: Dict[str, Any]) -> float:
    if not facts:
        return 0.4
    avg = sum(f.get("confidence", 0.5) for f in facts) / max(1, len(facts))
    miss_penalty = 0.05 * len(completeness.get("missing", []))
    return max(0.1, min(0.98, avg - miss_penalty))


def _rus_date(d: str) -> str:
    """
    –ü–æ–Ω–∏–º–∞–µ—Ç 'DD.MM.YYYY' –∏ ISO 'YYYY-MM-DD'.
    –ï—Å–ª–∏ —Ñ–æ—Ä–º–∞—Ç –Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Å—Ö–æ–¥–Ω—É—é —Å—Ç—Ä–æ–∫—É.
    """
    try:
        if re.fullmatch(r"\d{4}-\d{2}-\d{2}", d):
            dt = datetime.fromisoformat(d)
        else:
            dt = datetime.strptime(d, "%d.%m.%Y")
    except Exception:
        return d

    months = [
        "—è–Ω–≤–∞—Ä—è", "—Ñ–µ–≤—Ä–∞–ª—è", "–º–∞—Ä—Ç–∞", "–∞–ø—Ä–µ–ª—è", "–º–∞—è", "–∏—é–Ω—è",
        "–∏—é–ª—è", "–∞–≤–≥—É—Å—Ç–∞", "—Å–µ–Ω—Ç—è–±—Ä—è", "–æ–∫—Ç—è–±—Ä—è", "–Ω–æ—è–±—Ä—è", "–¥–µ–∫–∞–±—Ä—è",
    ]
    return f"{dt.day} {months[dt.month - 1]} {dt.year} –≥–æ–¥–∞"
