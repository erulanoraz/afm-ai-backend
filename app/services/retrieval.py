# app/services/retrieval.py
import logging
import re
from typing import List, Dict, Any, Optional
from sqlalchemy.orm import Session
from uuid import UUID

from app.db.models import File, Chunk

logger = logging.getLogger(__name__)


# ============================================================
# üßº –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ (Kazakhstan-ready, –±–µ–∑–æ–ø–∞—Å–Ω–∞—è)
# ============================================================

def normalize_text(text: str) -> str:
    if not text:
        return ""

    # normalize newlines, spaces
    text = text.replace("\r", "")
    text = re.sub(r"[ \t]+", " ", text)
    text = re.sub(r"\n{2,}", "\n", text)

    # ‚ö†Ô∏è –£–¥–∞–ª—è–µ–º –¢–û–õ–¨–ö–û —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —à—É–º, –ù–ï —Ñ–∞–±—É–ª—É
    garbage = [
        r"¬©\s?–í—Å–µ –ø—Ä–∞–≤–∞ –∑–∞—â–∏—â–µ–Ω—ã",
        r"—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–æ\s?—Å\s?–ø–æ–º–æ—â—å—é.*",
        r"—Å—Ç—Ä–∞–Ω–∏—Ü–∞\s?\d+\s?–∏–∑\s?\d+",
        r"–î–æ–∫—É–º–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω.*",
        r"QR[- ]?–∫–æ–¥.*",
        r"—ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç.*",
        r"–ü—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–æ –Ω–∞.*",
        r"–î–∞—Ç–∞ –ø–µ—á–∞—Ç–∏.*",
        # –ø–æ–¥–ø–∏—Å—å —Ä–∞–∑—Ä–µ—à–µ–Ω–æ –æ—Å—Ç–∞–≤–ª—è—Ç—å ‚Äî –≤–∞–∂–Ω–æ
    ]

    for g in garbage:
        text = re.sub(g, "", text, flags=re.IGNORECASE)

    return text.strip()



# ============================================================
# üß† –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è RU/KZ ‚Äî –±–µ–∑–æ–ø–∞—Å–Ω–∞—è
# ============================================================

def lemmatize(text: str) -> str:
    # –ø–æ–∫–∞ –ø—Ä–æ—Å—Ç–æ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
    return normalize_text(text)



# ============================================================
# üî• –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è Retrieval 3.1
# ============================================================

def get_file_docs_for_qualifier(
    db: Session,
    file_ids: Optional[List[str]] = None,
    case_id: Optional[str] = None,
) -> List[Dict[str, Any]]:

    query = db.query(File)

    if case_id:
        query = query.filter(File.case_id == case_id)

    if file_ids:
        query = query.filter(File.file_id.in_(file_ids))

    files = query.all()
    logger.info(f"üìÑ Retrieval: –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ = {len(files)}")

    docs: List[Dict[str, Any]] = []

    for f in files:
        file_id = str(f.file_id)

        try:
            chunks = (
                db.query(Chunk)
                .filter(Chunk.file_id == UUID(file_id))
                .order_by(
                    Chunk.page.asc(),
                    Chunk.start_offset.asc(),
                    Chunk.created_at.asc(),
                    Chunk.chunk_id.asc(),
                )
                .all()
            )
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —á–∞–Ω–∫–æ–≤ —Ñ–∞–π–ª–∞ {file_id}: {e}")
            continue

        if not chunks:
            logger.warning(f"‚ö†Ô∏è –§–∞–π–ª {file_id} –ø—É—Å—Ç ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é.")
            continue

        for ch in chunks:
            raw_text = getattr(ch, "text", "") or ""
            clean_text = lemmatize(raw_text)

            if not clean_text.strip():
                continue

            docs.append({
                "file_id": file_id,
                "page": ch.page or 1,
                "chunk_id": str(ch.chunk_id),
                "text": clean_text,
            })

    # -----------------------------
    # üçÄ –õ–æ–≥ –ø–æ—Å–ª–µ –Ω–∞–ø–æ–ª–Ω–µ–Ω–∏—è docs
    # -----------------------------
    logger.info("=== RETRIEVAL OUTPUT START ===")
    for d in docs[:20]:
        txt = d.get("text", "").replace("\n", " ")
        logger.info(f"PAGE={d.get('page')} | LEN={len(txt)} | {txt[:300]}")
    logger.info("=== RETRIEVAL OUTPUT END ===")

    logger.info(f"üì¶ Retrieval 3.1 –≤–µ—Ä–Ω—É–ª –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(docs)}")
    return docs



# ============================================================
# üîπ –ß–∞–Ω–∫–∏ –ø–æ file_id
# ============================================================

def get_chunks_by_file_id(db: Session, file_id: str) -> List[Dict[str, Any]]:
    try:
        chunks = (
            db.query(Chunk)
            .filter(Chunk.file_id == UUID(file_id))
            .order_by(
                Chunk.page.asc(),
                Chunk.start_offset.asc(),
                Chunk.created_at.asc(),
            )
            .all()
        )
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ get_chunks_by_file_id({file_id}): {e}")
        return []

    result = []

    for ch in chunks:
        clean_text = lemmatize(getattr(ch, "text", "") or "")
        result.append({
            "chunk_id": str(ch.chunk_id),
            "file_id": file_id,
            "page": ch.page or 1,
            "text": clean_text,
            "metadata": {
                "start_offset": getattr(ch, "start_offset", None),
                "created_at": getattr(ch, "created_at", None),
            },
        })

    if not result:
        logger.warning(f"‚ö†Ô∏è –§–∞–π–ª {file_id} –≤–µ—Ä–Ω—É–ª 0 —á–∞–Ω–∫–æ–≤. –°–æ–∑–¥–∞—é placeholder")
        return [{
            "chunk_id": f"{file_id}-empty",
            "file_id": file_id,
            "page": 1,
            "text": "",
            "metadata": {}
        }]

    return result



# ============================================================
# üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ (—É–ª—É—á—à–µ–Ω–Ω–∞—è)
# ============================================================

def get_file_text_stats(db: Session, case_id: str) -> Dict[str, Any]:
    try:
        files = db.query(File).filter(File.case_id == case_id).all()

        stats = {
            "case_id": case_id,
            "total_files": len(files),
            "files_with_chunks": 0,
            "total_chunks": 0,
            "total_chars": 0,
            "files": [],
        }

        for f in files:
            file_id = str(f.file_id)

            chunks = (
                db.query(Chunk)
                (Chunk.file_id == UUID(file_id))
                .all()
            )

            total_text = sum(len(c.text or "") for c in chunks)

            stats["total_chunks"] += len(chunks)
            stats["total_chars"] += total_text
            stats["files_with_chunks"] += 1 if chunks else 0

            stats["files"].append({
                "file_id": file_id,
                "filename": f.filename,
                "chunks": len(chunks),
                "text_length": total_text,
            })

        logger.info(
            f"üìä Retrieval Stats: —Ñ–∞–π–ª–æ–≤={stats['total_files']}, "
            f"—á–∞–Ω–∫–æ–≤={stats['total_chunks']}, "
            f"—Å–∏–º–≤–æ–ª–æ–≤={stats['total_chars']}"
        )

        return stats

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ get_file_text_stats: {e}")
        return {"error": str(e)}
