# app/services/retrieval.py
import logging
import re
from typing import List, Dict, Any, Optional
from uuid import UUID
from sqlalchemy.orm import Session

from app.db.models import File, Chunk

logger = logging.getLogger(__name__)

# ============================================================
# üî• –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø Retrieval 6.0 ‚Äî –§–û–ö–£–° –ù–ê –ö–ê–ß–ï–°–¢–í–û
# ============================================================

# –ù–µ –±–µ—Ä—ë–º –í–°–ï ‚Äî –±–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –≤—ã—Å–æ–∫–æ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ
TOP_K_WIDE = 200        # ‚Üê –±—ã–ª–æ 600! –°–õ–ò–®–ö–û–ú –ú–ù–û–ì–û
TOP_BASELINE_LIMIT = 150
TOP_RERANK_INPUT = 80   # ‚Üê –±—ã–ª–æ 300, –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –º–∞–ª–æ –Ω–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ


# ============================================================
# üî• –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø (—É–±–∏—Ä–∞–µ–º –º—É—Å–æ—Ä)
# ============================================================

def normalize_text(text: str) -> str:
    """–£–±–∏—Ä–∞–µ–º OCR –º—É—Å–æ—Ä –∏ —Ç–µ—Ö–Ω–∏—á–∫—É"""
    if not text:
        return ""

    text = text.replace("\r", "")
    text = re.sub(r"[ \t]+", " ", text)
    text = re.sub(r"\n{2,}", "\n", text)

    # –£–¥–∞–ª—è–µ–º —Ç–µ—Ö–º—É—Å–æ—Ä
    garbage = [
        r"¬©\s?–í—Å–µ –ø—Ä–∞–≤–∞ –∑–∞—â–∏—â–µ–Ω—ã",
        r"—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–æ\s?—Å\s?–ø–æ–º–æ—â—å—é.*",
        r"—Å—Ç—Ä–∞–Ω–∏—Ü–∞\s?\d+\s?–∏–∑\s?\d+",
        r"–î–æ–∫—É–º–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω.*",
        r"QR[- ]?–∫–æ–¥.*",
        r"—ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç.*",
        r"–ü—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–æ.*",
        r"–î–∞—Ç–∞ –ø–µ—á–∞—Ç–∏.*",
        r"—Ö–µ—à.*",
        r"—ç—Ü–ø.*",
    ]

    for g in garbage:
        text = re.sub(g, "", text, flags=re.IGNORECASE)

    return text.strip()


# ============================================================
# üî• CRISP baseline weight (–ì–õ–ê–í–ù–û–ï –£–õ–£–ß–®–ï–ù–ò–ï)
# ============================================================

def baseline_weight(filename: str, text: str) -> float:
    """
    –°—Ç—Ä–æ–≥–∏–π baseline ‚Äî –ù–ï –±–µ—Ä—ë–º –º—É—Å–æ—Ä.
    –¢–æ–ª—å–∫–æ —Ñ–∞–π–ª—ã —Å –†–ï–ê–õ–¨–ù–´–ú —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ–º.
    """
    fn = filename.lower()
    t = text.lower()

    # VERY STRONG ‚Äî –¥–æ–ø—Ä–æ—Å—ã –ø–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º–æ–≥–æ (THE GOLD)
    if any(x in fn for x in ["–ø—Ä–æ—Ç–æ–∫–æ–ª_–¥–æ–ø—Ä–æ—Å–∞_–ø–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º", "–¥–æ–ø—Ä–æ—Å–∞ –ø–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º", "–∫—É–∏"]):
        return 0.99

    # STRONG ‚Äî —Ä–∞–ø–æ—Ä—Ç—ã, –µ—Ä–¥—Ä
    if any(x in fn for x in ["—Ä–∞–ø–æ—Ä—Ç", "–µ—Ä–¥—Ä", "–ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –æ –≤–æ–∑–±—É–∂–¥–µ–Ω–∏–∏"]):
        return 0.90

    # MEDIUM ‚Äî –¥–æ–ø—Ä–æ—Å—ã –ø–æ—Ç–µ—Ä–ø–µ–≤—à–∏—Ö, —Å–≤–∏–¥–µ—Ç–µ–ª–µ–π
    if any(x in fn for x in ["–¥–æ–ø—Ä–æ—Å–∞_–ø–æ—Ç–µ—Ä–ø–µ–≤—à", "–¥–æ–ø—Ä–æ—Å–∞ –ø–æ—Ç–µ—Ä–ø–µ–≤—à", "—Å–≤–∏–¥–µ—Ç–µ–ª–µ–π"]):
        return 0.75

    # WEAK ‚Äî –ø—Ä–æ—Å—Ç–æ –ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
    if "–ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ" in fn:
        return 0.60

    # GARBAGE ‚Äî —Ñ–∞–π–ª—ã —Å–æ–¥–µ—Ä–∂–∞—Ç —Ç–æ–ª—å–∫–æ —Ç–µ—Ö–º—É—Å–æ—Ä
    if len(t) < 50 or t.count(" ") < 5:
        return 0.0

    # DEFAULT ‚Äî –æ—Å—Ç–∞–ª—å–Ω–æ–µ —Å –º–∞–ª—ã–º –≤–µ—Å–æ–º
    return 0.35


# ============================================================
# üî• –ì–õ–ê–í–ù–´–ô RETRIEVAL (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π)
# ============================================================

def get_file_docs_for_qualifier(
    db: Session,
    file_ids: Optional[List[str]] = None,
    case_id: Optional[str] = None,
) -> List[Dict[str, Any]]:
    """
    Retrieval 6.0 ‚Äî –∂—ë—Å—Ç–∫–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è, –ù–û –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞.
    """

    query = db.query(File)

    if case_id:
        query = query.filter(File.case_id == case_id)

    if file_ids:
        query = query.filter(File.file_id.in_(file_ids))

    files = query.all()
    logger.info(f"üìÑ Retrieval 6.0: –≤—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤ = {len(files)}")

    docs: List[Dict[str, Any]] = []

    # ============================================================
    # –ß–∏—Ç–∞–µ–º –¢–û–õ–¨–ö–û —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏
    # ============================================================
    for f in files:
        file_id = str(f.file_id)
        filename = (f.filename or "").lower()

        # üî¥ –§–ò–õ–¨–¢–† 1: –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª—ã –∫–æ—Ç–æ—Ä—ã–µ —è–≤–Ω–æ –º—É—Å–æ—Ä
        weight = baseline_weight(filename, "")
        if weight < 0.30:
            logger.debug(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫: {filename} (weight={weight})")
            continue

        try:
            chunks = (
                db.query(Chunk)
                .filter(Chunk.file_id == UUID(file_id))
                .order_by(
                    Chunk.page.asc(),
                    Chunk.start_offset.asc(),
                )
                .all()
            )
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —á–∞–Ω–∫–æ–≤ {file_id}: {e}")
            continue

        if not chunks:
            continue

        # üî¥ –§–ò–õ–¨–¢–† 2: –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ –∏ –º—É—Å–æ—Ä–Ω—ã–µ —á–∞–Ω–∫–∏
        for ch in chunks:
            raw = (ch.text or "").strip()
            
            # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ?
            if len(raw) < 30:
                continue
            
            # –¢–æ–ª—å–∫–æ —Å–ª—É–∂–µ–±–Ω–∞—è –∏–Ω—Ñ–æ?
            if raw.count(" ") < 3:
                continue

            clean = normalize_text(raw)
            
            if not clean or len(clean) < 20:
                continue

            # ‚úÖ –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–∏–µ —á–∞–Ω–∫–∏
            docs.append({
                "file_id": file_id,
                "filename": f.filename,
                "page": ch.page or 1,
                "chunk_id": str(ch.chunk_id),
                "text": clean,
            })

    logger.info(f"üì¶ –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞: {len(docs)} —á–∞–Ω–∫–æ–≤")

    if not docs:
        return []

    # ============================================================
    # Baseline —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
    # ============================================================
    for d in docs:
        d["baseline_weight"] = baseline_weight(
            filename=d["filename"],
            text=d["text"],
        )

    # üî¥ –ñ–Å–°–¢–ö–ê–Ø –°–û–†–¢–ò–†–û–í–ö–ê
    docs = sorted(docs, key=lambda x: x["baseline_weight"], reverse=True)
    docs = docs[:TOP_BASELINE_LIMIT]

    logger.info(f"‚úÖ Retrieval 6.0: –ø–µ—Ä–µ–¥–∞—ë–º {min(len(docs), TOP_RERANK_INPUT)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ reranker")

    return docs[:TOP_RERANK_INPUT]


# ============================================================
# üîç DEBUG SEARCH (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
# ============================================================

def search_chunks(db: Session, query: str, limit: int = 20) -> List[Dict[str, Any]]:
    """–£–ø—Ä–æ—â—ë–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è debug API."""

    if not query or not query.strip():
        return []

    pattern = f"%{query.strip()}%"

    try:
        chunks = (
            db.query(Chunk)
            .filter(Chunk.text.ilike(pattern))
            .order_by(Chunk.page.asc())
            .limit(limit)
            .all()
        )
    except Exception as e:
        logger.error(f"[search_chunks ERROR] {e}")
        return []

    results = []
    for ch in chunks:
        results.append({
            "file_id": str(ch.file_id),
            "chunk_id": str(ch.chunk_id),
            "page": ch.page,
            "text": ch.text[:300] + ("..." if len(ch.text) > 300 else "")
        })

    return results