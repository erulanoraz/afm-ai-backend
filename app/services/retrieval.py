import logging
import re
from typing import List, Dict, Any, Optional
from uuid import UUID
from sqlalchemy.orm import Session

from app.db.models import File, Chunk

logger = logging.getLogger(__name__)


# ============================================================
# üî• RETRIEVAL 7.6 ‚Äî GLOBAL COVERAGE (NO CASE_ID FILTER)
# ============================================================
#
# ‚Ä¢ –ü–æ–ª–Ω–æ–µ –æ—Ç–∫–ª—é—á–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ case_id
# ‚Ä¢ –§–∞–π–ª—ã –±–µ–∑ case_id –ù–ï –ø—Ä–æ–ø—É—Å–∫–∞—é—Ç—Å—è
# ‚Ä¢ –ü—Ä–∏–≤—è–∑–∫–∞ —Ç–æ–ª—å–∫–æ –ø–æ file_ids (–µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω—ã)
# ‚Ä¢ –ï—Å–ª–∏ file_ids –ø—É—Å—Ç–æ ‚Üí –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –í–°–ï —Ñ–∞–π–ª—ã –≤ –ë–î
# ============================================================

TOP_BASELINE_LIMIT = 400
TOP_RERANK_INPUT = 300
MIN_TEXT_LENGTH = 20


# ============================================================
# üîß –¢–µ–∫—Å—Ç–æ–≤–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
# ============================================================

def normalize_text(text: str) -> str:
    if not text:
        return ""

    text = text.replace("\r", "")
    text = re.sub(r"[ \t]+", " ", text)
    text = re.sub(r"\n{2,}", "\n", text)

    garbage = [
        r"¬©\s?–í—Å–µ –ø—Ä–∞–≤–∞ –∑–∞—â–∏—â–µ–Ω—ã",
        r"—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–æ\s?—Å\s?–ø–æ–º–æ—â—å—é.*",
        r"—Å—Ç—Ä–∞–Ω–∏—Ü–∞\s?\d+\s?–∏–∑\s?\d+",
        r"–î–æ–∫—É–º–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω.*",
        r"QR[- ]?–∫–æ–¥.*",
        r"—ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç.*",
        r"–ü—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–æ.*",
        r"–î–∞—Ç–∞ –ø–µ—á–∞—Ç–∏.*",
        r"—Ö–µ—à.*",
        r"—ç—Ü–ø.*",
    ]

    for g in garbage:
        text = re.sub(g, "", text, flags=re.IGNORECASE)

    return text.strip()


# ============================================================
# üîß –ú—è–≥–∫–∏–π baseline (—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ, –ù–ï —Ñ–∏–ª—å—Ç—Ä!)
# ============================================================

def baseline_weight(filename: str, text: str) -> float:
    fn = (filename or "").lower()
    t = (text or "").lower()

    # –±–∞–∑–æ–≤—ã–π –≤–µ—Å –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
    if any(x in fn for x in [
        "–¥–æ–ø—Ä–æ—Å", "–ø—Ä–æ—Ç–æ–∫–æ–ª_–¥–æ–ø—Ä–æ—Å–∞", "–¥–æ–ø—Ä–æ—Å–∞",
        "–æ–±—ä—è—Å–Ω–µ–Ω–∏–µ", "–ø–æ—è—Å–Ω–µ–Ω–∏"
    ]):
        weight = 0.95

    elif any(x in fn for x in ["—Ä–∞–ø–æ—Ä—Ç", "–µ—Ä–¥—Ä"]):
        weight = 0.90

    elif "–ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ" in fn:
        weight = 0.80

    elif "–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ" in fn:
        weight = 0.60

    else:
        weight = 0.50

    # —É—Å–∏–ª–∏–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã, –≥–¥–µ —Ñ–∏–≥—É—Ä–∏—Ä—É–µ—Ç –ø–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º—ã–π / –æ–±–≤–∏–Ω—è–µ–º—ã–π
    if any(x in t for x in ["–ø–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º", "–æ–±–≤–∏–Ω—è–µ–º", "—Å–æ–æ–±—â–µ–Ω–∏–µ –æ –ø–æ–¥–æ–∑—Ä–µ–Ω–∏–∏"]):
        weight = max(weight, 0.97)

    # —Å–ª–µ–≥–∫–∞ –æ–ø—É—Å–∫–∞–µ–º ¬´–∑–∞—è–≤–ª–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä–ø–µ–≤—à–µ–≥–æ¬ª
    if "–∑–∞—è–≤–ª–µ–Ω–∏–µ" in t and "–ø–æ—Ç–µ—Ä–ø–µ–≤—à" in t:
        weight = min(weight, 0.75)

    return weight


# ============================================================
# üîß –§–∏–ª—å—Ç—Ä –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –±–ª–æ–∫–æ–≤
# ============================================================

def _is_question_block(text: str) -> bool:
    if not text:
        return False

    low = text.lower().strip()

    if "–≤–æ–ø—Ä–æ—Å:" in low or "–≤–æ–ø—Ä–æ—Å :" in low or "–≤–æ–ø—Ä–æ—Å " in low:
        return True

    if "—Å–ø—Ä–æ—Å–∏–ª" in low or "—Å–ø—Ä–æ—Å–∏–ª–∞" in low or "–∫–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º" in low:
        return True

    if low.endswith("?"):
        return True

    # –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–µ –±–ª–æ–∫–∏ —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏
    if "?" in text and "\n" in text:
        return True

    return False


# ============================================================
# üî• –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø RETRIEVAL 7.6 (NO CASE FILTER)
# ============================================================

def get_file_docs_for_qualifier(
    db: Session,
    file_ids: Optional[List[str]] = None,
    case_id: Optional[str] = None,  # ‚Üê –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è
) -> List[Dict[str, Any]]:

    query = db.query(File)

    # ‚ùå –ë–û–õ–¨–®–ï –ù–ï–¢:
    # if case_id:
    #    query = query.filter(File.case_id == case_id)

    # ‚úî –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω—ã file_ids ‚Üí –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö
    if file_ids:
        query = query.filter(File.file_id.in_(file_ids))
        logger.info(f"üìÑ Retrieval 7.6: –∏—Å–ø–æ–ª—å–∑—É–µ–º file_ids ({len(file_ids)})")
    else:
        logger.info(f"üìÑ Retrieval 7.6: file_ids –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω—ã ‚Üí –∏—Å–ø–æ–ª—å–∑—É–µ–º –í–°–ï —Ñ–∞–π–ª—ã.")

    files = query.all()
    logger.info(f"üìÑ Retrieval 7.6: –≤—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤ = {len(files)}")

    docs: List[Dict[str, Any]] = []

    # ============================================================
    # –ß–∏—Ç–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã (–≤–∫–ª—é—á–∞—è —Ñ–∞–π–ª—ã –±–µ–∑ case_id)
    # ============================================================

    for f in files:
        file_id = str(f.file_id)
        filename = (f.filename or "").lower()

        try:
            chunks = (
                db.query(Chunk)
                .filter(Chunk.file_id == UUID(file_id))
                .order_by(Chunk.page.asc(), Chunk.start_offset.asc())
                .all()
            )
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —á–∞–Ω–∫–æ–≤ –¥–ª—è {file_id}: {e}")
            continue

        if not chunks:
            continue

        for ch in chunks:
            raw = (ch.text or "").strip()

            if len(raw) < MIN_TEXT_LENGTH:
                continue

            if raw.count(" ") < 3:
                continue

            clean = normalize_text(raw)
            if not clean or len(clean) < MIN_TEXT_LENGTH:
                continue

            if _is_question_block(clean):
                continue

            docs.append({
                "file_id": file_id,
                "filename": f.filename,
                "page": ch.page or 1,
                "chunk_id": str(ch.chunk_id),
                "text": clean,
            })

    logger.info(f"üì¶ Retrieval 7.6: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤ = {len(docs)}")

    if not docs:
        return []

    # ============================================================
    # baseline —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
    # ============================================================

    for d in docs:
        d["baseline_weight"] = baseline_weight(d["filename"], d["text"])

    docs = sorted(docs, key=lambda x: x["baseline_weight"], reverse=True)

    docs = docs[:TOP_BASELINE_LIMIT]

    logger.info(
        f"‚úÖ Retrieval 7.6: –ø–µ—Ä–µ–¥–∞—ë–º {min(len(docs), TOP_RERANK_INPUT)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ RAG Router"
    )

    return docs[:TOP_RERANK_INPUT]


# ============================================================
# üîç DEBUG SEARCH
# ============================================================

def search_chunks(db: Session, query: str, limit: int = 20) -> List[Dict[str, Any]]:
    if not query or not query.strip():
        return []

    pattern = f"%{query.strip()}%"

    try:
        chunks = (
            db.query(Chunk)
            .filter(Chunk.text.ilike(pattern))
            .order_by(Chunk.page.asc())
            .limit(limit)
            .all()
        )
    except Exception as e:
        logger.error(f"[search_chunks ERROR] {e}")
        return []

    results = []
    for ch in chunks:
        results.append({
            "file_id": str(ch.file_id),
            "chunk_id": str(ch.chunk_id),
            "page": ch.page,
            "text": ch.text[:300] + ("..." if len(ch.text) > 300 else "")
        })

    return results
