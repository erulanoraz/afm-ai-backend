# app/services/retrieval.py
import logging
import re
from typing import List, Dict, Any, Optional
from uuid import UUID
from sqlalchemy.orm import Session

from app.db.models import File, Chunk

logger = logging.getLogger(__name__)


# ============================================================
# üî• –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ (Kazakhstan legal safe)
# ============================================================

def normalize_text(text: str) -> str:
    if not text:
        return ""

    text = text.replace("\r", "")
    text = re.sub(r"[ \t]+", " ", text)
    text = re.sub(r"\n{2,}", "\n", text)

    garbage = [
        r"¬©\s?–í—Å–µ –ø—Ä–∞–≤–∞ –∑–∞—â–∏—â–µ–Ω—ã",
        r"—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–æ\s?—Å\s?–ø–æ–º–æ—â—å—é.*",
        r"—Å—Ç—Ä–∞–Ω–∏—Ü–∞\s?\d+\s?–∏–∑\s?\d+",
        r"–î–æ–∫—É–º–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω.*",
        r"QR[- ]?–∫–æ–¥.*",
        r"—ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç.*",
        r"–ü—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–æ.*",
        r"–î–∞—Ç–∞ –ø–µ—á–∞—Ç–∏.*",
    ]

    for g in garbage:
        text = re.sub(g, "", text, flags=re.IGNORECASE)

    return text.strip()


# ============================================================
# üî• –û—Ü–µ–Ω–∫–∞ —á–∞–Ω–∫–∞ (baseline weight)
# ============================================================

def baseline_weight(filename: str, text: str) -> float:
    fn = filename.lower()
    t = text.lower()

    # —Å—É–ø–µ—Ä –≤–∞–∂–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    strong = [
        "—Ä–∞–ø–æ—Ä—Ç", "–∫—É–∏", "–µ—Ä–¥—Ä", "–ø—Ä–æ—Ç–æ–∫–æ–ª_–¥–æ–ø—Ä–æ—Å–∞_–ø–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º",
        "–ø—Ä–æ—Ç–æ–∫–æ–ª –¥–æ–ø—Ä–æ—Å–∞ –ø–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º",
    ]
    medium = [
        "–ø—Ä–æ—Ç–æ–∫–æ–ª_–¥–æ–ø—Ä–æ—Å–∞_–ø–æ—Ç–µ—Ä–ø–µ–≤—à–µ–≥–æ",
        "–ø—Ä–æ—Ç–æ–∫–æ–ª_–¥–æ–ø—Ä–æ—Å–∞_–ø–æ—Ç–µ—Ä–ø–µ–≤—à",
        "–ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –æ –ø—Ä–∏–∑–Ω–∞–Ω–∏–∏ –ª–∏—Ü–∞ –ø–æ—Ç–µ—Ä–ø–µ–≤—à–∏–º",
        "–ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –æ –ø—Ä–∏–∑–Ω–∞–Ω–∏–∏ –ª–∏—Ü–∞ –≥—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–º –∏—Å—Ç—Ü–æ–º",
    ]

    if any(x in fn for x in strong):
        return 1.0

    if any(x in t for x in ["–æ–Ω –ø–æ–¥–æ–∑—Ä–µ–≤–∞–µ—Ç—Å—è", "–æ–Ω–∞ –ø–æ–¥–æ–∑—Ä–µ–≤–∞–µ—Ç—Å—è"]):
        return 0.95

    if any(x in fn for x in medium):
        return 0.80

    if "–ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ" in fn:
        return 0.70

    return 0.40


# ============================================================
# üî• Retrieval 4.0 ‚Äî –≥–ª–∞–≤–Ω—ã–π
# ============================================================

def get_file_docs_for_qualifier(
    db: Session,
    file_ids: Optional[List[str]] = None,
    case_id: Optional[str] = None,
) -> List[Dict[str, Any]]:

    query = db.query(File)

    if case_id:
        query = query.filter(File.case_id == case_id)

    if file_ids:
        query = query.filter(File.file_id.in_(file_ids))

    files = query.all()
    logger.info(f"üìÑ Retrieval: –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ = {len(files)}")

    docs: List[Dict[str, Any]] = []

    for f in files:
        file_id = str(f.file_id)

        try:
            chunks = (
                db.query(Chunk)
                .filter(Chunk.file_id == UUID(file_id))
                .order_by(
                    Chunk.page.asc(),
                    Chunk.start_offset.asc(),
                    Chunk.created_at.asc(),
                    Chunk.chunk_id.asc(),
                )
                .all()
            )
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —á–∞–Ω–∫–æ–≤ —Ñ–∞–π–ª–∞ {file_id}: {e}")
            continue

        if not chunks:
            continue

        for ch in chunks:
            raw_text = (ch.text or "").strip()
            if not raw_text:
                continue

            clean = normalize_text(raw_text)
            if not clean:
                continue

            docs.append({
                "file_id": file_id,
                "filename": f.filename,
                "page": ch.page or 1,
                "chunk_id": str(ch.chunk_id),
                "text": clean,
            })

    # ===========================================================
    # üî• BASELINE —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    # ===========================================================
    for d in docs:
        d["baseline_weight"] = baseline_weight(
            filename=d["filename"],
            text=d["text"]
        )

    docs = sorted(docs, key=lambda x: x["baseline_weight"], reverse=True)

    # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ ‚Äî –Ω–µ –±–æ–ª–µ–µ 400 —á–∞–Ω–∫–æ–≤
    docs = docs[:400]

    logger.info(f"üì¶ Retrieval 4.0 –≤–µ—Ä–Ω—É–ª –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(docs)}")
    return docs
